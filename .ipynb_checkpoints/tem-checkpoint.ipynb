{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''文件处理'''\n",
    "import os, sys\n",
    "from os import listdir, getcwd\n",
    "from os.path import join\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''word2vec特征提取'''\n",
    "from gensim.models import word2vec \n",
    "\n",
    "\n",
    "'''绘画，展示功能'''\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "'''分类器'''\n",
    "from sklearn.ensemble import AdaBoostClassifier as ADA#AdaBoost\n",
    "from xgboost.sklearn import XGBClassifier#XGBoost\n",
    "from sklearn import neighbors#KNN\n",
    "from sklearn.ensemble import RandomForestClassifier #随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier as RF#随机森林\n",
    "from sklearn.linear_model import LogisticRegression#逻辑回归\n",
    "#下面这些实验中没用到\n",
    "from sklearn import svm#运行巨慢，动不动几个小时。AUC=0.70+，调参后应该能上0.8吧\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ET#极端随机树\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn.neural_network import MLPClassifier# 多层感知机\n",
    "from sklearn.naive_bayes import CategoricalNB#朴素贝叶斯算法\n",
    "\n",
    "'''跟数据有关'''\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,train_test_split\n",
    "from sklearn.datasets import make_classification,make_hastie_10_2\n",
    "\n",
    "'''其他'''\n",
    "import datetime#显示运行时间\n",
    "import warnings;warnings.filterwarnings('ignore')#过滤警告\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_test(file_name):\n",
    "#该函数能够将 文本的的制表符通通改成逗号\n",
    "    #先复制，再清空原本的，最后将修改过的复制内容写入\n",
    "    with open(file_name, \"r+\") as file_object:\n",
    "        read_data=file_object.read()# 用一个临时变量 read_data 存储文本内容\n",
    "        file_object.seek(0)#把文件定位到position 0，没有这句的话，文件是定位到数据最后，truncate也是从这里删除，所以感觉就是没起作用。\n",
    "        file_object.truncate()   #清空文件\n",
    "        file_object.write(read_data.replace('\\t', ','))#写入复制内容\n",
    "    \n",
    "dateset_path='class_II_similarity_reduced_5cv_sep'#数据集的路径\n",
    "dirs = os.listdir(dateset_path)# os.listdir() 方法用于返回指定的文件夹包含的文件或文件夹的名字的列表。\n",
    "\n",
    "for file in dirs: #对dateset_path路径下每个文件名进行操作\n",
    "    '''\n",
    "        ①删除宿主为mouse的数据。\n",
    "    '''\n",
    "    if 'H-2-' in file:\n",
    "        os.remove(dateset_path+'\\\\'+file)#os.remove删除文件\n",
    "        continue\n",
    "\n",
    "    '''\n",
    "        ②将文本的的制表符通通改成逗号，目的是为了生成合格的csv文件。将每个文件的后缀从'.txt'更改为‘.csv'\n",
    "    '''\n",
    "    modify_test(dateset_path+'\\\\'+file)#该函数能够将 文本的的制表符通通改成逗号，标准csv是用逗号分开不同类型的信息的\n",
    "    portion = os.path.splitext(dateset_path+'\\\\'+file)  # 分离文件名与扩展名\n",
    "    # 如果后缀是‘.txt'\n",
    "    if portion[1] == '.txt':#更改后缀\n",
    "       # 重新组合文件名和后缀名\n",
    "        newname = portion[0] + '.csv'\n",
    "        os.rename(dateset_path+'\\\\'+file, newname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ③删除不必要的列\n",
    "'''\n",
    "dirs = os.listdir(dateset_path)#后缀都改了，所以文件名列表都要更新\n",
    "for file in dirs:\n",
    "    # 加载数据集,添加头一行的信息，方便后续操作\n",
    "    dataframe = pd.read_csv(dateset_path+'\\\\'+file,header=0,names=('host','allele','len','antigen','description','operator','value'))\n",
    "    # 删除不必要的属性。不会修改源文件\n",
    "    dataframe =dataframe.drop(['host','allele','len','antigen','operator'], axis=1)\n",
    "    # 比较阈值，添加标签信息Boole\n",
    "    true_value=[]#真实值，非 1 即 0\n",
    "    threshold=1500#阈值，IC50，单位为nM。若低于该值，则记为强亲和，判断该肽段为该种MHC分子的亲和肽\n",
    "    for i in dataframe['value']:#根据亲和力大小来判断结合力的强弱。强为1，弱为0\n",
    "        if(i>=threshold):\n",
    "            true_value.append(0)\n",
    "        else:\n",
    "            true_value.append(1)\n",
    "    dataframe.insert(1,\"Boole\",true_value)#添加标签列表\n",
    "    dataframe.to_csv(dateset_path+'\\\\'+file,header=None) # 写入同名文件，写入的时候是先清空文件再写的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    ④将train和test类的文件分别移动到两个文件夹中。不难理解，将数据集分为训练集和测试集\n",
    "'''\n",
    "dateset_path='class_II_similarity_reduced_5cv_sep'#数据集的路径\n",
    "#采用新的训练集路径to_train_path 和 测试集路径 to_test_path\n",
    "to_train_path = dateset_path +'\\\\'+ 'train'\n",
    "to_test_path = dateset_path +'\\\\' + 'test'\n",
    "#  如果不存在train或test文件夹，则创建\n",
    "if not os.path.isdir(to_train_path):\n",
    "     os.makedirs(to_train_path)\n",
    "if not os.path.isdir(to_test_path):\n",
    "     os.makedirs(to_test_path)\n",
    "\n",
    "for file in dirs:#dirs包含dateset_path路径下的所有文件名，以列表形式存储，包括train文件和test文件，现在将它们分开。\n",
    "     from_path = dateset_path+'\\\\'+file\n",
    "     if '_train_' in file:# 这样的缩进才是正确的，有毒\n",
    "         shutil.move(from_path, to_train_path)\n",
    "     else:\n",
    "         shutil.move(from_path, to_test_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    ⑥将每个文件夹的csv文件合并成一个，并删除多余的文件夹。\n",
    "    因为每个文件夹的文件合并成一个文件后，就没有必要用一个文件夹存储一个文件，所以把合并了的文件提出来之后，要删除空文件夹。\n",
    "'''\n",
    "for k in range(2):# k==0时，处理训练集。k==1 时，处理测试集\n",
    "    if k == 0:\n",
    "        data_category='train'\n",
    "    else:\n",
    "        data_category='test'\n",
    "        \n",
    "    folder = os.listdir('class_II_similarity_reduced_5cv_sep'+'\\\\'+data_category)# 获取文件夹名，方便后面删除空文件夹\n",
    "    dateset_path='class_II_similarity_reduced_5cv_sep'+'\\\\'+data_category\n",
    "    dirs = os.listdir(dateset_path)\n",
    "    read_data='num,description,Boole,value\\n'# 加上头行信息\n",
    "    for file in dirs:\n",
    "        file_name=dateset_path+'\\\\'+file\n",
    "        with open(file_name, \"r\") as file_object:\n",
    "            read_data+=file_object.read().rstrip()\n",
    "        read_data += '\\n'\n",
    "        os.remove(file_name)\n",
    "\n",
    "    com_csv='class_II_similarity_reduced_5cv_sep'+'\\\\'+data_category+'.csv'\n",
    "    # 把合成文件放在train的目录下，本来是 train/i 的\n",
    "    with open(com_csv, \"w\") as file_object:\n",
    "        file_object.write(read_data.rstrip())\n",
    "        \n",
    "\n",
    "    os.rmdir('class_II_similarity_reduced_5cv_sep'+'\\\\'+data_category)\n",
    "    \n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['K', 'P', 'T', 'A', 'A', 'G', 'P', 'K', 'D', 'N', 'G', 'G', 'A', 'C', 'G'],\n",
       " ['I', 'Y', 'K', 'A', 'S', 'P', 'T', 'L', 'A', 'F', 'P', 'A', 'G', 'V', 'C']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis=[]\n",
    "tem=[]\n",
    "for i in 'KPTAAGPKDNGGACG':\n",
    "    tem.append(i)\n",
    "lis.append(tem)\n",
    "lis\n",
    "tem=[]\n",
    "for i in 'IYKASPTLAFPAGVC':\n",
    "    tem.append(i)\n",
    "lis.append(tem)\n",
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    ①先训练所有的csv文件，获得模型。保存备份模型、删除模型。\n",
    "'''\n",
    "\n",
    "# 引入数据集\n",
    "dateset_path='class_II_similarity_reduced_5cv_sep'#数据集的路径\n",
    "dateset = os.listdir(dateset_path)# 获取文件夹\n",
    "lis=[]\n",
    "for data in dateset:# train文件夹和test文件夹\n",
    "    dataframe=pd.read_csv('class_II_similarity_reduced_5cv_sep'+'\\\\'+data)\n",
    "    for i in dataframe['description']:\n",
    "        tem_lis=[]\n",
    "        for j in i:\n",
    "            tem_lis.append(j)\n",
    "        lis.append(tem_lis)# 一个残基一个残基的加入，从而实现切分词汇\n",
    "        \n",
    "\n",
    "# 构建模型\n",
    "model = word2vec.Word2Vec(lis,size=4,window=5,sg=1,min_count=1)\n",
    "\n",
    "# 保存模型\n",
    "model.save('word2vec_s04.model')  \n",
    "\n",
    "# 删除占用大内存的model\n",
    "del model \n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L': [0.18704472, -0.26065737, -0.24672244, -0.49416706], 'A': [-0.7160991, -0.39639765, 0.109033614, -0.5321107], 'S': [0.1771334, -0.36034122, -0.16427441, -0.41917834], 'K': [0.092939146, -0.5153967, 0.08515208, -0.3385036], 'V': [-0.12873943, -0.39658388, -0.03795533, -0.43979782], 'G': [-0.10216831, -0.40500155, 0.038609073, -0.44964755], 'I': [0.16380377, -0.44907415, -0.018054571, -0.37054676], 'T': [-0.25376412, -0.5291301, 0.03181604, -0.33664027], 'E': [-0.00014762262, -0.4655229, 0.11863356, -0.40334922], 'D': [0.17615737, -0.39223203, 0.066088736, -0.4472649], 'P': [-0.4081028, -0.57643825, -0.19369566, -0.23927844], 'N': [0.2708956, -0.32607934, -0.045681573, -0.4788281], 'F': [-0.022464724, -0.32182202, -0.08999622, -0.48798257], 'R': [0.1587508, -0.2635773, -0.42405245, -0.4389825], 'Y': [-0.092686735, -0.44521028, -0.0091692535, -0.39003477], 'Q': [0.022781765, -0.48462424, -0.47987655, -0.20903994], 'M': [0.23231736, -0.31081232, -0.37908542, -0.39812788], 'H': [0.10117175, 0.050249938, -0.21588546, -0.8224027], 'C': [0.42734578, -0.2910602, 0.13746692, -0.56041026], 'W': [0.24248551, -0.6555227, 0.059559505, -0.18221143]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    从备份的模型获得词向量，以字典的形式保存。\n",
    "'''\n",
    "loaded_model = word2vec.Word2Vec.load('word2vec_s04.model')  # 加载模型\n",
    "# 将获得的词汇及其对应的向量按字典的格式存放到 word_vector_dict 中\n",
    "word_vector_dict={}\n",
    "# print(model.wv.index2word) # 获得所有的词汇\n",
    "for word in loaded_model.wv.index2word:\n",
    "    #(word,model[word]) #获得词汇及其对应的向量\n",
    "    word_vector_dict[word]=list(loaded_model[word])\n",
    "    \n",
    "# 展示训练的词及其词向量\n",
    "# print(len(word_vector_dict))\n",
    "print(word_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "def weight_row(rows):# 根据氨基酸序列长度生成 等差列表 [0.1,0.2,0.3,...,0.1*rows]。系数值可调整，这里只是预设\n",
    "    w=[]\n",
    "    for i in range(1,rows+1):\n",
    "        w.append(round(0.3*i,2))\n",
    "    return w\n",
    "\n",
    "def row_value_change(lt,k):# 行乘以倍数，行以列表表示\n",
    "    c = []\n",
    "    for i in range(len(lt)):\n",
    "        c.append(float(lt[i])*k)\n",
    "    return c\n",
    "\n",
    "def list_add(a,b,length):# 实现两个列表对应项相加\n",
    "    c = []\n",
    "    for i in range(length):\n",
    "        c.append(a[i]+b[i])\n",
    "    return c\n",
    "\n",
    "''' \n",
    "    实现 “ 氨基酸序列 -> 特征矩阵 -> 特征行向量 ” 的映射\n",
    "'''\n",
    "n_cols=len(word_vector_dict['L'])# n_cols 的值等于 word2vec.Word2Vec()中的 size 参数。代表的词向量的列数，特征行向量end_row需要用到\n",
    "\n",
    "def feature_rows(path):# 将给定文件中的氨基酸序列组转换为行向量组，返回行向量组（列表形式）\n",
    "    aa_sequences = pd.read_csv(path,usecols=[1])# 获取氨基酸序列（位于第二列），aa 即 氨基酸aminoacid\n",
    "    feature_rows=[] #用于收集\n",
    "    for aa in aa_sequences['description']: # 遍历每个氨基酸序列\n",
    "    #     print(aa)\n",
    "        n_rows=len(aa)#15\n",
    "        # 由序列和词向量生成对应的矩阵。矩阵以列表存储\n",
    "        '''\n",
    "            ①将氨基酸序列转换成矩阵。矩阵以列表存储。\n",
    "        '''\n",
    "        matrix=[]\n",
    "        for a in aa:# 遍历每个氨基酸残基\n",
    "    #         print(a,end=' ')\n",
    "    #         print(word_vector_dict[a])\n",
    "            matrix.append(word_vector_dict[a])\n",
    "    #     print(\"matrix=\",end=\" \")\n",
    "    #     print(matrix)\n",
    "\n",
    "        '''\n",
    "            ②根据矩阵的行数（由氨基酸序列长度决定），生成对应的权重行向量 weight_row\n",
    "        '''\n",
    "        w_row=weight_row(n_rows)# 返回一个（1*len(aa)）的权重行向量\n",
    "    #     print(\"w_row=\",end=\" \")\n",
    "    #     print(w_row)\n",
    "\n",
    "        '''\n",
    "            ③权重行向量 weight_row 与 特征矩阵 matrix 相乘，得目标特征行向量 end_row。\n",
    "            数学表达式为：end_row = weight * matrix = 0.1*r1+0.2*r2+0.3*r3+...+（0.1*len）*r_len 。\n",
    "            其中r1,r2,...,r_len 都为行向量，end_row 自然也是行向量。\n",
    "        '''\n",
    "        end_row=[0]*n_cols#初始化\n",
    "        for i in range(n_rows):\n",
    "            change_value_row=row_value_change(matrix[i],w_row[i])#行向量乘以系数\n",
    "            end_row=list_add(end_row,change_value_row,n_cols)#行向量累加\n",
    "\n",
    "    #     print('end_row=',end=' ')\n",
    "    #     print(end_row)\n",
    "    #     print('')\n",
    "        for i in range(len(end_row)):# 降低特征向量中元素的位数，从而降低后续的计算量。但效果不是很明显\n",
    "            end_row[i]=round(end_row[i],4)# 仅仅保留4位小数\n",
    "        feature_rows.append(end_row)\n",
    "    return feature_rows\n",
    "\n",
    "'''\n",
    "    ④将（标签值，end_row）这两列数据读入临时文件中，并覆盖同名文件。\n",
    "    最终的数据文件应该只包含（标签值，end_row）这两个有用的信息。但是使用pandas工具处理csv的时候，始终会有下标。\n",
    "    所以最终的文件包含（下标，标签值，end_row）\n",
    "'''\n",
    "\n",
    "'''\n",
    "    先弄train数据集\n",
    "'''\n",
    "file_name='class_II_similarity_reduced_5cv_sep'+'\\\\'+'train.csv'\n",
    "file = pd.read_csv(file_name,usecols=[2])# 清洗完毕的file文件，只包含（end_row,标签值）\n",
    "file.insert(1,\"feature_row\",feature_rows(file_name))\n",
    "file.to_csv(file_name)# header参数 根据需要更改\n",
    "   \n",
    "'''\n",
    "    再弄test数据集\n",
    "'''\n",
    "\n",
    "file_name='class_II_similarity_reduced_5cv_sep'+'\\\\'+'test.csv'\n",
    "file = pd.read_csv(file_name,usecols=[2])# 清洗完毕的file文件，只包含（end_row,标签值）\n",
    "file.insert(1,\"feature_row\",feature_rows(file_name))\n",
    "file.to_csv(file_name)# header参数 根据需要更改\n",
    "    \n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4参数寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    5折交叉验证进行训练测试，训练集按5折分层抽样。\n",
    "'''\n",
    "def Stratified5Fold(clfs): \n",
    "    ### 5折stacking\n",
    "    n_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)#分层抽样\n",
    "    aucs=[]\n",
    "    run_time=[]\n",
    "    for i,clf in enumerate(clfs):\n",
    "        print(\"分类器：{}\".format(clf))\n",
    "        tem_auc=0\n",
    "        tem_time=0.0\n",
    "        for j,(train_index,test_index) in enumerate(skf.split(X_train,y_train)):\n",
    "            tr_x = X_train[train_index]#按train_index所标示的行号取样，作为训练数据的特征值\n",
    "            tr_y = y_train[train_index]#按train_index所标示的行号取样，作为训练数据的标签\n",
    "\n",
    "            start = datetime.datetime.now()\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            end = datetime.datetime.now()\n",
    "            tem_time+=(end-start).total_seconds()\n",
    "            \n",
    "            #每得到一个模型就可以进行一次对测试集的预测\n",
    "            pred = clf.predict_proba(X_test)[:,1]\n",
    "            tem_auc+=roc_auc_score(y_test,pred)\n",
    "    #         show_auc(model,test_data,test_label,title_name)\n",
    "\n",
    "        run_time.append(tem_time)\n",
    "        aucs.append(tem_auc/5)\n",
    "\n",
    "        print('各模型的auc= ',aucs)\n",
    "        print('各模型的运行时间= ',run_time)\n",
    "        print(' ')\n",
    "\n",
    "#     #按序输出各分类器的auc\n",
    "#     return aucs,run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.1 Adaboost\n",
    "    n_estimators从5开始，间隔为25，迭代到980。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tree=[5]\n",
    "for i in range(1,40):\n",
    "    n_tree.append(n_tree[i-1]+25)\n",
    "ada_clfs=[]\n",
    "print(n_tree)\n",
    "for i in range(len(n_tree)):\n",
    "    ada_clfs.append(ADA(n_estimators=n_tree[i]))\n",
    "print(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Stratified5Fold(ada_clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.2XGBoost\n",
    "    ①n_estimators值以5，间隔为10，迭代到995。\n",
    "    ②基于n_estimators=500使用hyperopt来自动调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ①n_estimators值以5，间隔为10，迭代到995。\n",
    "'''\n",
    "n_tree=[5]\n",
    "for i in range(1,100):\n",
    "    n_tree.append(n_tree[i-1]+10)\n",
    "xgb_clfs=[]\n",
    "print(n_tree)\n",
    "for i in range(len(n_tree)):\n",
    "    xgb_clfs.append(XGBClassifier(n_estimators=n_tree[i], objective=\"binary:logistic\"))\n",
    "# print(xgb_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stratified5Fold(xgb_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ②基于n_estimators=500使用hyperopt来自动调参。\n",
    "'''\n",
    "#coding:utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from random import shuffle\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp,space_eval,rand,Trials,partial,STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    关于hyperopt\n",
    "##### 详细教程 1. https://www.cnblogs.com/gczr/p/7156270.html 2.https://cloud.tencent.com/developer/article/1471190 3.https://blog.csdn.net/qq_41076797/article/details/102941095\n",
    "##### github https://github.com/hyperopt/hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)#分层抽样\n",
    "for j,(train_index,test_index) in enumerate(skf.split(X_train,y_train)):\n",
    "    tr_x = X_train[train_index]#按train_index所标示的行号取样，作为训练数据的特征值\n",
    "    tr_y = y_train[train_index]#按train_index所标示的行号取样，作为训练数据的标签\n",
    "    clf.fit(tr_x, tr_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBM(argsDict):\n",
    "    max_depth=argsDict[\"max_depth\"]+2#[2,22]\n",
    "    min_child_weight=argsDict[\"min_child_weight\"]#[0.05,2.5]]\n",
    "    gamma = argsDict['gamma']#[0.01,2.5]\n",
    "\n",
    "    auc_cv=[]\n",
    "    n_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)#分层抽样\n",
    "    model=xgb.XGBClassifier(\n",
    "                    n_estimators=500,   #树的数量\n",
    "                    max_depth=max_depth,  #最大深度\n",
    "                    min_child_weight=min_child_weight,  #控制叶子节点样本数最低值，单位为百个\n",
    "                    gamma=gamma,#控制叶子节点分区的最小损失阈值\n",
    "                    objective=\"binary:logistic\"#二分类\n",
    "    ) \n",
    "    \n",
    "    for j,(train_index,test_index) in enumerate(skf.split(X_train,y_train)):\n",
    "        tr_x = X_train[train_index]#按train_index所标示的行号取样，作为训练数据的特征值\n",
    "        tr_y = y_train[train_index]#按train_index所标示的行号取样，作为训练数据的标签\n",
    "        model.fit(tr_x, tr_y)\n",
    "        pred = model.predict_proba(X_test)[:,1]\n",
    "        auc_cv.append(roc_auc_score(y_test,pred))\n",
    "    \n",
    "    s=0\n",
    "    for i in range(5):\n",
    "        s+=auc_cv[i]\n",
    "    print('auc= ',s/5)\n",
    "    return -s/5\n",
    "\n",
    "space = {\"max_depth\":hp.randint(\"max_depth\",20),\n",
    "         \"min_child_weight\":hp.uniform(\"min_child_weight\",0.05,2.5),\n",
    "         \"gamma\":hp.uniform(\"gamma\",0.01,2.5),\n",
    "        }\n",
    "algo = partial(tpe.suggest,n_startup_jobs=1)\n",
    "best = fmin(GBM,space,algo=tpe.suggest,max_evals=20)#max_evals表示想要训练的最大模型数量，越大越容易找到最优解,algo = tpe.suggest\n",
    "\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.3KNN\n",
    "以1开始、间距为1，迭代到100，比较两种权重函数在不同的AUC和运行时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    weights='distance' 时\n",
    "'''\n",
    "\n",
    "n_k=[1]\n",
    "for i in range(1,100):\n",
    "    n_k.append(n_k[i-1]+1)\n",
    "knn_clfs_d=[]\n",
    "print(n_k)\n",
    "for i in range(len(n_k)):\n",
    "    knn_clfs_d.append(neighbors.KNeighborsClassifier(n_k[i],weights='distance'))\n",
    "# print(knn_clfs_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stratified5Fold(knn_clfs_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    weights='uniform' 时\n",
    "'''\n",
    "n_k=[1]\n",
    "for i in range(1,100):\n",
    "    n_k.append(n_k[i-1]+1)\n",
    "knn_clfs_u=[]\n",
    "print(n_k)\n",
    "for i in range(len(n_k)):\n",
    "    knn_clfs_u.append(neighbors.KNeighborsClassifier(n_k[i],weights='uniform'))\n",
    "print(knn_clfs_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stratified5Fold(knn_clfs_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.4随机森林\n",
    "    ①将树的个数从5开始，间距为10，枚举到145\n",
    "    ②基于n_estimators=145，并继续探究参数max_features和criterion的较优值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ①将树的个数从5开始，间距为10，枚举到145\n",
    "'''\n",
    "n_tree=[5]\n",
    "for i in range(1,15):\n",
    "    n_tree.append(n_k[i-1]+10)\n",
    "rf_clfs=[]\n",
    "print(n_tree)\n",
    "for i in range(len(n_tree)):\n",
    "    rf_clfs.append(RandomForestClassifier(n_estimators=n_tree[i]))\n",
    "print(rf_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stratified5Fold(rf_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ①基于n_estimators=145，并继续探究参数max_features和criterion的较优值。\n",
    "'''\n",
    "rf_clfs=[ \n",
    "    RandomForestClassifier(n_estimators=145,max_features='sqrt',criterion='gini'),\n",
    "    RandomForestClassifier(n_estimators=145,max_features='auto',criterion='gini'),\n",
    "    RandomForestClassifier(n_estimators=145,max_features='log2',criterion='gini'),\n",
    "    RandomForestClassifier(n_estimators=145,max_features='sqrt',criterion='entropy'),\n",
    "    RandomForestClassifier(n_estimators=145,max_features='auto',criterion='entropy'),\n",
    "    RandomForestClassifier(n_estimators=145,max_features='log2',criterion='entropy'),\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stratified5Fold(rf_clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5Stacking集成学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.1 生成不同泛化性能的个体学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    形式\t算法\t参数名称\t数值\n",
    "    欠拟合 \t随机森林\tn_estimators\t2\n",
    "            XGBoost    n_estimators\t30\n",
    "                    max_depth\t6\n",
    "                    min_child_weight\t1\n",
    "                    gamma\t0\n",
    "             AdaBoost\tn_estimators\t50\n",
    "             KNN\t     n_neighbors\t5\n",
    "                      weights\t'distance'\n",
    "    局部较优\t随机森林\tn_estimators\t145\n",
    "             XGBoost\tn_estimators\t500\n",
    "                        max_depth\t13\n",
    "                        min_child_weight\t0.509353199\n",
    "                        gamma\t0.297658057\n",
    "             AdaBoost\tn_estimators\t980\n",
    "             KNN\t     n_neighbors\t50\n",
    "                     weights\t'distance'\n",
    "    高值参数\t随机森林\tn_estimator\t500\n",
    "            XGBoost\t     n_estimators\t1000\n",
    "                     max_depth\t16\n",
    "                    min_child_weight\t0.25\n",
    "                    gamma\t0.297658057\n",
    "            AdaBoost\tn_estimators\t2000\n",
    "            KNN\t       n_neighbors\t3000\n",
    "                    weights\t'distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_fitting_clfs=[ #欠拟合\n",
    "      ADA(n_estimators=50),\n",
    "      XGBClassifier(n_estimators=30, max_depth=6,min_child_weight=1,gamma=0),\n",
    "      neighbors.KNeighborsClassifier(n_neighbors=5,weights='distance'),\n",
    "      RandomForestClassifier(n_estimators=2),\n",
    "     ]\n",
    "\n",
    "performed_parameter_clfs=[#调参后\n",
    "      ADA(n_estimators=980),\n",
    "      XGBClassifier(n_estimators=500, max_depth=13,min_child_weight=0.509353199,gamma=0.297658057),\n",
    "      neighbors.KNeighborsClassifier(n_neighbors=50,weights='distance'),\n",
    "      RandomForestClassifier(n_estimators=145),\n",
    "]\n",
    "\n",
    "high_parameter_clfs=[#高值参数\n",
    "      ADA(n_estimators=2000),\n",
    "      XGBClassifier(n_estimators=1000, max_depth=13,min_child_weight=0.509353199,gamma=0.297658057),\n",
    "      neighbors.KNeighborsClassifier(n_neighbors=3000,weights='distance'),\n",
    "      RandomForestClassifier(n_estimators=145),\n",
    "]\n",
    "\n",
    "tre_clfs=[under_fitting_clfs,performed_parameter_clfs,high_parameter_clfs]\n",
    "# type(high_parameter_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    各种学习器的预测性能\n",
    "'''\n",
    "for c,clfs in enumerate(tre_clfs):\n",
    "    if c==0:\n",
    "        print('欠拟合组：')\n",
    "    elif c==1:\n",
    "        print('调参后组：')\n",
    "    else:\n",
    "        print('高参数组：')\n",
    "    Stratified5Fold(clfs)\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2 一级学习器最优组合的寻找\n",
    "    ①一级学习器的训练和训练结果的保存；\n",
    "    ②基于第一层的结果遍历所有组合，探究不同组合的元学习效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c,clfs in enumerate(tre_clfs):\n",
    "    if c==0:\n",
    "        print('欠拟合组：')\n",
    "    elif c==1:\n",
    "        print('调参后组：')\n",
    "    else:\n",
    "        print('高参数组：')\n",
    "    '''\n",
    "        ①一级学习器的训练和训练结果的保存；\n",
    "    '''\n",
    "\n",
    "    # 创建一个空矩阵用于保存一级学习器的训练结果\n",
    "    X_train_stack  = np.zeros((X_train.shape[0], len(clfs)))\n",
    "    # 创建一个空矩阵用于保存一级学习器的测试结果\n",
    "    X_test_stack = np.zeros((X_test.shape[0], len(clfs))) \n",
    "\n",
    "    ### 5折stacking\n",
    "    n_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)#\n",
    "    aucs=[]\n",
    "    run_time=[]\n",
    "    ii=[]\n",
    "    for i,clf in enumerate(clfs):\n",
    "        print(\"分类器：{}\".format(clf))\n",
    "        tem_auc=0\n",
    "\n",
    "        X_stack_test_n = np.zeros((X_test.shape[0], n_folds))\n",
    "        tem_ii=0.0\n",
    "        for j,(train_index,test_index) in enumerate(skf.split(X_train,y_train)):\n",
    "            tr_x = X_train[train_index]\n",
    "            tr_y = y_train[train_index]\n",
    "\n",
    "            start = datetime.datetime.now()\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            end = datetime.datetime.now()\n",
    "            tem_ii+=(end-start).total_seconds()\n",
    "\n",
    "            #每得到一个模型就可以进行一次对测试集的预测\n",
    "            pred = clf.predict_proba(X_test)[:,1]\n",
    "            tem_auc+=roc_auc_score(y_test,pred)\n",
    "    #         show_auc(model,test_data,test_label,title_name)\n",
    "\n",
    "            #生成stacking训练数据集\n",
    "            X_train_stack [test_index, i] = clf.predict_proba(X_train[test_index])[:,1]#对验证集的预测\n",
    "            X_stack_test_n[:,j] = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "        ii.append(tem_ii/5)\n",
    "#         print('本次运行时间: ',ii)\n",
    "        aucs.append(tem_auc/5)\n",
    "#         print('本次AUC ',tem_auc/5)\n",
    "\n",
    "        #生成stacking测试数据集\n",
    "        X_test_stack[:,i] = X_stack_test_n.mean(axis=1) \n",
    "    print('ACU组= ',aucs,' 运行时间组= ',ii,end='\\n\\n')\n",
    "\n",
    "    '''\n",
    "        ②基于第一层的结果遍历所有组合，探究不同组合的元学习效果\n",
    "    '''\n",
    "\n",
    "    def meta_learn(x_stack,y_stack):\n",
    "        clf_second =LogisticRegression(solver=\"lbfgs\")\n",
    "        clf_second.fit(x_stack,y_train)\n",
    "        pred = clf_second.predict_proba(y_stack)[:,1]\n",
    "        tem_auc=roc_auc_score(y_test,pred)#得出预测结果\n",
    "        return tem_auc\n",
    "\n",
    "    t=[0.1,0.2,0.3,0.4]\n",
    "    aucs=[]#记录各组的成绩\n",
    "    id={}\n",
    "    for i in range(len(t)):\n",
    "        # 第一层\n",
    "        for j in range(i+1,len(t)):# 01、02、03、12、13、23\n",
    "\n",
    "            for k in range(j+1,len(t)):# 012 、013、123\n",
    "\n",
    "                for ii in range(k+1,len(t)):# 0123 \n",
    "                    #创建数据部分\n",
    "                    f=4\n",
    "                    x_stack=np.zeros((X_train.shape[0], f))#创建空矩阵，用于存储第一层的训练输出\n",
    "                    y_stack=np.zeros((X_test.shape[0], f))#创建空矩阵，用于存储第一层的测试输出\n",
    "                    tem_id=''\n",
    "                    for t1,t2 in enumerate([i,j,k,ii]):#更换矩阵对应的列即做到保存输出\n",
    "                        x_stack[:,t1]= X_train_stack[:,t2]\n",
    "                        y_stack[:,t1]=X_test_stack[:,t2]\n",
    "                        if(str(t2))=='0':\n",
    "                            t2='4'\n",
    "                        tem_id+=str(t2)\n",
    "                    #元学习器学习部分\n",
    "                    tem_auc=meta_learn(x_stack,y_stack)#得出预测结果\n",
    "                    aucs.append(tem_auc)\n",
    "                    id[tem_id]=tem_auc\n",
    "                    print(i,j,k,ii)\n",
    "\n",
    "                f=3\n",
    "                x_stack=np.zeros((X_train.shape[0], f))#创建空矩阵，用于存储第一层的训练输出\n",
    "                y_stack=np.zeros((X_test.shape[0], f))#创建空矩阵，用于存储第一层的测试输出\n",
    "                tem_id=''\n",
    "                for t1,t2 in enumerate([i,j,k]):#更换矩阵对应的列即做到保存输出\n",
    "                    x_stack[:,t1]= X_train_stack[:,t2]\n",
    "                    y_stack[:,t1]=X_test_stack[:,t2]\n",
    "                    if(str(t2))=='0':\n",
    "                        t2=4\n",
    "                    tem_id+=str(t2)\n",
    "                #元学习器学习部分\n",
    "                tem_auc=meta_learn(x_stack,y_stack)#得出预测结果\n",
    "                aucs.append(tem_auc)\n",
    "                id[tem_id]=tem_auc\n",
    "                print(i,j,k)\n",
    "\n",
    "            f=2\n",
    "            x_stack=np.zeros((X_train.shape[0], f))#创建空矩阵，用于存储第一层的训练输出\n",
    "            y_stack=np.zeros((X_test.shape[0], f))#创建空矩阵，用于存储第一层的测试输出\n",
    "            tem_id=''\n",
    "            for t1,t2 in enumerate([i,j]):#更换矩阵对应的列即做到保存输出\n",
    "                x_stack[:,t1]= X_train_stack[:,t2]\n",
    "                y_stack[:,t1]=X_test_stack[:,t2]\n",
    "                if(str(t2))=='0':\n",
    "                    t2='4'\n",
    "                tem_id+=str(t2)\n",
    "            #元学习器学习部分\n",
    "            tem_auc=meta_learn(x_stack,y_stack)#得出预测结果\n",
    "            aucs.append(tem_auc)\n",
    "            id[tem_id]=tem_auc \n",
    "            print(i,j)\n",
    "\n",
    "    print(id) \n",
    "    print('\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
