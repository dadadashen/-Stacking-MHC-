<!DOCTYPE html>
<!-- saved from url=(0038)https://www.jianshu.com/p/3c2dfd6e8e4e -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script type="text/javascript" async="" charset="utf-8" src="./一文入门sklearn二分类实战 - 简书_files/core.php"></script><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge,chrome=1"><meta http-equiv="Cache-Control" content="no-siteapp"><meta http-equiv="Cache-Control" content="no-transform"><meta name="applicable-device" content="pc,mobile"><meta name="MobileOptimized" content="width"><meta name="HandheldFriendly" content="true"><meta name="theme-color" content="#ec7259"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta name="google" value="notranslate"><meta property="wb:webmaster" content="294ec9de89e7fadb"><meta property="qc:admins" content="104102651453316562112116375"><meta property="qc:admins" content="11635613706305617"><meta property="qc:admins" content="1163561616621163056375"><meta name="360-site-verification" content="604a14b53c6b871206001285921e81d8"><meta name="google-site-verification" content="cV4-qkUJZR6gmFeajx_UyPe47GW9vY6cnCrYtCHYNh4"><meta name="google-site-verification" content="HF7lfF8YEGs1qtCE-kPml8Z469e2RHhGajy6JPVy5XI"><meta name="tencent-site-verification" content="da26ce22cfed7aba6a96d8409f9b53a6"><meta name="apple-mobile-web-app-title" content="简书"><link href="data:image/vnd.microsoft.icon;base64,AAABAAEAICAAAAEAIACoEAAAFgAAACgAAAAgAAAAQAAAAAEAIAAAAAAAABAAABILAAASCwAAAAAAAAAAAAAAAAAASWTtHEhh5qZIYObmSGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDm5khh5qZJZO0cAAAAAElk7RxIYeXtSGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hh5e1JZO0cSGHmpkhg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hh5qZIYObmSGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDm5khg5f9IYOX/SGDl/0hg5f9IYOX/ipnu/5qn8P9qfen/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/TWTl/5qn8P+grfH/nKnx/3iJ6/9JYeX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f/Cyvb///////T2/f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f99juz//////////////////////8vS9/9LY+X/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/8LK9v///////f3+/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/2l96f+hrfH/o6/x/9HX+P///////////5Gg7/9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/wsr2///////9/f7/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/Vmzn////////////usP1/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f/Cyvb///////39/v9IYOX/SGDl/46d7v/+/v7//v7+//7+/v/+/v7//v7+//7+/v/+/v7/8fP9/8LK9v9keOn/SGDl/0hg5f9JYeX///////////+9xvX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/8LK9v///////f3+/0hg5f9IYOX/j53v////////////ydD3/6ax8v+msfL/prHy/6248//t8Pz//////+/x/P9dcuj/SGDl/0lh5f///////////73G9f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/wsr2///////9/f7/SGDl/0hg5f+Pne////////////+RoO//SGDl/0hg5f9IYOX/SGDl/5Kg7////////////56q8f9IYOX/SWHl////////////vcb1/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f/Cyvb///////39/v9IYOX/SGDl/4+d7////////////5Oh7/9JYeX/SWHl/0lh5f9JYeX/h5fu////////////ucL1/0hg5f9JYeX///////////+9xvX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/8LK9v///////f3+/0hg5f9IYOX/j53v//////////////////////////////////////////////////////+4wfX/SGDl/0lh5f///////////73G9f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/wsr2///////9/f7/SGDl/0hg5f+Pne/////////////g5Pr/xs32/8bN9v/Gzfb/xs32/9jd+f///////////7fB9P9IYOX/SWHl////////////vcb1/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f/Cyvb///////39/v9IYOX/SGDl/4+d7////////////5uo8P9IYOX/SGDl/0hg5f9IYOX/gZHt////////////t8D0/0hg5f9JYeX///////////+9xvX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/8LK9v///////f3+/0hg5f9IYOX/j53v////////////m6jw/0hg5f9IYOX/SGDl/0hg5f+Bke3///////////+2wPT/SGDl/0lh5f///////////73G9f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/wsr2///////9/f7/SGDl/0hg5f+Pne///////////////////////////////////////////////////////7W/9P9IYOX/SWHl////////////vcb1/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f/Cyvb///////39/v9IYOX/SGDl/4WV7f/l6Pv/5ej7/+Xo+//l6Pv/5ej7/+Xo+//l6Pv/5ej7/+Xo+//l6Pv/pbHy/0hg5f9JYeX///////////+9xvX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/2h86f96i+z/eozs/7S+9P/K0ff/p7Ly/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0lh5f///////////73G9f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9gdej///////////+8xfX/bYDq/5il8P+YpfD/mKXw/5il8P+YpfD/mKXw/5il8P+YpfD/mKXw/5il8P+YpfD/mafw////////////vcb1/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/7zF9f//////9fb9/1906P/P1fj///////////////////////////////////////////////////////////////////////////+9xvX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/oK3x/1Rq5v9/kOz///////z9/v99juz/SGDl/6u28//AyPb/wMj2/8DI9v/K0ff/4eX6/8DI9v/AyPb/wMj2/8DI9v/AyPb/wMj2/8DI9v/AyPb/wMj2/5Wj8P9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f/8/f7/9fb9/6248/9Uaub/TWTl/0hg5f9/kOz/6Ov7/+Hl+v9ccuf/SGDl/3WH6///////vMX1/1Fo5v9IYOX/SGDl/0hg5f/L0vf/9/j9/8TL9v9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/6Ov8f/6+/7//////87U+P9PZub/SGDl/7rD9f//////9fb9/05l5f9IYOX/YXXo/+7w/P//////3eL6/1lu5/9IYOX/TWTl//f4/f//////rbjz/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/22A6v/09v3//////9fc+f+Zp/D/9vf9///////Y3fn/j53v/5Si7/+Wo/D/Znrp/93i+v//////3uL6/5mn8P+yvPT///////////+yvPT/mafw/5mn8P96i+z/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/32O7P////////////////////////////////////////////////+cqfH/X3To//r7/v////////////////////////////////////////////Hz/f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/8zT9////////////2l86f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/usP1////////////iJju/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/jJvu////////////kZ/v/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9/kOz///////////+eqvH/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5uZIYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9KYeX/SmHl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYObmSGHmpkhg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hh5qZJZO0cSGHl7Uhg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYeXtSWTtHAAAAABJZO0cSGHmpkhg5uZIYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYOX/SGDl/0hg5f9IYObmSGHmpklk7RwAAAAAgAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAAE=" rel="shortcut icon" type="image/x-icon"><link rel="dns-prefetch" href="https://cdn2.jianshu.io/"><link rel="dns-prefetch" href="https://upload-images.jianshu.io/"><meta http-equiv="mobile-agent" content="format=html5; url=https://www.jianshu.com/p/3c2dfd6e8e4e"><meta name="apple-itunes-app" content="app-id=888237539, app-argument=jianshu://notes/50429503"><meta property="al:ios:url" content="jianshu://notes/50429503"><meta property="al:ios:app_store_id" content="888237539"><meta property="al:ios:app_name" content="简书"><meta property="al:android:url" content="jianshu://notes/50429503"><meta property="al:android:package" content="com.jianshu.haruki"><meta property="al:android:app_name" content="简书"><title>一文入门sklearn二分类实战 - 简书</title><meta name="robots" content="index,follow"><meta name="googlebot" content="index,follow"><meta name="description" content="在小白我的第一篇文里就提出过一个问题，就是现在的教程都太“分散”太“板块”，每一个知识点都单独用一个例子，机器学习算法里也是这样的，可能逻辑回归用葡萄酒的案例讲，决策树又用鸢..."><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@jianshu.com"><meta property="fb:app_id" content="865829053512461"><meta property="og:url" content="https://www.jianshu.com/p/3c2dfd6e8e4e"><meta property="og:type" content="article"><meta property="og:title" content="一文入门sklearn二分类实战"><meta property="og:description" content="在小白我的第一篇文里就提出过一个问题，就是现在的教程都太“分散”太“板块”，每一个知识点都单独用一个例子，机器学习算法里也是这样的，可能逻辑回归用葡萄酒的案例讲，决策树又用鸢..."><meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18317213-599af8d696612c30.png"><meta property="og:site_name" content="简书"><script type="text/javascript" src="./一文入门sklearn二分类实战 - 简书_files/z_stat.php"></script><meta name="next-head-count" content="46"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/[slug].js" as="script"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/_app.js" as="script"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/webpack-f504f6266e52ee1a40ff.js" as="script"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/commons.f8bed0abd693d06a6ea3.js" as="script"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/styles.848aa67b1a6c6dddd4c2.js" as="script"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/main-550fe512e049aff0be03.js" as="script"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/commons.d40cf249.chunk.css" as="style"><link rel="stylesheet" href="./一文入门sklearn二分类实战 - 简书_files/commons.d40cf249.chunk.css"><link rel="preload" href="./一文入门sklearn二分类实战 - 简书_files/styles.1b0f30c8.chunk.css" as="style"><link rel="stylesheet" href="./一文入门sklearn二分类实战 - 简书_files/styles.1b0f30c8.chunk.css"><link rel="stylesheet" type="text/css" href="./一文入门sklearn二分类实战 - 简书_files/12.eb9b3ce2.chunk.css"><script charset="utf-8" src="./一文入门sklearn二分类实战 - 简书_files/12.98627683ee1c7fe319fe.js"></script><link rel="stylesheet" type="text/css" href="./一文入门sklearn二分类实战 - 简书_files/13.381d9ae9.chunk.css"><script charset="utf-8" src="./一文入门sklearn二分类实战 - 简书_files/13.2f908a0a2a391402a589.js"></script><script charset="utf-8" src="./一文入门sklearn二分类实战 - 简书_files/11.75b56346ed2db2849ee4.js"></script><link rel="stylesheet" type="text/css" href="./一文入门sklearn二分类实战 - 简书_files/10.df2f166d.chunk.css"><script charset="utf-8" src="./一文入门sklearn二分类实战 - 简书_files/10.837c58848681fa2956a8.js"></script><script charset="utf-8" src="./一文入门sklearn二分类实战 - 简书_files/15.bc317719308d3046905f.js"></script></head><body class=""><svg class="wCYvWN" style="display:none;width:0;height:0" width="0" height="0" focusable="false" aria-hidden="true"><symbol id="ic-icon_requests" viewBox="0 0 1024 1024"><path d="M934.4 627.2A38.4 38.4 0 0 1 972.8 665.6v128a140.8 140.8 0 0 1-140.8 140.8H192A140.8 140.8 0 0 1 51.2 793.6V665.6a38.4 38.4 0 1 1 76.8 0v128c0 35.328 28.672 64 64 64h640c35.328 0 64-28.672 64-64V665.6a38.4 38.4 0 0 1 38.4-38.4zM587.4688 91.392l281.8048 244.224a64 64 0 0 1-41.8816 112.384h-148.992V665.6a89.6 89.6 0 0 1-89.6 89.6h-153.6A89.6 89.6 0 0 1 345.6 665.6V448H196.608a64 64 0 0 1-41.8816-112.384l281.8048-244.224a115.2 115.2 0 0 1 150.9376 0zM486.8608 149.4016L230.912 371.2H384a38.4 38.4 0 0 1 38.4 38.4v256c0 7.0656 5.7344 12.8 12.8 12.8h153.6a12.8 12.8 0 0 0 12.8-12.8V409.6a38.4 38.4 0 0 1 38.4-38.4h153.088L537.088 149.4016a38.4 38.4 0 0 0-50.2784 0z"></path></symbol><symbol id="ic-icon_others" viewBox="0 0 1024 1024"><path d="M512 435.2a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z m-307.2 0a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z m614.4 0a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z"></path></symbol><symbol id="ic-icon_money" viewBox="0 0 1024 1024"><path d="M512 51.2a460.8 460.8 0 1 1 0 921.6 460.8 460.8 0 0 1 0-921.6z m0 76.8a384 384 0 1 0 0 768 384 384 0 0 0 0-768zM435.0976 311.3984L510.1568 450.56l75.264-139.52a37.7344 37.7344 0 0 1 49.664-16.0256 33.9456 33.9456 0 0 1 14.7968 47.104l-76.288 135.7824h102.4a29.0304 29.0304 0 1 1 0 58.0608h-127.0272v55.0912h126.976a29.0304 29.0304 0 1 1 0 58.0608h-126.976v49.152a38.8096 38.8096 0 1 1-77.6192 0v-49.152H345.2416a29.0304 29.0304 0 1 1 0-58.0608h126.1056v-55.0912H345.2416a29.0304 29.0304 0 0 1 0-58.0608h101.4272L369.0496 342.784a34.304 34.304 0 0 1 15.0016-48.0768 38.9632 38.9632 0 0 1 51.0464 16.6912z"></path></symbol><symbol id="ic-icon_follows" viewBox="0 0 1024 1024"><path d="M844.8 614.4a38.4 38.4 0 0 1 38.4 38.4v102.4h102.4a38.4 38.4 0 1 1 0 76.8h-102.4512l0.0512 102.4a38.4 38.4 0 1 1-76.8 0l-0.0512-102.4H704a38.4 38.4 0 1 1 0-76.8h102.4v-102.4a38.4 38.4 0 0 1 38.4-38.4z m-370.176-89.6c80.5888 0 158.3104 16.896 227.4816 48.64a38.4 38.4 0 1 1-32.0512 69.7856 468.2752 468.2752 0 0 0-195.3792-41.6256c-175.616 0-327.7312 93.184-378.112 223.9488-13.4144 38.5536-4.7616 57.5488 7.8336 57.5488H665.6a38.4 38.4 0 1 1 0 76.8H104.448c-74.24 0-109.1584-76.8-80.0256-160.768 62.8736-163.1232 244.3264-274.3296 450.2528-274.3296zM460.8 12.8a243.2 243.2 0 1 1 0 486.4 243.2 243.2 0 0 1 0-486.4z m0 76.8a166.4 166.4 0 1 0 0 332.8 166.4 166.4 0 0 0 0-332.8z"></path></symbol><symbol id="ic-icon_comments" viewBox="0 0 1024 1024"><path d="M537.6 51.2a435.2 435.2 0 1 1 0 870.4h-51.2a433.2544 433.2544 0 0 1-209.2544-53.504l-126.976 94.8224a51.2 51.2 0 0 1-81.8688-41.0624v-314.2144A435.2 435.2 0 0 1 486.4 51.2h51.2z m0 76.8h-51.2a358.4 358.4 0 0 0-344.32 458.24c2.048 6.9632 3.072 14.1824 3.072 21.4016v263.168l86.016-64.256a76.8 76.8 0 0 1 82.944-5.7856c52.3264 28.7744 111.104 44.032 172.288 44.032h51.2a358.4 358.4 0 0 0 0-716.8z m89.6 435.2a38.4 38.4 0 1 1 0 76.8h-230.4a38.4 38.4 0 1 1 0-76.8h230.4z m0-204.8a38.4 38.4 0 1 1 0 76.8h-230.4a38.4 38.4 0 0 1 0-76.8h230.4z"></path></symbol><symbol id="ic-icon_chat" viewBox="0 0 1024 1024"><path d="M870.4 153.6a102.4 102.4 0 0 1 102.4 102.4v512a102.4 102.4 0 0 1-102.4 102.4H153.6a102.4 102.4 0 0 1-102.4-102.4V256a102.4 102.4 0 0 1 102.4-102.4h716.8z m0 76.8H153.6a25.6 25.6 0 0 0-25.6 25.6v512a25.6 25.6 0 0 0 25.6 25.6h716.8a25.6 25.6 0 0 0 25.6-25.6V256a25.6 25.6 0 0 0-25.6-25.6z m-113.7664 97.3312a38.4 38.4 0 0 1 47.1552 60.5696l-274.5856 213.9136a38.4 38.4 0 0 1-47.2064 0L208.384 389.12a38.4 38.4 0 1 1 47.2064-60.6208l250.0096 194.7648z"></path></symbol><symbol id="ic-icon_collection" viewBox="0 0 1024 1024"><path d="M819.2 51.2a102.4 102.4 0 0 1 102.4 102.4v707.6864a76.8 76.8 0 0 1-104.192 71.7824L512 816.4864l-305.408 116.5824A76.8 76.8 0 0 1 102.4 861.2864V153.6a102.4 102.4 0 0 1 102.4-102.4h614.4z m0 76.8H204.8a25.6 25.6 0 0 0-25.6 25.6v707.6864l305.408-116.5312a76.8 76.8 0 0 1 54.784 0l305.408 116.5312V153.6a25.6 25.6 0 0 0-25.6-25.6z m-293.6832 105.984a25.6 25.6 0 0 1 8.192 8.192l63.744 102.2464 116.9408 28.9792a25.6 25.6 0 0 1 13.4144 41.3184L650.24 506.9312l8.5504 120.1664a25.6 25.6 0 0 1-35.1232 25.5488L512 607.3856l-111.616 45.2608a25.6 25.6 0 0 1-35.1744-25.5488L373.76 506.9312l-77.568-92.16a25.6 25.6 0 0 1 13.4144-41.3696l116.9408-28.9792L490.2912 242.176a25.6 25.6 0 0 1 35.2256-8.192zM512 352.512l-20.2752 32.512a76.8 76.8 0 0 1-46.6944 33.9456l-37.1712 9.216 24.6784 29.2864a76.8 76.8 0 0 1 17.8176 54.9376l-2.7136 38.1952 35.4816-14.3872a76.8 76.8 0 0 1 57.7536 0l35.4816 14.336-2.7136-38.144a76.8 76.8 0 0 1 17.8176-54.9376l24.6784-29.2864-37.1712-9.216a76.8 76.8 0 0 1-46.6944-33.9456L512 352.512z"></path></symbol><symbol id="ic-icon_help" viewBox="0 0 1024 1024"><path d="M512 51.2a460.8 460.8 0 1 1 0 921.6 460.8 460.8 0 0 1 0-921.6z m0 76.8a384 384 0 1 0 0 768 384 384 0 0 0 0-768zM512 716.8a51.2 51.2 0 1 1 0 102.4 51.2 51.2 0 0 1 0-102.4z m0-481.8944a166.4 166.4 0 0 1 166.4 166.4c0 46.848-26.7264 78.336-79.8208 116.992l-23.552 16.7936a299.008 299.008 0 0 0-18.0736 13.7216c-4.9664 4.1984-6.4 5.9392-6.5536 2.2528v89.088a38.4 38.4 0 1 1-76.8 0V547.84c0.512-23.1936 12.7488-39.936 33.9456-57.7536 6.2464-5.2736 13.1584-10.496 22.7328-17.408 1.2288-0.8704 18.2272-12.9024 23.1424-16.4864 33.9968-24.7296 48.1792-41.472 48.1792-54.8864a89.6 89.6 0 0 0-179.2 0 38.4 38.4 0 0 1-76.8 0A166.4 166.4 0 0 1 512 234.9056z"></path></symbol><symbol id="ic-icon_like" viewBox="0 0 1024 1024"><path d="M921.344 180.5312a269.3632 269.3632 0 0 1 0 377.2928l-372.8896 378.0096a51.2 51.2 0 0 1-72.9088 0l-372.8896-378.0096a269.3632 269.3632 0 0 1 0-377.2928 260.608 260.608 0 0 1 372.1216 0l37.2224 37.7344 37.2224-37.7344a260.608 260.608 0 0 1 372.1216 0zM157.3376 234.496a192.5632 192.5632 0 0 0 0 269.4144L512 863.4368l354.6624-359.5776a192.5632 192.5632 0 0 0 0-269.4144 183.808 183.808 0 0 0-262.7584 0L515.4304 324.096 400.128 443.5968a38.4 38.4 0 0 1-55.2448-53.3504L458.0864 272.896l-37.9904-38.5024a183.808 183.808 0 0 0-262.7584 0z"></path></symbol><symbol id="ic-icon_purchased" viewBox="0 0 1024 1024"><path d="M819.2 51.2a102.4 102.4 0 0 1 102.4 102.4v716.8a102.4 102.4 0 0 1-102.4 102.4H204.8a102.4 102.4 0 0 1-102.4-102.4V153.6a102.4 102.4 0 0 1 102.4-102.4h614.4z m0 76.8H204.8a25.6 25.6 0 0 0-25.6 25.6v716.8a25.6 25.6 0 0 0 25.6 25.6h614.4a25.6 25.6 0 0 0 25.6-25.6V153.6a25.6 25.6 0 0 0-25.6-25.6z m-67.1744 341.0944a38.4 38.4 0 0 1 10.3424 53.3504l-186.112 275.8656a38.4 38.4 0 0 1-53.2992 10.3936l-148.5824-100.1984a38.4 38.4 0 1 1 43.008-63.6928l116.736 78.7456 164.5568-244.0704a38.4 38.4 0 0 1 53.3504-10.3936zM473.6 435.2a38.4 38.4 0 0 1 0 76.8h-179.2a38.4 38.4 0 0 1 0-76.8h179.2z m102.4-179.2a38.4 38.4 0 1 1 0 76.8h-281.6a38.4 38.4 0 0 1 0-76.8h281.6z"></path></symbol><symbol id="ic-icon_logout" viewBox="0 0 1024 1024"><path d="M832 64c70.8096 0 128 58.0608 128 129.4336v45.5168a38.4 38.4 0 1 1-76.8 0v-45.568c0-29.184-23.04-52.5824-51.2-52.5824H243.2c-28.16 0-51.2 23.4496-51.2 52.6336v637.1328c0 29.184 23.04 52.6336 51.2 52.6336h588.8c28.16 0 51.2-23.4496 51.2-52.6336v-45.5168a38.4 38.4 0 0 1 76.8 0v45.568c0 71.3216-57.1904 129.3824-128 129.3824H243.2c-70.8096 0-128-58.0608-128-129.4336V193.4336c0-71.3728 57.1904-129.4336 128-129.4336z m-52.6336 248.064l160.8192 160.8192a38.4 38.4 0 0 1 2.1504 56.32l-162.9184 162.9184a38.4 38.4 0 0 1-54.272-54.272l97.3312-97.3824h-413.696c-13.9264 0-25.4464-14.2336-27.392-32.768l-0.3072-5.632c0-19.3024 10.24-35.2768 23.552-37.9904l4.096-0.4608 413.6448 0.0512-97.28-97.28a38.4 38.4 0 0 1 54.272-54.3232z"></path></symbol><symbol id="ic-icon_wallet" viewBox="0 0 1024 1024"><path d="M870.4 128a102.4 102.4 0 0 1 102.4 102.4v563.2a102.4 102.4 0 0 1-102.4 102.4H153.6a102.4 102.4 0 0 1-102.4-102.4v-563.2a102.4 102.4 0 0 1 102.4-102.4h716.8zM870.4 204.8H153.6a25.6 25.6 0 0 0-25.6 25.6V307.2H358.4a204.8 204.8 0 1 1 0 409.6H128v76.8a25.6 25.6 0 0 0 25.6 25.6h716.8a25.6 25.6 0 0 0 25.6-25.6v-563.2A25.6 25.6 0 0 0 870.4 204.8zM358.4 384H128v256H358.4a128 128 0 1 0 0-256z m-25.6 51.2a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z"></path></symbol><symbol id="ic-icon_mine" viewBox="0 0 1024 1024"><path d="M512.3072 549.4272h6.5024-7.168a588.0832 588.0832 0 0 1 28.16 0.6144l6.3488 0.4096c3.6352 0.1536 7.2704 0.4096 10.9056 0.6656l4.864 0.512c62.8736 5.2224 123.2384 20.6336 178.176 45.1584 1.536 0.7168 3.072 1.536 4.608 2.4064l1.8944 0.8704c100.352 46.6432 178.8416 122.6752 215.9616 216.32 29.184 81.664-5.8368 156.416-80.128 156.416l-178.176-0.0512-0.768 0.0512H141.568C67.328 972.8 32.256 898.048 61.44 816.384c37.12-93.6448 115.5584-169.6768 215.9616-216.32l1.9456-0.8704a39.168 39.168 0 0 1 4.608-2.4064c54.8864-24.576 115.2512-39.936 178.1248-45.2608l4.864-0.3584c3.584-0.3072 7.2704-0.5632 10.9056-0.768l6.3488-0.3072a671.5904 671.5904 0 0 1 28.1088-0.6656h-7.1168 6.5024z m370.176 348.672c12.5952 0 21.248-18.4832 7.8336-56.0128-47.0528-118.6304-182.7328-205.568-343.4496-216.7296a229.9392 229.9392 0 0 0-7.9872-0.512l7.9872 0.512a507.6992 507.6992 0 0 0-69.7344 0l7.9872-0.512c-164.1984 8.8576-303.616 96.6144-351.4368 217.2416-13.4144 37.5296-4.7616 55.9616 7.8848 55.9616h740.864zM498.3808 51.2c134.5024 0 243.5584 105.984 243.5584 236.6464 0 130.7136-109.056 236.6464-243.5584 236.6464S254.8736 418.56 254.8736 287.8464 363.9296 51.2 498.432 51.2z m0 74.752c-92.0064 0-166.656 72.4992-166.656 161.8944 0 89.4464 74.6496 161.9456 166.656 161.9456s166.6048-72.4992 166.6048-161.9456c0-89.3952-74.5984-161.8944-166.6048-161.8944z"></path></symbol><symbol id="ic-icon_setting" viewBox="0 0 1024 1024"><path d="M708.864 68.608a102.4 102.4 0 0 1 88.6784 51.2L994.4576 460.8a102.4 102.4 0 0 1 0 102.4l-196.9152 340.992a102.4 102.4 0 0 1-88.6784 51.2h-393.728a102.4 102.4 0 0 1-88.6784-51.2L29.5424 563.2a102.4 102.4 0 0 1 0-102.4l196.9152-340.992a102.4 102.4 0 0 1 88.6784-51.2h393.728z m0 76.8h-393.728a25.6 25.6 0 0 0-22.1696 12.8L96.0512 499.2a25.6 25.6 0 0 0 0 25.6l196.9152 340.992a25.6 25.6 0 0 0 22.1696 12.8h393.728a25.6 25.6 0 0 0 22.1696-12.8l196.9152-340.992a25.6 25.6 0 0 0 0-25.6l-196.9152-340.992a25.6 25.6 0 0 0-22.1696-12.8zM512 345.6a166.4 166.4 0 1 1 0 332.8 166.4 166.4 0 0 1 0-332.8z m0 76.8a89.6 89.6 0 1 0 0 179.2 89.6 89.6 0 0 0 0-179.2z"></path></symbol><symbol id="ic-spinner" viewBox="0 0 1024 1024"><path d="M300.571429 817.90476233q0 30.285714-21.428572 51.714285T227.428571 891.04761933q-29.714286 0-51.428571-21.714286t-21.714286-51.428571q0-30.285714 21.428572-51.714286T227.428571 744.76190433t51.714286 21.428572T300.571429 817.90476233z m284.571428 117.714285q0 30.285714-21.428571 51.714286T512 1008.76190433t-51.714286-21.428571T438.857143 935.61904733t21.428571-51.714285T512 862.47619033t51.714286 21.428572 21.428571 51.714285zM182.857143 533.33333333q0 30.285714-21.428572 51.714286T109.714286 606.47619033t-51.714286-21.428571T36.571429 533.33333333t21.428571-51.714286T109.714286 460.19047633t51.714285 21.428571T182.857143 533.33333333z m686.857143 284.571429q0 29.714286-21.714286 51.428571t-51.428571 21.714286q-30.285714 0-51.714286-21.428572T723.428571 817.90476233t21.428572-51.714286 51.714286-21.428572 51.714285 21.428572 21.428572 51.714286zM318.857143 248.76190433q0 37.714286-26.857143 64.571429t-64.571429 26.857143-64.571428-26.857143-26.857143-64.571429 26.857143-64.571428 64.571428-26.857143 64.571429 26.857143 26.857143 64.571428z m668.571428 284.571429q0 30.285714-21.428571 51.714286T914.285714 606.47619033t-51.714285-21.428571T841.142857 533.33333333t21.428572-51.714286T914.285714 460.19047633t51.714286 21.428571T987.428571 533.33333333z m-365.714285-402.285714q0 45.714286-32 77.714285t-77.714286 32-77.714286-32-32-77.714285 32-77.714286T512 21.33333333t77.714286 32 32 77.714286z m302.857143 117.714285q0 53.142857-37.714286 90.571429T796.571429 376.76190433q-53.142857 0-90.571429-37.428571T668.571429 248.76190433q0-52.571429 37.428571-90.285714t90.571429-37.714286q52.571429 0 90.285714 37.714286t37.714286 90.285714z"></path></symbol><symbol id="ic-alipay" viewBox="0 0 1024 1024"><path d="M1023.795 853.64v6.348a163.807 163.807 0 0 1-163.807 163.807h-696.18A163.807 163.807 0 0 1 0 859.988v-696.18A163.807 163.807 0 0 1 163.807 0h696.181a163.807 163.807 0 0 1 163.807 163.807V853.64z" fill="#009FE9"></path><path d="M844.836 648.267c-40.952-14.333-95.623-34.809-156.846-57.128a949.058 949.058 0 0 0 90.094-222.573H573.325V307.14h245.711v-43.41l-245.71 2.458V143.33H472.173c-18.223 0-21.704 20.476-21.704 20.476v102.38H204.759v40.952h245.71v61.427H245.712v40.952h409.518a805.522 805.522 0 0 1-64.909 148.246c-128.384-42.795-266.186-77.604-354.233-55.08a213.564 213.564 0 0 0-112.003 63.27c-95.418 116.917-26.21 294.034 175.274 294.034 119.989 0 236.087-67.366 325.771-177.73 134.322 65.932 398.666 176.297 398.666 176.297V701.3s-32.352-4.095-178.96-53.033z m-563.702 144.97c-158.893 0-204.759-124.699-126.336-194.112a191.86 191.86 0 0 1 90.913-46.276c93.575-10.238 189.811 35.629 293.624 86.614-74.941 94.598-166.674 153.774-258.2 153.774z" fill="#FFFFFF"></path></symbol><symbol id="ic-wechat-pay" viewBox="0 0 1076 1024"><path d="M410.493712 644.226288c-64.448471 36.97706-74.006881-20.759958-74.006881-20.759958l-80.772173-193.983933c-31.078562-92.178305 26.897497-41.56191 26.897498-41.561909s49.746372 38.732181 87.50193 62.333712c37.732946 23.602608 80.745253 6.927882 80.745254 6.927882l528.043743-250.842313C881.479874 81.578667 720.547129 0 538.352656 0 241.013636 0 0 217.098768 0 484.919453c0 154.046856 79.806318 291.154103 204.11518 380.019214L181.698086 997.56551s-10.92805 38.720336 26.945952 20.759958c25.808892-12.243853 91.603314-56.122953 130.768353-82.82771 61.570288 22.083298 128.651441 34.345455 198.970414 34.345455 297.315331 0 538.378498-217.098768 538.378499-484.924837 0-77.573115-20.313102-150.8338-56.295235-215.861568-168.236416 104.176656-559.545472 346.282128-609.973434 375.167327z" fill="#00cc22"></path></symbol><symbol id="ic-emoji" viewBox="0 0 1024 1024"><path d="M32 512C32 246.90332 246.90332 32 512 32 777.09668 32 992 246.90332 992 512 992 777.09668 777.09668 992 512 992 246.90332 992 32 777.09668 32 512ZM920 512C920 286.66782219 737.33217875 104 512 104 286.66782219 104 104 286.66782219 104 512 104 737.33217875 286.66782219 920 512 920 737.33217875 920 920 737.33217875 920 512ZM667.59732781 584C661.80988156 664.88360937 594.35802594 728.70452469 512 728.70452469 429.64197219 728.70452469 362.19011938 664.88360937 356.40267313 584 401.27447563 599.63467063 454.6818125 608.70452469 512 608.70452469 569.3181875 608.70452469 622.72552156 599.63467063 667.59732781 584L667.59732781 584ZM368 440C394.50966781 440 416 418.50966781 416 392 416 365.49033219 394.50966781 344 368 344 341.49033219 344 320 365.49033219 320 392 320 418.50966781 341.49033219 440 368 440ZM656 440C682.50966594 440 704 418.50966781 704 392 704 365.49033219 682.50966594 344 656 344 629.49033406 344 608 365.49033219 608 392 608 418.50966781 629.49033406 440 656 440Z"></path></symbol><symbol id="ic-dislike" viewBox="0 0 1137 1024"><path d="M771.413333 668.728889c-18.773333 3.015111-25.031111 20.878222-28.16 29.866667v217.884444c0 59.733333-49.948444 107.52-112.412444 107.52a115.427556 115.427556 0 0 1-112.412445-92.558222c-31.857778-190.919111-146.830222-263.850667-230.627555-290.133334a27.420444 27.420444 0 0 1-19.228445-26.168888V37.944889C268.572444 17.066667 285.582222 0 306.631111 0h567.864889c59.335111 11.946667 99.953778 32.824889 128 89.543111l128.113778 429.909333c24.974222 77.653333-15.644444 152.291556-106.211556 149.276445H771.413333z m-605.866666-32.824889H81.180444C37.546667 635.904 0 600.064 0 558.250667V80.611556C0 35.84 34.360889 0 81.180444 0H165.546667c29.297778 0 53.077333 23.779556 53.077333 53.077333v529.749334a53.077333 53.077333 0 0 1-53.077333 53.077333z"></path></symbol><symbol id="ic-close" viewBox="0 0 1024 1024"><path d="M511.99967832 371.66626953L792.60221182 91.06373692a99.19122714 99.19122714 0 0 1 140.33340878 140.33340878L652.33308711 511.99967832l280.60253349 280.6025335a99.19122714 99.19122714 0 1 1-140.33340878 140.33340878L511.99967832 652.33308711l-280.60253262 280.60253349a99.19122714 99.19122714 0 1 1-140.33340878-140.33340878L371.66626953 511.99967832 91.06373692 231.3971457A99.19122714 99.19122714 0 1 1 231.3971457 91.06373692L511.99967832 371.66626953z"></path></symbol><symbol id="ic-right" viewBox="0 0 1024 1024"><path d="M570.461091 506.693818L241.384727 177.524364A93.090909 93.090909 0 1 1 373.015273 45.893818L768 440.878545a93.090909 93.090909 0 0 1 0 131.630546l-394.984727 394.891636A93.090909 93.090909 0 0 1 241.384727 835.770182l329.076364-329.076364z"></path></symbol><symbol id="ic-more" viewBox="0 0 4096 1024"><path d="M3495.04988446 991.9952a481.91759063 481.91759063 0 0 1-483.83758031-479.9976c0-265.19867437 216.71891625-479.9976 483.83758031-479.9976S3978.64746665 246.79892562 3978.64746665 511.9976c0 264.95867531-216.47891719 479.9976-483.59758219 479.9976M2065.61703196 991.9952a481.91759063 481.91759063 0 0 1-483.35758312-479.9976c0-265.19867437 216.47891719-479.9976 483.35758312-479.9976 267.11866406 0 483.59758219 214.79892563 483.59758219 479.9976 0 264.95867531-216.47891719 479.9976-483.59758219 479.9976M622.26424884 991.9952A481.91759063 481.91759063 0 0 1 138.66666665 511.9976C138.66666665 246.79892562 355.14558384 32 622.26424884 32S1105.86183102 246.79892562 1105.86183102 511.9976c0 264.95867531-216.47891719 479.9976-483.59758218 479.9976"></path></symbol><symbol id="ic-toggle" viewBox="0 0 1024 1024"><path d="M350.366755 1023.926863a50.025855 50.025855 0 0 0 50.025855-50.025855V174.803795a73.137215 73.137215 0 0 0-124.47954-52.146834L36.096141 359.109577a50.830365 50.830365 0 0 0-0.731372 71.674471 50.537816 50.537816 0 0 0 71.601334 0.877647l193.301659-189.425388v731.737838c0 27.645867 22.453125 50.025855 50.025855 50.025855z"></path><path d="M720.953024 0.005851a47.246641 47.246641 0 0 0-47.246641 47.246641v799.389761a73.137215 73.137215 0 0 0 124.9915 51.634874l190.522446-191.619503a51.415462 51.415462 0 0 0 1.31647-71.089374 45.783897 45.783897 0 0 0-67.066827-0.292548l-155.343445 165.802066V47.252492A47.246641 47.246641 0 0 0 720.953024 0.005851z"></path></symbol><symbol id="ic-notebook" viewBox="0 0 1024 1024"><path d="M178.390055 120.591045C111.268624 120.591045 56.888889 174.401955 56.888889 240.556383V903.97778C56.888889 970.302855 111.097977 1024 178.390055 1024h545.731364c67.121431 0 121.558049-53.81091 121.558049-120.02222V240.613265c0-66.268192-54.209088-120.02222-121.558049-120.02222H178.390055z m455.117432 301.136319H269.06087a30.147761 30.147761 0 0 1 0-60.238641h364.503499a30.147761 30.147761 0 0 1 0 60.238641z m303.18409 301.136318a30.318409 30.318409 0 0 1-30.375291-30.318409V180.317742c0-66.268192-53.81091-120.02222-121.330519-120.022219H329.697688a30.147761 30.147761 0 0 1 0-60.23864l454.946784 0.056882C885.326618 0.113765 967.009987 80.887013 967.009987 180.602155v511.943118a30.318409 30.318409 0 0 1-30.31841 30.318409z m-303.18409-120.47728H269.06087a30.147761 30.147761 0 1 1 0-60.238641h364.503499a30.147761 30.147761 0 0 1 0 60.238641z"></path></symbol><symbol id="ic-check" viewBox="0 0 1433 1024"><path d="M586.88355555 719.072L1197.82755555 108.128a96 96 0 0 1 135.744 135.744L654.75555555 922.688a96 96 0 0 1-135.744 0L111.68355555 515.456A96 96 0 0 1 247.42755555 379.52l339.456 339.456z"></path></symbol><symbol id="ic-plus" viewBox="0 0 1024 1024"><path d="M437.00000029 437.00000029V136.99999971a74.99999971 74.99999971 0 1 1 149.99999942 0v300.00000058h300.00000058a74.99999971 74.99999971 0 0 1 0 149.99999942H586.99999971v300.00000058a74.99999971 74.99999971 0 0 1-149.99999942 0V586.99999971H136.99999971a74.99999971 74.99999971 0 1 1 0-149.99999942h300.00000058z"></path></symbol><symbol id="ic-pencil" viewBox="0 0 1092 1024"><path d="M597.469867 198.724267L81.92 718.574933 0 1024l309.4528-74.410667 512-521.762133-223.982933-229.102933z m400.861866-37.888L858.5216 17.885867a58.845867 58.845867 0 0 0-84.309333 0L657.2032 137.6256l223.914667 229.1712 117.1456-119.808a61.576533 61.576533 0 0 0 0-86.152533h0.068266zM546.133333 930.884267h546.133334V1024H546.133333v-93.115733z m273.066667-186.1632h273.066667v93.115733h-273.066667v-93.115733z"></path></symbol><symbol id="ic-sugar" viewBox="0 0 1024 1024"><path d="M562.362182 689.338182L352.581818 422.632727a182.737455 182.737455 0 0 1 109.056-85.876363l209.780364 266.612363a182.737455 182.737455 0 0 1-109.056 85.876364z m-28.811637 5.725091a183.296 183.296 0 0 1-193.349818-245.76l193.349818 245.76z m150.248728-118.225455l-193.349818-245.76a183.249455 183.249455 0 0 1 193.349818 245.76zM558.545455 282.065455l-40.215273-150.109091 151.738182-40.680728-13.544728-50.548363-202.333091 54.178909L503.156364 277.550545a235.659636 235.659636 0 0 0-199.493819 345.553455L120.925091 672.069818l13.544727 50.548364 200.704-53.76A236.683636 236.683636 0 0 0 465.454545 744.075636l40.215273 150.155637-151.738182 40.634182 13.544728 50.594909 202.333091-54.225455-48.965819-182.690909a235.613091 235.613091 0 0 0 199.493819-345.553455l182.690909-48.919272-13.544728-50.594909-200.704 53.76A236.683636 236.683636 0 0 0 558.545455 282.065455z m126.882909-32.256l126.464-33.885091-13.544728-50.594909-101.189818 27.136-27.089454-101.189819-50.594909 13.544728 40.680727 151.738181 25.274182-6.74909zM338.618182 776.378182l-126.464 33.885091 13.544727 50.548363 101.189818-27.089454 27.089455 101.143273 50.594909-13.498182-40.680727-151.738182-25.274182 6.749091z m-153.6-67.304727l-50.548364 13.544727 40.680727 151.738182 50.548364-13.498182-40.634182-151.738182z m653.963636-392.052364l50.548364-13.544727-40.680727-151.738182-50.548364 13.544727 40.634182 151.738182z"></path></symbol><symbol id="ic-like" viewBox="0 0 1084 1024"><path d="M728.064 343.943529c-17.648941-2.891294-23.552-20.239059-26.503529-28.912941V104.026353C701.560471 46.200471 654.396235 0 595.425882 0c-53.007059 0-97.28 40.478118-106.134588 89.569882-29.997176 184.862118-138.541176 255.457882-217.630118 280.937412a26.142118 26.142118 0 0 0-18.130823 24.877177v560.067764c0 19.817412 16.022588 35.84 35.84 35.84h535.973647c56.018824-11.565176 94.328471-31.804235 120.892235-86.738823l120.832-416.105412c23.552-75.173647-14.757647-147.395765-100.231529-144.564706h-238.772706z m-571.813647 31.744H76.619294C35.358118 375.687529 0 410.383059 0 450.861176v462.426353c0 43.369412 32.406588 78.004706 76.619294 78.004706h79.631059c27.708235 0 50.115765-22.407529 50.115765-50.115764V425.863529a50.115765 50.115765 0 0 0-50.115765-50.115764z"></path></symbol><symbol id="ic-reply" viewBox="0 0 1092 1024"><path d="M173.24799969 781.568C95.168 781.568 32 723.07200031 32 650.94399969V162.56C32 90.49599969 95.04000031 32 172.99200031 32h742.01599969C992.96 32 1056.00000031 90.49599969 1056.00000031 162.62400031v488.31999938c0 72.06400031-63.36 130.62400031-141.12 130.62400031h-343.68l-287.55200062 196.224a12.79999969 12.79999969 0 0 1-19.968-10.87999969l5.63200031-185.34400031H173.24799969z"></path></symbol><symbol id="ic-shang" viewBox="0 0 1024 1024"><path d="M827.512471 177.88486233c35.056941 0 61.982118 8.914824 79.028705 26.684236 17.950118 17.769412 26.985412 44.453647 26.985412 80.052706 0 17.769412-3.614118 43.610353-10.842353 77.402353-3.553882 17.769412-14.336 65.776941-50.236235 65.776941a48.549647 48.549647 0 0 1-35.056941-15.058824 39.273412 39.273412 0 0 1-3.614118-48.971294c9.938824-30.238118 15.299765-56.922353 15.299765-77.402353 0-26.684235-7.228235-26.684235-13.492706-26.684235H189.861647c-0.843294 0-1.807059 0-3.553882 1.807059-1.807059 1.807059-1.807059 2.650353-1.807059 4.457411v108.483765c0 17.829647-11.685647 39.152941-43.971765 39.152941-31.442824 0-43.128471-21.323294-43.12847-39.152941v-110.230588c0-29.394824 8.071529-50.718118 25.118117-65.837177 16.203294-14.275765 38.610824-21.383529 70.053647-21.383529h54.814118a267.685647 267.685647 0 0 0-27.828706-33.731765c-19.757176-22.287059-14.396235-40.96-9.878588-50.778353 8.071529-15.058824 21.504-23.973647 36.743529-23.973647a43.369412 43.369412 0 0 1 30.59953 13.312c3.614118 3.614118 8.071529 8.914824 13.43247 16.022589 5.421176 7.107765 11.685647 16.022588 18.913883 26.684235 11.625412 17.769412 22.407529 35.538824 32.286117 53.36847h128.421647V75.60533333c0-17.769412 10.842353-39.152941 43.128471-39.152941 32.346353 0 43.971765 21.383529 43.971765 39.152941v102.279529h136.553411c7.168-10.661647 15.239529-21.323294 22.467765-32.88847 9.818353-15.119059 19.696941-32.045176 29.635765-48.971294a45.537882 45.537882 0 0 1 22.407529-23.973647 40.357647 40.357647 0 0 1 30.539294-0.90353c7.228235 2.650353 13.492706 7.107765 18.853647 12.468706a35.659294 35.659294 0 0 1 11.685647 25.780706c0 11.565176-4.457412 27.587765-34.093176 68.487529h32.286118z m-532.540236 341.534118a46.622118 46.622118 0 0 1-30.539294-16.022588 57.825882 57.825882 0 0 1-16.143059-31.081412 169.381647 169.381647 0 0 1-3.614117-40.056471V388.76862733c0-15.119059 1.807059-27.587765 3.614117-37.345882 2.650353-12.468706 8.071529-22.287059 15.23953-30.298353 7.228235-8.854588 17.950118-14.215529 31.442823-15.962353 9.878588-0.903529 19.757176-1.807059 32.346353-1.807059h367.314824c30.539294 0 52.043294 8.011294 65.536 23.130353 11.685647 14.215529 17.950118 35.538824 17.950117 63.126588v46.260706c0 29.394824-6.264471 50.718118-20.660705 64.933647-13.432471 13.372235-34.093176 20.48-62.825412 20.48H326.415059c-12.589176 0-22.467765 0-31.442824-1.807059z m32.346353-129.867294h-0.903529v43.610353c0 5.360941 0.903529 7.107765 1.807059 8.011294h366.411294c-0.903529 0 0-2.650353 0-7.107765v-49.814588H327.318588v5.300706z m577.41553 527.480471c15.299765 11.565176 23.371294 24.877176 19.757176 39.152941 0 16.865882-9.878588 31.984941-26.925176 38.189176a36.743529 36.743529 0 0 1-17.046589 3.614118 64.451765 64.451765 0 0 1-31.442823-9.818353 2255.149176 2255.149176 0 0 0-139.14353-74.691765 2297.976471 2297.976471 0 0 0-155.407058-71.198117c-8.071529-4.397176-33.249882-15.058824-33.249883-39.152942 0-0.843294 0.903529-1.746824 0.90353-3.493647-5.421176 13.312-12.589176 26.684235-19.757177 39.152941-30.539294 53.308235-91.557647 93.364706-180.525176 119.145412-84.389647 24.937412-144.564706 37.345882-184.982588 37.345883-34.153412 0-45.778824-22.226824-45.778824-41.803295 0-8.854588 3.614118-36.442353 44.875294-44.453647 86.196706-11.565176 152.696471-26.684235 199.378824-43.610353 43.971765-16.865882 76.318118-39.152941 96.075294-66.68047 19.757176-27.587765 30.539294-71.137882 31.442823-129.867294 0-19.576471 9.035294-33.792 24.274824-39.996236H272.504471c-6.264471 0-8.975059 0-8.975059 14.21553v176.128c0 24.877176-17.046588 41.803294-43.068236 41.803294-25.178353 0-42.224941-16.022588-42.224941-41.803294v-184.139294c0-29.334588 8.071529-51.621647 23.311059-66.740706 15.299765-15.058824 37.767529-22.226824 68.306824-22.226824h489.411764c33.249882 0 55.717647 7.107765 70.053647 21.38353 13.492706 14.215529 20.660706 36.442353 20.660706 67.584v180.525176c0 24.937412-17.046588 41.803294-43.12847 41.803294-25.118118 0-42.164706-15.962353-42.164706-41.803294v-170.767059a31.503059 31.503059 0 0 0-1.807059-13.312c-1.807059-0.903529-3.614118-0.903529-7.228235-0.903529H524.890353c15.239529 6.204235 25.118118 20.48 25.118118 40.056471 0 37.345882-6.264471 72.884706-17.950118 107.580235 0.903529-0.843294 0.903529-1.807059 1.807059-2.650353 4.457412-8.914824 16.143059-15.119059 29.635764-15.119059 6.264471 0 15.239529 1.807059 54.814118 18.672941l98.785882 46.260706c20.600471 9.758118 40.357647 19.576471 60.114824 28.431059 20.660706 8.914824 40.417882 18.672941 59.271529 28.491294 19.757176 9.758118 35.056941 17.769412 45.778824 23.130353 9.938824 5.300706 16.263529 8.914824 18.913882 9.758118l3.614118 1.807059z"></path></symbol><symbol id="ic-diamond" viewBox="0 0 1026 1024"><path d="M751.144277 307.2l-123.016533-238.933333h159.778133a81.92 81.92 0 0 1 59.1872 25.258666l160.256 167.492267A27.306667 27.306667 0 0 1 987.620011 307.2h-236.475734z m270.506667 111.547733L640.927744 946.039467a27.306667 27.306667 0 0 1-48.128-24.234667L766.504277 375.466667h-56.388266l-170.5984 590.165333a27.306667 27.306667 0 0 1-52.462934 0.034133L315.500544 375.466667H259.112277l174.523734 545.5872a27.306667 27.306667 0 0 1-48.128 24.302933L5.160277 418.747733A27.306667 27.306667 0 0 1 27.346944 375.466667H999.464277a27.306667 27.306667 0 0 1 22.152534 43.281066zM18.301611 261.0176L178.557611 93.525333A81.92 81.92 0 0 1 237.744811 68.266667h159.744L274.506411 307.2H38.030677a27.306667 27.306667 0 0 1-19.729066-46.1824zM453.877077 68.266667h117.896534l122.9824 238.933333H330.894677l122.9824-238.933333z"></path></symbol><symbol id="ic-money" viewBox="0 0 1024 1024"><path d="M884.363636 512C884.363636 306.349242 717.650758 139.636364 512 139.636364 306.349242 139.636364 139.636364 306.349242 139.636364 512 139.636364 717.650758 306.349242 884.363636 512 884.363636 717.650758 884.363636 884.363636 717.650758 884.363636 512ZM46.545455 512C46.545455 254.936553 254.936553 46.545455 512 46.545455 769.063447 46.545455 977.454545 254.936553 977.454545 512 977.454545 769.063447 769.063447 977.454545 512 977.454545 254.936553 977.454545 46.545455 769.063447 46.545455 512ZM470.626262 520.334527 346.368469 520.334527C335.022727 520.334527 325.818182 511.082366 325.818182 499.669243L325.818182 478.939206C325.818182 467.431177 335.018859 458.273923 346.368469 458.273923L459.41918 458.273923 362.752701 343.071299C355.558764 334.497899 356.572921 321.312715 365.315879 313.976502L381.196011 300.65149C390.011671 293.254272 402.96653 294.447111 410.293884 303.179511L512.303444 424.749773 614.313007 303.179511C621.64036 294.447111 634.595221 293.254272 643.410879 300.65149L659.29101 313.976502C668.033969 321.312715 669.048129 334.497899 661.854189 343.071299L565.18771 458.273923 677.63153 458.273923C688.977273 458.273923 698.181818 467.526083 698.181818 478.939206L698.181818 499.669243C698.181818 511.177272 688.981141 520.334527 677.63153 520.334527L553.373738 520.334527 553.373738 582.395136 677.63153 582.395136C688.977273 582.395136 698.181818 591.647297 698.181818 603.060419L698.181818 623.790457C698.181818 635.298486 688.981141 644.455741 677.63153 644.455741L553.373738 644.455741 553.373738 737.562275C553.373738 748.871415 544.025139 758.175739 532.493056 758.175739L491.506944 758.175739C479.797853 758.175739 470.626262 748.946776 470.626262 737.562275L470.626262 644.455741 346.368469 644.455741C335.022727 644.455741 325.818182 635.203579 325.818182 623.790457L325.818182 603.060419C325.818182 591.552391 335.018859 582.395136 346.368469 582.395136L470.626262 582.395136 470.626262 520.334527Z"></path></symbol><symbol id="ic-others" viewBox="0 0 1024 1024"><path d="M232.727273 579.87878833C271.28679 579.87878833 302.545455 548.62012233 302.545455 510.06060633 302.545455 471.50108933 271.28679 440.24242433 232.727273 440.24242433 194.167756 440.24242433 162.909091 471.50108933 162.909091 510.06060633 162.909091 548.62012233 194.167756 579.87878833 232.727273 579.87878833ZM512 579.87878833C550.559516 579.87878833 581.818182 548.62012233 581.818182 510.06060633 581.818182 471.50108933 550.559516 440.24242433 512 440.24242433 473.440484 440.24242433 442.181818 471.50108933 442.181818 510.06060633 442.181818 548.62012233 473.440484 579.87878833 512 579.87878833ZM791.272727 579.87878833C829.832243 579.87878833 861.090909 548.62012233 861.090909 510.06060633 861.090909 471.50108933 829.832243 440.24242433 791.272727 440.24242433 752.713211 440.24242433 721.454545 471.50108933 721.454545 510.06060633 721.454545 548.62012233 752.713211 579.87878833 791.272727 579.87878833Z"></path></symbol><symbol id="ic-requests" viewBox="0 0 1024 1024"><path d="M418.909091 372.363636 418.909091 698.181818 605.090909 698.181818 605.090909 372.363636 696.24367 372.363638 511.582813 151.061383 326.921959 372.363638 418.909091 372.363636ZM325.818182 791.272727 325.818182 465.454545 139.636364 465.454545 512 10.219745 884.363636 465.454545 698.181818 465.454545 698.181818 791.272727 325.818182 791.272727ZM791.549193 930.909091C842.809195 930.909091 884.363636 889.589313 884.363636 837.818182L884.363636 744.727273 977.454545 744.727273 977.454545 884.363636C977.454545 961.482668 914.929478 1024 838.088048 1024L185.911951 1024C108.94196 1024 46.545455 961.32575 46.545455 884.363636L46.545455 744.727273 139.636364 744.727273 139.636364 837.818182C139.636364 889.230871 181.271913 930.909091 232.450806 930.909091L791.549193 930.909091Z"></path></symbol><symbol id="ic-follows" viewBox="0 0 1024 1024"><path d="M742.39037 893.84038533C732.980703 903.25005233 718.025272 903.19522133 708.767223 893.93717233L565.458115 753.71907433C556.207048 744.46800633 556.2434 729.40742933 565.554902 720.09592233L605.034543 682.83743133C614.444209 673.42776433 629.399645 673.48259533 638.657694 682.74064433L728.38723 766.49557833 941.491782 540.05939833C950.503205 531.04797933 965.609342 531.03876833 974.920848 540.35027433L1010.521307 576.49106033C1019.930973 585.90072733 1019.963043 600.76926333 1010.812183 609.92012233L742.39037 893.84038533ZM139.636364 859.15151533C139.636364 859.15151533 140.67734 830.50210333 143.721266 818.93653533 149.029303 798.76835333 158.415813 778.50853133 172.872966 759.57276033 185.195819 743.43246833 200.763606 728.89474933 220.104544 716.30744833 248.816792 697.62118933 341.214879 652.42733033 332.078782 657.40617733 390.138425 625.76575033 412.212292 582.79679733 388.559923 518.84814133 384.958515 509.11105133 363.507092 452.78421333 358.209139 437.93037933 342.817466 394.77685333 333.272371 357.97333033 324.783144 304.62989533 312.248334 225.86538933 369.770254 160.96969733 466.26749 160.96969733 571.044971 160.96969733 629.910798 218.91014833 619.986167 306.71758733 613.807085 361.38656133 604.889014 392.41438433 587.2357 428.78513033 587.680116 427.86951433 552.225592 496.14089933 539.29385 524.72497933L624.108763 563.09615833C635.7853 537.28656933 670.418623 470.59648633 670.983024 469.43366333 693.277635 423.50053933 705.13172 382.25771733 712.48809 317.17282433 728.985321 171.21479033 623.998925 67.87878833 466.26749 67.87878833 313.335459 67.87878833 211.18413 183.12496033 232.849136 319.26053733 242.258212 378.38399033 253.244797 420.74554033 270.528472 469.20365633 276.225846 485.17734633 297.956064 542.23624033 301.249676 551.14114433 307.065226 566.86458533 308.380666 564.30394833 287.532892 575.66524233 282.015993 578.67175233 275.594213 581.88642833 265.97112 586.50462633 270.601051 584.28268633 245.145458 596.43114733 238.070951 599.89143433 212.184458 612.55306833 190.997437 624.18127233 169.326586 638.28490833 140.984774 656.73008533 117.593202 678.57391433 98.881863 703.08175233 60.377931 753.51364533 47.027784 804.23828133 46.549412 840.92027533L46.545455 947.98348733 587.235705 947.98345933 512 859.15151533 139.636364 859.15151533Z"></path></symbol><symbol id="ic-chats" viewBox="0 0 1024 1024"><path d="M124.121212 139.636364C88.436364 139.636364 47.010909 181.527273 46.545455 217.212121L46.545455 799.030305C46.545455 842.472727 88.436364 884.363636 124.121212 876.606059L899.878786 876.606059C935.563636 884.363636 977.454545 842.472727 977.454545 799.030305L977.454545 217.212121C977.454545 181.527273 935.563636 139.636364 899.878786 139.636364L124.121212 139.636364ZM512 473.016869 139.636364 232.727273 884.363636 232.727273 512 473.016869ZM139.636364 791.272727 139.636364 331.612007 512 578.515503 884.363636 331.612007 884.363636 791.272727 139.636364 791.272727Z"></path></symbol><symbol id="ic-comments" viewBox="0 0 1024 1024"><path d="M977.454545 164.91403167C977.454545 113.60050467 935.731963 71.75757567 884.264546 71.75757567L139.735452 71.75757567C88.181004 71.75757567 46.545455 113.46514167 46.545455 164.91403167L46.545455 676.78293967C46.545455 728.09646367 88.22568 769.93939367 139.640844 769.93939367L186.181818 769.93939367 186.181818 956.12121167 512 769.93939367 884.524167 769.93939367C935.858506 769.93939367 977.454545 728.23182767 977.454545 676.78293967L977.454545 164.91403167ZM884.363636 164.84848467L884.363636 676.84848467 502.393986 676.84848467 279.272727 769.93939367 279.272727 676.84848467 139.636364 676.84848467 139.636364 164.84848467 884.363636 164.84848467Z"></path></symbol><symbol id="ic-likes" viewBox="0 0 1024 1024"><path d="M511.646501 852.318427C513.3925 850.741015 516.884503 847.586202 516.884503 847.586202 738.668074 646.043071 808.239081 574.380446 853.64177 489.88787 874.584837 450.913673 884.363636 415.390578 884.363636 379.345455 884.363636 287.398144 813.401856 216.436364 721.454545 216.436364 669.217853 216.436364 616.89613 241.028421 582.874945 280.979901L512 364.209197 441.125053 280.979901C407.103871 241.028421 354.782148 216.436364 302.545455 216.436364 210.598144 216.436364 139.636364 287.398144 139.636364 379.345455 139.636364 415.547805 149.501383 451.227391 170.635978 490.39044 216.182926 574.790321 286.220326 646.813794 507.042118 847.054141 507.042118 847.054141 490.96233 871.005342 511.646501 852.318427ZM512 220.625455 578.083025 164.351628C620.609936 138.33686 670.384463 123.345455 721.454545 123.345455 864.814545 123.345455 977.454545 235.985455 977.454545 379.345455 977.454545 555.287273 819.2 698.647273 579.490909 916.48L512 977.454545 444.509091 916.014545C204.8 698.647273 46.545455 555.287273 46.545455 379.345455 46.545455 235.985455 159.185455 123.345455 302.545455 123.345455 353.615536 123.345455 403.390064 138.33686 445.916975 164.351628L512 220.625455Z"></path></symbol><symbol id="ic-nav-mode" viewBox="0 0 1024 1024"><path d="M194.56 597.333333l-64.853333 166.4c-2.858667 9.088-3.413333 15.786667-1.706667 20.053334 1.706667 4.266667 10.24 6.954667 25.6 8.106666l56.32 4.266667v24.746667a521.557333 521.557333 0 0 0-34.56-1.28c-17.365333-0.298667-40.832-0.426667-70.4-0.426667s-53.034667 0.128-70.4 0.426667c-17.365333 0.298667-28.885333 0.725333-34.56 1.28v-24.746667l53.76-4.266667c18.773333-1.706667 31.872-12.245333 39.253333-31.573333L292.693333 256h41.813334l188.586666 498.346667c3.968 9.685333 7.68 16.213333 11.093334 19.626666 3.413333 3.413333 9.685333 6.528 18.773333 9.386667l46.933333 14.506667v23.893333a1946.538667 1946.538667 0 0 0-99.84-2.56h-71.68c-45.525333 0-82.773333 0.853333-111.786666 2.56v-23.893333l53.76-8.533334c16.512-2.858667 25.898667-6.272 28.16-10.24 2.261333-3.968 1.152-11.648-3.413334-23.04L336.213333 597.333333H194.56z m127.146667-39.253333L271.36 398.506667l-61.44 159.573333h111.786667z m427.52 273.92c-23.338667 0-43.392-4.138667-60.16-12.373333a110.805333 110.805333 0 0 1-40.96-33.706667 146.816 146.816 0 0 1-23.466667-50.346667c-5.12-19.328-7.68-40.106667-7.68-62.293333 0-30.165333 4.565333-56.874667 13.653333-80.213333 9.088-23.338667 21.333333-42.965333 36.693334-58.88a153.6 153.6 0 0 1 53.76-36.266667 171.946667 171.946667 0 0 1 64.853333-12.373333c15.914667 0 29.866667 1.152 41.813333 3.413333 11.946667 2.261333 21.888 4.821333 29.866667 7.68 9.685333 2.858667 17.621333 6.272 23.893333 10.24l56.32-31.573333 7.68 2.56v262.826666c0 22.741333 9.685333 33.578667 29.013334 32.426667 4.565333 0 8.106667-0.725333 10.666666-2.133333s4.992-2.688 7.253334-3.84c2.261333-1.706667 4.266667-3.413333 5.973333-5.12l5.12 11.946666c-4.565333 10.794667-10.538667 20.48-17.92 29.013334-6.272 7.381333-14.634667 14.08-25.173333 20.053333-10.538667 5.973333-23.466667 8.96-38.826667 8.96-25.045333 0-42.794667-7.552-53.333333-22.613333-10.538667-15.061333-15.786667-33.408-15.786667-55.04v-20.48a181.888 181.888 0 0 1-12.373333 36.266666 129.92 129.92 0 0 1-20.053334 31.146667 92.373333 92.373333 0 0 1-29.44 22.186667 93.44 93.44 0 0 1-41.386666 8.533333z m103.253333-196.266667c0-11.946667-0.554667-24.448-1.706667-37.546666a143.872 143.872 0 0 0-7.68-36.266667 66.858667 66.858667 0 0 0-18.346666-27.733333c-8.234667-7.381333-19.498667-11.093333-33.706667-11.093334-7.978667 0-16.341333 2.133333-25.173333 6.4-8.832 4.266667-16.938667 11.221333-24.32 20.906667-7.381333 9.685333-13.525333 22.912-18.346667 39.68-4.821333 16.768-7.253333 37.418667-7.253333 61.866667 0 17.066667 1.28 33.28 3.84 48.64 2.56 15.36 6.698667 28.714667 12.373333 40.106666 5.674667 11.392 12.672 20.352 20.906667 26.88 8.234667 6.528 18.048 9.813333 29.44 9.813334 11.392 0 21.333333-3.712 29.866666-11.093334 8.533333-7.381333 15.786667-17.194667 21.76-29.44 5.973333-12.245333 10.538667-26.325333 13.653334-42.24 3.114667-15.914667 4.693333-32.426667 4.693333-49.493333v-9.386667z"></path></symbol><symbol id="ic-write" viewBox="0 0 1024 1024"><path d="M151.007 942.26766666c-22.993 21.261-64.594 64.683-43.381-35.512 37.218-163.524 447.875-792.517 794.003-852.861 97.053 0-180.459 232.311-180.459 232.311 0 0 95.338 11.846 157.962-54.296-37.533 195.818-214.91 195.149-260.719 210.729 53.59 7.89 93.441 31.594 176.379 7.368-20.222 55.519-100.075 123.839-387.175 191.454-126.92 45.617-233.617 279.547-256.61 300.808z"></path></symbol><symbol id="ic-night" viewBox="0 0 1024 1024"><path d="M386.573124 0C340.147815 173.910685 384.73085 366.612567 521.059141 502.940859 657.387432 639.26915 850.089314 683.852184 1024 637.426876 1001.155802 722.908399 956.572765 804.336916 889.51398 871.395701 686.126909 1074.782772 355.991373 1074.782772 152.6043 871.395701-50.782772 668.00863-50.782772 337.87309 152.6043 134.486017 219.663081 67.427236 301.091602 22.8442 386.573124 0Z"></path></symbol><symbol id="ic-mark" viewBox="0 0 1024 1024"><path d="M268.190476 113.777778C214.552381 113.777778 171.154286 155.291291 171.154286 206.03003L170.666667 967.111111 512 828.732735 853.333333 967.111111 853.333333 206.03003C853.333333 155.291291 809.447617 113.777778 755.809525 113.777778L268.190476 113.777778Z"></path></symbol><symbol id="ic-user" viewBox="0 0 1081 1024"><path d="M803.557507 705.46529267C921.627438 734.68811367 1009.013675 841.17086467 1009.013675 968.09948167L1009.013675 999.65574867 113.777778 999.65574867 113.777778 968.09948167C113.777778 845.98765767 194.87339 742.62628167 306.138642 709.26557267 386.042902 689.94662967 446.772745 624.07875167 455.097973 555.06783567 413.092784 526.09210167 380.671395 482.61667367 369.546671 434.01441967L331.247877 266.69273867C301.999138 138.90939267 384.447284 35.55555567 515.334601 35.55555567L604.020878 35.55555567C735.179184 35.55555567 819.313243 139.39951167 792.544825 267.49752267L757.903491 433.27071867C747.227432 484.36012067 713.120956 529.80140967 668.826761 558.82664367 675.211167 624.51970267 729.457687 683.91321067 803.557507 705.46529267Z"></path></symbol><symbol id="ic-setting" viewBox="0 0 1024 1024"><path d="M846.327484 515.083444C846.327484 499.504829 844.9529 484.842604 843.120122 470.180378L939.799171 394.578276C948.504866 387.705358 950.795839 375.334105 945.297505 365.253825L853.658596 206.718508C848.160262 196.638228 835.78901 192.972672 825.708726 196.638228L711.618281 242.457684C687.792168 224.129902 662.133271 209.009481 634.183401 197.554617L616.772011 76.13306C615.397427 65.136391 605.775343 56.888889 594.320475 56.888889L411.042655 56.888889C399.587791 56.888889 389.965705 65.136391 388.591122 76.13306L371.179729 197.554617C343.22986 209.009481 317.570966 224.588096 293.744849 242.457684L179.654404 196.638228C169.11593 192.514478 157.202871 196.638228 151.704536 206.718508L60.065625 365.253825C54.109096 375.334105 56.858263 387.705358 65.56396 394.578276L162.243011 470.180378C160.410233 484.842604 159.035649 499.963024 159.035649 515.083444 159.035649 530.203865 160.410233 545.324285 162.243011 559.986511L65.56396 635.588614C56.858263 642.461531 54.567291 654.832782 60.065625 664.913067L151.704536 823.448383C157.202871 833.528661 169.574124 837.194217 179.654404 833.528661L293.744849 787.709207C317.570966 806.036986 343.22986 821.15741 371.179729 832.612272L388.591131 947.866943C389.965715 958.86361 399.587801 967.111111 411.042665 967.111111L594.320486 967.111111C605.775349 967.111111 615.397439 958.86361 616.772022 947.866943L634.183401 832.612272C662.133271 821.15741 687.792168 805.578792 711.618281 787.709207L825.708726 833.528661C836.247205 837.652412 848.160262 833.528661 853.658596 823.448383L945.297505 664.913067C950.795839 654.832782 948.504866 642.461531 939.799171 635.588614L843.120122 559.986511C844.9529 545.324285 846.327484 530.662059 846.327484 515.083444ZM502.681566 675.45154C414.250017 675.45154 342.313471 603.514994 342.313471 515.083444 342.313471 426.651895 414.250017 354.71535 502.681566 354.71535 591.113114 354.71535 663.04966 426.651895 663.04966 515.083444 663.04966 603.514994 591.113114 675.45154 502.681566 675.45154L502.681566 675.45154Z"></path></symbol><symbol id="ic-wallet" viewBox="0 0 1081 1024"><path d="M78.590372 320.640733C79.612385 263.784132 108.255224 220.849417 161.254046 208.613675L811.774629 58.429164C843.73446 51.050656 875.603143 70.885317 882.986092 102.864395L930.36408 264.731771C930.36408 264.731771 432.674221 267.670638 193.893052 264.731771 191.508185 264.702418 137.002048 273.909722 137.002047 320.464627L888.060649 320.640726C942.735462 320.640726 988.818119 385.402493 988.818119 439.966797L988.818119 835.991046C988.818119 890.695509 928.709194 947.609862 874.118087 947.609862L193.893052 947.609862C139.218237 947.609862 78.590372 890.555352 78.590372 835.991046L78.590372 320.640733ZM870.350433 600.907349C870.350433 562.701243 839.378261 531.729073 801.172156 531.729073 762.96605 531.729073 731.993879 562.701243 731.993879 600.907349 731.993879 639.113461 762.96605 670.085626 801.172156 670.085626 839.378261 670.085626 870.350433 639.113461 870.350433 600.907349Z"></path></symbol><symbol id="ic-like-filled" viewBox="0 0 1024 1024"><path d="M455.286273 907.310814C215.315919 689.706627 56.888889 546.190377 56.888889 370.056797 56.888889 226.540546 169.651657 113.777778 313.167908 113.777778 394.24527 113.777778 472.0609 151.520688 522.850741 211.163805 573.640585 151.520688 651.45621 113.777778 732.533578 113.777778 876.049829 113.777778 988.812595 226.540546 988.812595 370.056797 988.812595 546.190377 830.385567 689.706627 590.415212 907.776774L522.850741 968.817778 455.286273 907.310814Z"></path></symbol><symbol id="ic-feedback" viewBox="0 0 1024 1024"><path d="M158.037561 937.474543C158.037561 937.474543 428.526853 971.883105 479.47676 768.097638L795.921232 768.097638C859.047925 768.097638 910.222222 716.754256 910.222222 654.123816L910.222222 227.7516C910.222222 164.805596 859.18947 113.777778 796.11115 113.777778L227.888848 113.777778C164.867044 113.777778 113.777778 165.12116 113.777778 227.7516L113.777778 654.123816C113.777778 717.069824 164.241867 768.097638 227.072055 768.097638L285.060607 768.097638C254.825383 906.665296 158.037561 937.474543 158.037561 937.474543ZM284.444444 512 625.777778 512 625.777778 597.333333 284.444444 597.333333 284.444444 512 284.444444 512ZM284.473374 312.888889 739.584484 312.888889 739.584484 398.222222 284.473374 398.222222 284.473374 312.888889 284.473374 312.888889Z"></path></symbol><symbol id="ic-signout" viewBox="0 0 1024 1024"><path d="M376.88888867 568.888889L746.66666667 568.888889 632.88888867 682.666667 703.99999967 755.238935 945.77777767 540.444444 945.77777767 483.555556 703.99999967 270.947385 632.88888867 341.333333 746.66666667 455.111111 376.88888867 455.111111 376.88888867 568.888889ZM489.07269967 113.777778L489.07269967 227.888848 206.20427767 227.888848 206.20427767 796.111167 489.07269967 796.111167 489.07269967 910.222222 206.20427967 910.222222C143.37645767 910.222222 92.44444467 859.18947 92.44444467 796.11115L92.44444467 227.888848C92.44444467 164.867044 143.37274867 113.777778 206.20427967 113.777778L489.07269967 113.777778Z"></path></symbol><symbol id="ic-nav-discover" viewBox="0 0 1024 1024"><path d="M13.3565216 512C13.3565216 236.60633067 236.60633067 13.3565216 512 13.3565216S1010.6434784 236.60633067 1010.6434784 512 787.39366933 1010.6434784 512 1010.6434784 13.3565216 787.39366933 13.3565216 512z m926.05217387 0a427.40869547 427.40869547 0 1 0-854.81739094 0 427.40869547 427.40869547 0 0 0 854.81739094 0z m-321.3638496-253.548336c120.29180267-82.53736853 173.6704-39.4165792 119.4369856 95.9295072l-104.9525792 261.6691008-217.9784352 148.8332064c-120.434272 82.1574496-175.90242347 37.70694507-124.13848107-98.6364288l93.93493333-247.42214507 233.65008747-160.37324053zM512 583.23478293a71.23478293 71.23478293 0 1 0 0-142.46956586 71.23478293 71.23478293 0 0 0 0 142.46956586z"></path></symbol><symbol id="ic-nav-notification" viewBox="0 0 1024 1024"><path d="M513.024 1001.828174A111.88313 111.88313 0 0 0 625.39687 890.434783H400.695652a111.88313 111.88313 0 0 0 112.328348 111.393391z m-320.823652-489.293913v5.030956c0 43.408696-13.445565 84.591304-36.062609 122.301218-8.013913 13.312-16.562087 25.154783-25.065739 35.350261-4.897391 5.921391-8.592696 9.794783-10.329043 11.486608l-3.250087 3.650783C57.433043 768.934957 96.478609 845.913043 196.118261 845.913043h634.434782c99.906783 0 135.43513-75.063652 74.306783-153.11026l-1.736348-2.181566-2.048-1.869913a180.535652 180.535652 0 0 1-11.53113-11.976347 298.206609 298.206609 0 0 1-27.603478-36.151653c-24.709565-38.021565-39.357217-79.070609-39.357218-121.366261 0-2.31513 0-4.630261 0.089044-6.945391l-0.044522-76.755478c0-132.096-84.591304-258.626783-189.350957-283.202783C624.951652 91.447652 571.65913 44.521739 507.458783 44.521739 437.426087 44.521739 395.931826 92.16 382.085565 151.863652 268.332522 200.214261 192.333913 310.761739 192.333913 435.556174v63.933217c-0.133565 3.116522-0.133565 6.233043-0.089043 13.089392z m65.80313-72.525913c0-105.427478 67.31687-198.210783 166.199652-232.848696l21.99374-7.657739v-29.028174c0-33.124174 27.38087-60.237913 61.261913-60.237913 33.836522 0 61.217391 27.113739 61.217391 60.237913v29.028174l21.993739 7.702261C689.508174 241.797565 756.869565 334.625391 756.869565 440.05287l0.089044 77.06713-0.089044 7.568696c0 56.765217 18.788174 109.968696 49.775305 158.230261 18.432 28.672 36.953043 49.597217 49.997913 61.618086l-3.739826-4.096c28.093217 36.285217 22.216348 48.88487-22.394435 48.88487H196.118261c-45.456696 0-53.426087-15.894261-26.178783-52.001391l-3.250087 3.695304c12.02087-11.798261 29.072696-32.411826 45.946435-60.816696 28.226783-47.549217 45.278609-100.396522 45.278609-157.250782v-5.431652l0.044522-11.842783V440.008348z"></path></symbol><symbol id="ic-nav-follow" viewBox="0 0 1024 1024"><path d="M725.25913 780.14701467L721.474783 780.05797067H868.173913c24.620522 0 44.521739-19.945739 44.521739-44.744347V134.71536267A44.521739 44.521739 0 0 0 868.08487 89.97101467H155.91513C131.33913 89.97101467 111.304348 110.00579667 111.304348 134.71536267v600.598261A44.432696 44.432696 0 0 0 155.737043 780.05797067h146.832696l-3.784348 0.089044c69.053217-3.873391 154.757565 43.186087 213.036522 106.184348 51.778783-60.594087 125.373217-111.170783 213.437217-106.184348zM44.521739 735.31362367V134.71536267A111.482435 111.482435 0 0 1 155.91513 23.18840567h712.16974A111.304348 111.304348 0 0 1 979.478261 134.71536267v600.598261a111.34887 111.34887 0 0 1-111.304348 111.526956h-146.69913c-66.960696-3.784348-172.78887 74.885565-201.861566 145.67513-4.541217 11.130435-10.685217 10.596174-15.449043-0.756869-29.606957-70.611478-134.90087-148.702609-201.594435-144.918261H155.737043A111.215304 111.215304 0 0 1 44.521739 735.31362367zM489.73913 179.01449267h44.52174V735.53623167h-44.52174V179.01449267z m333.913044 178.086957V290.31884067H623.304348v66.782609h200.347826zM623.304348 512.92753667h200.347826v-66.782609H623.304348V512.92753667zM200.347826 357.10144967H400.695652V290.31884067H200.347826v66.782609z m0 155.826087H400.695652v-66.782609H200.347826V512.92753667z"></path></symbol><symbol id="ic-nav-download" viewBox="0 0 1024 1024"><path d="M222.608696 133.342609C222.608696 84.279652 262.41113 44.521739 311.785739 44.521739h400.428522C761.455304 44.521739 801.391304 84.680348 801.391304 133.342609v757.314782A88.909913 88.909913 0 0 1 712.214261 979.478261H311.785739A89.266087 89.266087 0 0 1 222.608696 890.657391V133.342609zM445.217391 912.695652c0-12.288 9.616696-22.26087 22.038261-22.260869h89.488696c12.154435 0 22.038261 10.329043 22.038261 22.260869 0 12.288-9.616696 22.26087-22.038261 22.26087h-89.488696a22.394435 22.394435 0 0 1-22.038261-22.26087zM289.391304 845.913043h445.217392V111.304348h-445.217392V845.913043zM356.173913 489.73913l155.826087 184.141913L667.826087 489.73913h-89.043478V356.173913h-133.565218v133.565217H356.173913z"></path></symbol><symbol id="ic-paid" viewBox="0 0 1024 1024"><path d="M511.850132 0C229.23293 0 0.000088 229.132871 0.000088 511.850044s229.132871 511.850044 511.850044 511.850044 511.850044-229.232842 511.850044-511.850044-229.132871-511.850044-511.850044-511.850044z m226.833545 576.031241c29.191448 0 52.984477 23.693059 52.984477 52.884506 0 14.195841-5.398418 27.491946-15.195548 37.289076-9.997071 9.997071-23.293176 15.39549-37.588988 15.395489H564.034843v173.649127c0 29.091477-23.593088 52.584594-52.484623 52.584594-29.091477 0-52.884506-23.693059-52.984478-52.684565V681.800254H285.016587c-29.191448 0-52.884506-23.693059-52.884507-52.884507s23.693059-52.884506 52.884507-52.884506h173.449185V454.766768H284.816646c-29.191448 0-52.884506-23.593088-52.884507-52.584595s23.693059-52.584594 52.884507-52.584594H474.161173L330.703202 205.939666c-10.097042-9.997071-15.595431-23.193205-15.595431-37.289075 0-14.09587 5.498389-27.491946 15.595431-37.489017 9.997071-9.997071 23.093234-15.39549 37.189105-15.39549 14.09587 0 27.292004 5.498389 37.289075 15.595431l116.46588 116.465879L637.813229 131.261544c9.997071-10.097042 23.193205-15.595431 37.389046-15.595431 14.09587 0 27.192034 5.498389 37.189104 15.39549 20.493996 20.593967 20.593967 54.184126 0 74.678122L569.23332 349.597579h169.350386c29.191448 0 52.984477 23.593088 52.984477 52.584594 0 27.591916-21.493703 50.285268-48.685736 52.484624h-178.947574V576.131212h174.748804v-0.099971z"></path></symbol></svg><div id="__next"><header style="width:100%"><div class="_1CSgtu"><div class="_2oDcyf _1soATd"><a class="_1AawTM _1OhGeD" href="https://www.jianshu.com/" aria-label="简书" target="_blank" rel="noopener noreferrer"><svg class="wCYvWN" style="width:60px;height:30px" width="60" height="30" focusable="false" aria-hidden="true" viewBox="0 0 106 50" version="1.1"><g><path d="M79.6542664,49.2735656 L75.6602511,49.6932377 L75.6602511,27.3313525 L59.1137321,27.3313525 C58.6314725,27.3313525 57.9655336,26.8821721 57.8498237,26.1776639 L57.5346557,23.1870902 L75.6602511,23.1887295 L75.6602511,12.1260246 L62.1759992,12.1260246 C61.6180832,12.0858607 61.0229458,11.7788934 60.8894344,10.9870902 L60.5819534,7.93790984 L75.6602511,7.93790984 L75.6602511,0.409631148 L81.2074496,0.409631148 L81.2074496,7.93790984 L97.4727855,7.93790984 L97.4727855,23.1887295 L103.836831,23.1887295 L103.836831,38.1235656 C103.836831,42.2026639 100.70174,44.4715164 97.7187702,44.4715164 L92.357274,44.4715164 C91.6217473,44.4715164 91.0290374,43.9440574 90.9659229,43.3719262 L90.6681519,40.5223361 L92.1274725,40.5223361 L95.7933733,40.5223361 C97.0864115,40.5223361 98.3321137,39.6739754 98.3321137,38.1235656 L98.3321137,27.3313525 L81.2074496,27.3313525 L81.2074496,47.4452869 C81.2074496,48.5985656 80.4148771,49.1264344 79.6542664,49.2735656 L79.6542664,49.2735656 Z M81.260045,22.917418 L91.9745412,22.917418 L91.9745412,12.0514344 L81.260045,12.0514344 L81.260045,22.917418 Z M104.57519,13.9920082 L100.167289,13.9920082 C99.5871214,9.9170082 97.5274038,5.26987705 95.0355947,1.96209016 L100.167289,1.96209016 C102.583037,4.95553279 104.693327,8.85922131 105.720556,12.5608607 C105.867015,13.1711066 105.472144,13.9920082 104.57519,13.9920082 L104.57519,13.9920082 Z"></path><path d="M4.98236412,12.0515574 L9.99834885,12.0515574 C11.3953641,13.8056557 12.3169977,15.077377 13.7310053,18.5515574 C13.8337687,18.8339344 13.9130664,20.2007377 12.5900893,20.2007377 L8.57624962,20.2007377 C7.43047863,15.8630328 6.43521145,14.337623 4.97831832,12.0540164 C3.51980687,13.0105738 1.86507405,13.8138525 -4.04580153e-05,14.3802459 L-4.04580153e-05,10.3880328 C2.35987557,9.24172131 4.08662366,7.6892623 5.34608168,5.85278689 C6.43399771,4.2654918 7.18206641,2.47901639 7.58300534,0.409754098 L11.8043947,0.409754098 C12.772555,0.409754098 13.2313489,1.06877049 13.0962191,1.56139344 C12.9129443,2.1392623 12.5342573,2.99377049 12.1640664,3.78270492 L25.2676084,3.78270492 L25.0038221,6.30278689 C24.9370664,6.85483607 24.5227763,7.57532787 23.6181351,7.57532787 L17.5377,7.57532787 C18.2930511,9.24336066 18.7571046,10.7601639 18.9112496,11.3568852 C19.0508298,11.9036066 18.7004634,12.7810656 17.765074,12.7810656 L13.9850817,12.7810656 C13.7127992,10.9581148 13.4073412,9.70811475 12.633784,7.57532787 L9.82963893,7.57532787 C8.54266947,9.20852459 6.93891374,10.7679508 4.98236412,12.0515574 L4.98236412,12.0515574 Z M14.1651198,43.4847541 L14.1651198,22.1413115 L34.8367382,22.1413115 L34.8367382,36.8896721 C34.9253412,41.1093443 31.729158,43.4847541 28.4929214,43.4847541 L14.1651198,43.4847541 Z M35.3776618,49.3056557 C34.6793565,49.3056557 34.1008069,48.7921311 34.0028985,48.0851639 L33.669929,45.2757377 L38.4682496,45.2372131 C39.5310817,45.2372131 40.5696389,44.5277869 40.5696389,42.9769672 L40.5696389,18.4761475 L16.1390664,18.4761475 C15.6288908,18.4761475 15.0078603,18.2769672 14.819326,17.2298361 L14.5478527,14.3802459 L45.940845,14.3802459 L45.940845,43.4048361 C45.940845,45.4257377 44.1275168,49.2339344 39.6552878,49.3056557 L35.3776618,49.3056557 Z M6.84505115,49.2904918 L3.0626313,49.6933607 L3.0626313,20.2007377 L8.42250916,20.2007377 L8.42250916,47.3646721 C8.42250916,48.0146721 8.09399008,49.0794262 6.84505115,49.2904918 L6.84505115,49.2904918 Z M41.1040893,12.617541 L37.1335397,12.617541 C36.8025931,10.8986885 36.5436618,9.44581967 35.7462344,7.47942623 L32.2174863,7.47942623 C31.3132496,8.52245902 30.2645779,9.69581967 28.8404557,10.8204098 C27.3799214,11.9736885 25.5018603,13.1154918 22.9704023,13.9917213 L22.9704023,9.96672131 C27.0331962,7.6904918 29.0237305,4.6007377 29.9906771,0.409754098 L34.2080206,0.409754098 C35.3258756,0.409754098 35.6547992,1.21385246 35.5848069,1.4892623 C35.3121198,2.35811475 34.9075397,2.9892623 34.6032954,3.70360656 L48.620784,3.70360656 L48.3602344,6.23434426 C48.2724405,6.99745902 47.669616,7.47942623 47.0243107,7.47942623 L40.7173107,7.47942623 C41.4892496,9.14459016 41.945616,10.3318852 42.1398145,11.2503279 C42.223158,11.6478689 41.9775779,12.5761475 41.1040893,12.617541 L41.1040893,12.617541 Z M19.1422649,39.6040164 L27.579784,39.6040164 C29.230471,39.6040164 29.861616,38.5290164 29.861616,37.3347541 L29.861616,34.5634426 L19.1693718,34.5589344 L19.1422649,39.6040164 Z M19.1422649,30.6786066 L29.8620206,30.6786066 L29.8620206,26.0220492 L19.1422649,26.0220492 L19.1422649,30.6786066 Z"></path></g></svg></a><div class="_7hb9O4"><i aria-label="ic-nav-mode" tabindex="-1" class="anticon _1nZg8v"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-nav-mode"></use></svg></i><span class="_1jKNin" aria-label="简书钻"><svg class="wCYvWN" style="width:54px;height:24px" width="54" height="24" focusable="false" aria-hidden="true" viewBox="0 0 50 22" version="1.1"><g transform="translate(1.000000, 4.000000)" fill="#EA6F5A" stroke="#EA6F5A" stroke-width="0.5"><path d="M6.97355121,0.977899795 L5.71487604,5.02704222 L14.0382941,5.02704222 L12.7812515,0.977899795 L6.97355121,0.977899795 Z M5.94914309,0.977899795 L5.39898286,0.977899795 C5.34645865,0.977985274 5.29616908,0.99894266 5.25940216,1.03606786 L1.33563333,5.02704222 L4.69046792,5.02704222 L5.94914309,0.977899795 Z M8.80687445,13.8613144 L4.56068234,5.99650966 L1.40664806,5.99650966 L8.80687445,13.8613144 Z M5.67079793,5.99650966 L9.99290234,14.0010793 L14.0962487,5.99650966 L5.67079793,5.99650966 Z M6.61929374,0.00843234872 L14.581108,0.00843234872 C14.8986337,0.00843234872 15.2014667,0.135271015 15.4234898,0.359864308 L19.6476429,4.6562209 C20.0841892,5.10085541 20.0917287,5.80620568 19.6647844,6.25988166 L10.8491606,15.6281688 C10.4068293,16.0970986 9.66444122,16.1231211 9.18970108,15.6863369 C9.1594994,15.6580607 9.1594994,15.6580607 9.13093026,15.6281688 L0.315306487,6.25988166 C-0.111412478,5.80599166 -0.10351395,5.10063881 0.333264249,4.6562209 L4.55741731,0.359864308 C4.7786002,0.135221626 5.08206407,0.00849721138 5.39898286,0.00843234872 L6.62011001,0.00843234872 L6.61929374,0.00843234872 Z M13.8056596,0.977899178 L15.0643348,5.02704222 L18.6452738,5.02704222 L14.721505,1.03606786 C14.6845399,0.998742398 14.6339153,0.977768057 14.581108,0.977899795 L13.8056596,0.977899178 Z M11.1487286,13.8879747 L18.5742591,5.99650966 L15.1949366,5.99650966 L11.1487286,13.8879747 Z"></path></g><g transform="translate(19.000000, 5.000000)"><path d="M5.16425826,0.266666667 C3.98605079,0.266666667 3.03092493,1.22179253 3.03092493,2.4 L3.03092493,8.73301908 L2.96783132,8.90526634 L1.31769477,10.8555055 C1.236217,10.9518012 1.19150756,11.0738592 1.19150756,11.2 C1.19150756,11.4945519 1.43028903,11.7333333 1.72484089,11.7333333 L26.5448138,11.7333333 C27.7230213,11.7333333 28.6781471,10.7782075 28.6781471,9.6 L28.6781471,2.4 C28.6781471,1.22179253 27.7230213,0.266666667 26.5448138,0.266666667 L5.16425826,0.266666667 Z" stroke="#EA6F5A" stroke-width="0.533333333" fill="#EA6F5A"></path><path d="M6.79203604,2 L7.57631592,2 L7.57631592,4.96927224 C7.96845586,4.26091644 8.52725527,3.9115903 9.25271416,3.9115903 C9.96836954,3.9115903 10.5369725,4.17358491 10.9487194,4.71698113 C11.3212523,5.20215633 11.5173223,5.80377358 11.5173223,6.54123989 C11.5173223,7.29811321 11.3212523,7.91913747 10.9487194,8.40431267 C10.527169,8.92830189 9.94876255,9.2 9.21350016,9.2 C8.42922028,9.2 7.87042087,8.87978437 7.52729842,8.2393531 L7.52729842,9.06415094 L6.79203604,9.06415094 L6.79203604,2 Z M9.08605468,4.55202156 C8.62529025,4.55202156 8.25275731,4.72668464 7.97825935,5.09541779 C7.6841544,5.45444744 7.54690542,5.92991914 7.54690542,6.51212938 L7.54690542,6.60916442 C7.54690542,7.17196765 7.6743509,7.62803235 7.92924186,7.97735849 C8.20373982,8.36549865 8.60568326,8.55956873 9.11546518,8.55956873 C9.66446109,8.55956873 10.076208,8.35579515 10.350706,7.96765499 C10.5859899,7.61832884 10.7134354,7.14285714 10.7134354,6.54123989 C10.7134354,5.93962264 10.5859899,5.47385445 10.331099,5.13423181 C10.0467975,4.74609164 9.6350506,4.55202156 9.08605468,4.55202156 Z M14.6250313,3.9115903 C15.4387217,3.9115903 16.0563421,4.18328841 16.468089,4.72668464 C16.840622,5.21185984 17.0366919,5.90080863 17.0562989,6.77412399 L13.0564716,6.77412399 C13.0956856,7.33692722 13.242738,7.77358491 13.517236,8.08409704 C13.7917339,8.39460916 14.1740704,8.54986523 14.6544418,8.54986523 C15.0661888,8.54986523 15.4093112,8.44312668 15.6642022,8.2393531 C15.8798791,8.06469003 16.0465386,7.80269542 16.1739841,7.45336927 L16.958264,7.45336927 C16.840622,7.93854447 16.6151415,8.32668464 16.2720191,8.63719677 C15.8504686,9.00592992 15.3112762,9.2 14.6544418,9.2 C13.9289829,9.2 13.340773,8.9574124 12.9094191,8.4916442 C12.4584582,8.006469 12.2427812,7.36603774 12.2427812,6.5509434 C12.2427812,5.81347709 12.4486547,5.19245283 12.8800086,4.69757412 C13.3113625,4.17358491 13.8897689,3.9115903 14.6250313,3.9115903 Z M14.6446383,4.56172507 C14.1936774,4.56172507 13.8309479,4.70727763 13.55645,4.99838275 C13.281952,5.28948787 13.1250961,5.67762803 13.0760786,6.17250674 L16.2426086,6.17250674 C16.1445736,5.09541779 15.6053812,4.56172507 14.6446383,4.56172507 Z M19.3307106,2.42695418 L19.3307106,4.04743935 L20.5855584,4.04743935 L20.5855584,4.69757412 L19.3307106,4.69757412 L19.3307106,7.89002695 C19.3307106,8.07439353 19.3601211,8.21024259 19.4385491,8.28787062 C19.5071736,8.36549865 19.634619,8.41401617 19.811082,8.41401617 L20.4581129,8.41401617 L20.4581129,9.06415094 L19.69344,9.06415094 C19.2816931,9.06415094 18.9777846,8.9574124 18.8013217,8.74393531 C18.6346622,8.54986523 18.5562342,8.26846361 18.5562342,7.89002695 L18.5562342,4.69757412 L17.5366704,4.69757412 L17.5366704,4.04743935 L18.5562342,4.04743935 L18.5562342,2.74716981 L19.3307106,2.42695418 Z M23.6638569,3.9115903 C24.3893158,3.9115903 24.9187047,4.09595687 25.2716307,4.47439353 C25.5657356,4.80431267 25.7225916,5.26037736 25.7225916,5.8425876 L25.7225916,9.06415094 L24.9873292,9.06415094 L24.9873292,8.21994609 C24.7912592,8.4916442 24.5265648,8.7245283 24.2030493,8.89919137 C23.8305164,9.09326146 23.408966,9.2 22.9482015,9.2 C22.4384196,9.2 22.0266727,9.06415094 21.7325677,8.81185984 C21.4188558,8.54986523 21.2619998,8.20053908 21.2619998,7.7541779 C21.2619998,7.11374663 21.5168907,6.64797844 22.0364762,6.35687332 C22.4482231,6.11428571 23.016826,5.98814016 23.7226779,5.98814016 L24.9383117,5.97843666 L24.9383117,5.81347709 C24.9383117,4.96927224 24.4971543,4.55202156 23.6148394,4.55202156 C23.212896,4.55202156 22.8893805,4.6296496 22.6540966,4.80431267 C22.3992056,4.97897574 22.2423496,5.23126685 22.1835286,5.58059299 L21.4090523,5.58059299 C21.4874803,5.00808625 21.7325677,4.58113208 22.1541181,4.29973046 C22.5266511,4.03773585 23.0266295,3.9115903 23.6638569,3.9115903 Z M24.9383117,6.58975741 L23.7716954,6.59946092 C22.6344896,6.59946092 22.0658867,6.98760108 22.0658867,7.74447439 C22.0658867,7.98706199 22.1541181,8.19083558 22.3403846,8.34609164 C22.5266511,8.4916442 22.7815421,8.56927224 23.114861,8.56927224 C23.6246429,8.56927224 24.0559969,8.41401617 24.4187263,8.10350404 C24.7618487,7.79299191 24.9383117,7.43396226 24.9383117,7.02641509 L24.9383117,6.58975741 Z" fill="#FFFFFF"></path></g></svg></span><a href="https://www.jianshu.com/sign_in" target="_blank" class="_2MpoKb _1OyPqC _1AT95S _2WY0RL" role="button" tabindex="-1"><span>登录</span></a><a href="https://www.jianshu.com/sign_up" target="_blank" class="_2MpoKb _1OyPqC _3Mi9q9 _2WY0RL" role="button" tabindex="-1"><span>注册</span></a><a href="https://www.jianshu.com/writer" target="_blank" class="_1OyPqC _3Mi9q9 _2WY0RL _1YbC5u" role="button" tabindex="-1"><i aria-label="ic-write" style="margin-right:2px" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-write"></use></svg></i><span>写文章</span></a></div><div class="_1YyUun"><div class="_2RZATq"><nav class="_3JYrtj"><a class="hM7XFL _1OhGeD" href="https://www.jianshu.com/">首页</a><a class="hM7XFL _1OhGeD" href="https://www.jianshu.com/apps?utm_medium=desktop&amp;utm_source=navbar-apps">下载APP</a></nav><div class="_1F7CTF" role="button" tabindex="0"><div class="_1o4qyK">抽奖</div><img class="_1YyGPQ" src="data:image/png;base64,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" alt="reward"></div></div></div></div><div class="_3t3lfz KEKfID"><div class="FTZkZo"><div class="_16zCst"><h1 class="_2zeTMs" title="一文入门sklearn二分类实战">一文入门sklearn二分类实战</h1></div><div class="_26qd_C"><a class="qzhJKO" href="https://www.jianshu.com/u/1eb8ed4c42ca"><img class="_2JlnTn" src="./一文入门sklearn二分类实战 - 简书_files/fc1ef06e-6f88-4559-9010-33832c3baec3" alt=""><span class="_22gUMi">猴小白</span></a><button data-locale="zh-CN" type="button" class="_1OyPqC _3Mi9q9"><span>关注</span></button><button type="button" class="_1OyPqC _3Mi9q9 _1YbC5u"><span>赞赏支持</span></button></div></div></div></div><div class="VYwngI"></div></header><div class="_21bLU4 _3kbg6I"><div class="_3VRLsv" role="main"><div class="_gp-ck"><section class="ouvJEz"><h1 class="_1RuRku">一文入门sklearn二分类实战</h1><div class="rEsl9f"><div class="_2mYfmT"><a class="_1qp91i _1OhGeD" href="https://www.jianshu.com/u/1eb8ed4c42ca" target="_blank" rel="noopener noreferrer"><img class="_13D2Eh" src="./一文入门sklearn二分类实战 - 简书_files/fc1ef06e-6f88-4559-9010-33832c3baec3(1)" alt=""></a><div style="margin-left: 8px;"><div class="_3U4Smb"><span class="FxYr8x"><a class="_1OhGeD" href="https://www.jianshu.com/u/1eb8ed4c42ca" target="_blank" rel="noopener noreferrer">猴小白</a></span><button data-locale="zh-CN" type="button" class="_3kba3h _1OyPqC _3Mi9q9 _34692-"><span>关注</span></button></div><div class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-diamond"></use></svg></i><span>0.072</span></span><time datetime="2019-07-12T20:01:40.000Z">2019.07.13 04:01:40</time><span>字数 5,937</span><span>阅读 4,429</span></div></div></div></div><article class="_2rhmJa"><p>在小白我的第一篇文里就提出过一个问题，就是现在的教程都太“分散”太“板块”，每一个知识点都单独用一个例子，机器学习算法里也是这样的，可能逻辑回归用葡萄酒的案例讲，决策树又用鸢尾花的数据集了。今天，我们只用乳腺癌数据集，专门来看一看二分类问题的实战应用。</p>
<p>导入包</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 解决坐标轴刻度负号乱码</span>
plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'axes.unicode_minus'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span>
<span class="token comment"># 解决中文乱码问题</span>
plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Simhei'</span><span class="token punctuation">]</span>

<span class="token operator">%</span>matplotlib inline
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<h1>导入数据</h1>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token comment"># 读入癌症数据集</span>
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

cancer<span class="token operator">=</span>datasets<span class="token punctuation">.</span>load_breast_cancer<span class="token punctuation">(</span><span class="token punctuation">)</span>
X<span class="token operator">=</span>cancer<span class="token punctuation">.</span>data
y<span class="token operator">=</span>cancer<span class="token punctuation">.</span>target

X_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-bash"><code class="  language-bash">print('训练集维度:{}\n测试集维度:{}'.format(X_train.shape,X_test.shape))
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<p>训练集维度:(426, 30)<br>
测试集维度:(143, 30)</p>
<h1>一、逻辑回归</h1>
<p>我们先来看看分类里最简单却又是最经典的逻辑回归：</p>
<h2>1.1 sklearn实战</h2>
<p>废话不多说，看看用sklearn怎么做逻辑回归。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

lr <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>                                        <span class="token comment"># 实例化一个LR模型</span>
lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>                                          <span class="token comment"># 训练模型</span>
y_prob <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>                           <span class="token comment"># 预测1类的概率</span>
y_pred <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>                                      <span class="token comment"># 模型对测试集的预测结果</span>
fpr_lr<span class="token punctuation">,</span>tpr_lr<span class="token punctuation">,</span>threshold_lr <span class="token operator">=</span> metrics<span class="token punctuation">.</span>roc_curve<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_prob<span class="token punctuation">)</span>    <span class="token comment"># 获取真阳率、伪阳率、阈值</span>
auc_lr <span class="token operator">=</span> metrics<span class="token punctuation">.</span>auc<span class="token punctuation">(</span>fpr_lr<span class="token punctuation">,</span>tpr_lr<span class="token punctuation">)</span>                              <span class="token comment"># AUC得分</span>
score_lr <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>                 <span class="token comment"># 模型准确率</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>score_lr<span class="token punctuation">,</span>auc_lr<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>得到准确率为0.9720, AUC值为0.9954。这个结果可以说是相当不错了。<br>
可以查看一下LR模型的默认参数。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-undefined"><code class="  language-undefined">lr
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 550px; max-height: 93px; background-color: transparent;">
<div class="image-container-fill" style="padding-bottom: 16.91%;"></div>
<div class="image-view" data-width="550" data-height="93"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-753d97319b161c5c.png" data-original-width="550" data-original-height="93" data-original-format="image/png" data-original-filesize="5898" data-image-index="0" style="cursor: zoom-in;" class="" src="./一文入门sklearn二分类实战 - 简书_files/18317213-753d97319b161c5c.png"></div>
</div>
<div class="image-caption">LR默认参数</div>
</div>
<h2>1.2 对逻辑回归的理解</h2>
<p>这里的乳腺癌数据集有30个特征，我们需要根据这些特征去判断一个是否会患乳腺癌。不妨换一个思路，如果按照我们熟悉的线性回归，这个问题该怎么思考呢？那么我们会根据每个人自身的特征给他打个分，我们规定分数越高就越不健康。进一步，我们希望把每个人的得分进行归一化，让分数限制在[0,1]这个区间，这样，打分的问题就变成了概率问题。<br>
那怎么把得分变成概率呢？有很多方法，最经典的就要说一说sigmoid函数了。<br>
<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cnblogs.com%2Fxitingxie%2Fp%2F9924523.html" target="_blank">https://www.cnblogs.com/xitingxie/p/9924523.html</a><br>
我随意贴了一个网上找的比较全的sigmoid函数解析，偷个懒。</p>
<h2>1.3 损失函数</h2>
<p>在这里，不得不说一下，小白我刚开始接触逻辑回归的时候，就掉入了这个坑，LR带有回归二字，我想当然地就认为损失函数肯定和回归一样嘛，真实值和预测值之间差值的平方和嘛！but，它本质其实还是分类问题啊，分类模型的损失函数，是交叉熵。<br>
熵这个问题可能又会让很多初学者很困扰了，我刚开始学的时候也是一知半解，在这里分享一篇觉得写的很好的交叉熵的文：<br>
<a href="https://www.jianshu.com/p/8a0ad237b0ed" target="_blank">https://www.jianshu.com/p/8a0ad237b0ed</a><br>
另外贴一篇损失函数推导：<br>
<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cnblogs.com%2Fshayue%2Fp%2F10520414.html" target="_blank">https://www.cnblogs.com/shayue/p/10520414.html</a></p>
<h2>1.4 求解损失函数(max_iter)</h2>
<p>使用梯度下降。这个经典方法我在这儿也不过多说了，同样，贴个链接大家可以自己去看：<br>
<a href="https://www.jianshu.com/p/93d9fea7f4c2" target="_blank">https://www.jianshu.com/p/93d9fea7f4c2</a><br>
我们这里只来看一看max_iter这个参数，它表示梯度下降法中最大迭代次数，max_iter越大，代表步长越小，模型迭代时间越长。我们来看一看效果。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-go"><code class="  language-go">trainList<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>  # 用来记录训练集得分
testList<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>   # 用来记录测试集得分

<span class="token keyword">for</span> i in <span class="token keyword">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">101</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    lr<span class="token operator">=</span><span class="token function">LogisticRegression</span><span class="token punctuation">(</span>max_iter<span class="token operator">=</span>i<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    trainList<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span><span class="token function">accuracy_score</span><span class="token punctuation">(</span>y_train<span class="token punctuation">,</span>lr<span class="token punctuation">.</span><span class="token function">predict</span><span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    testList<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span><span class="token function">accuracy_score</span><span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>lr<span class="token punctuation">.</span><span class="token function">predict</span><span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
plt<span class="token punctuation">.</span><span class="token function">figure</span><span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>style<span class="token punctuation">.</span><span class="token function">use</span><span class="token punctuation">(</span><span class="token string">'seaborn-colorblind'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">plot</span><span class="token punctuation">(</span><span class="token keyword">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">101</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>trainList<span class="token punctuation">,</span>color<span class="token operator">=</span><span class="token string">'orange'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">plot</span><span class="token punctuation">(</span><span class="token keyword">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">101</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>testList<span class="token punctuation">,</span>color<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 700px; max-height: 304px; background-color: transparent;">
<div class="image-container-fill" style="padding-bottom: 26.21%;"></div>
<div class="image-view" data-width="1160" data-height="304"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-599af8d696612c30.png" data-original-width="1160" data-original-height="304" data-original-format="image/png" data-original-filesize="19460" data-image-index="1" style="cursor: zoom-in;" class="" src="./一文入门sklearn二分类实战 - 简书_files/18317213-599af8d696612c30.png"></div>
</div>
<div class="image-caption">max_iter</div>
</div><br>
<p>我们可以看到，基本迭代到30次的时候测试集和训练集的得分就不再提高了，继续增大迭代次数也只是浪费了资源。</p>

<h2>1.5 如何防止过拟合(penalty &amp; C)</h2>
<p>逻辑回归和线性回归一样，为了防止过拟合，可以在损失函数的后面添加正则项。同样的，我也贴个详解的连接在后面，但大家只要知道我们有l1和l2两种正则项，sklearn里大多数算法默认的都是l2范式，它会让特征前的系数尽量小防止过拟合，但不会像l1范式那样直接将某些系数减小到0：<br>
<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fred_stone1%2Farticle%2Fdetails%2F80755144" target="_blank">https://blog.csdn.net/red_stone1/article/details/80755144</a><br>
在sklearn中，通过penalty选择l1还是l2范式，而通过C控制对系数的惩罚力度。要注意的是，C是加在损失函数前面的，而不是加在正则项前面，因此C越大，表示对系数的惩罚力度越小，模型越容易过拟合。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-bash"><code class="  language-bash">lr1=LogisticRegression(penalty='l1').fit(X_train,y_train)
lr2=LogisticRegression(penalty='l2').fit(X_train,y_train)

print('L1正则得分：{}\nL2正则得分：{}'.format(metrics.accuracy_score(y_test,lr1.predict(X_test)),metrics.accuracy_score(y_test,lr2.predict(X_test))))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>当然，这个数据集过于理想，因此使用l1和l2基本没有不同。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-bash"><code class="  language-bash">list1=[]  # 用来记录l1正则得分
list2=[]   # 用来记录l2正则得分

for i in np.linspace(0.05,1,20):
    lr1=LogisticRegression(penalty='l1',C=i).fit(X_train,y_train)
    lr2=LogisticRegression(penalty='l2',C=i).fit(X_train,y_train)
    list1.append(metrics.accuracy_score(y_test,lr1.predict(X_test)))
    list2.append(metrics.accuracy_score(y_test,lr2.predict(X_test)))
    
plt.figure(figsize=(20, 5))
plt.style.use('seaborn-colorblind')
plt.plot(np.linspace(0.05,1,20),list1,color='orange',label='l1-test')
plt.plot(np.linspace(0.05,1,20),list2,color='red',label='l2-test')
plt.legend(loc='lower right')
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 700px; max-height: 304px; background-color: transparent;">
<div class="image-container-fill" style="padding-bottom: 26.07%;"></div>
<div class="image-view" data-width="1166" data-height="304"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-9e16571696d1e629.png" data-original-width="1166" data-original-height="304" data-original-format="image/png" data-original-filesize="26462" data-image-index="2" style="cursor: zoom-in;" class="" src="./一文入门sklearn二分类实战 - 简书_files/18317213-9e16571696d1e629.png"></div>
</div>
<div class="image-caption">正则系数C</div>
</div><br>
<p>我们一般是要防止过拟合，所以对C的调参一般是在[0,1]之间调整。可以看到，加入C之后，l1和l2的区别就出来了。C取默认值1.0的时候其实就是得分最高的了，再调整C意义不大。</p>

<h2>1.6 如何处理样本不均衡(class_weight)</h2>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-css"><code class="  language-css">pd.<span class="token function">Series</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>.value_<span class="token function">counts</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<p>在乳腺癌的数据集中，正例：负例是357:212，并不存在样本不均衡问题。但在实际现实问题建模时候，样本不均衡却是常态，比如在处理信用违约时，不违约的人总是多数，而违约的人是非常少的，除了我们常用的上采用的方法，还可以通过class_weight参数调节，在这里就不举例了。</p>
<h1>二、支持向量机</h1>
<p>支持向量机在深度学习没有兴起的时候那可是不可撼动的王者地位，当然，它这么好用，也可想而知，这家伙肯定是很难很复杂的。</p>
<h2>2.1 sklearn实战</h2>
<p>同样，废话不多说，先来看支持向量机算法在乳腺癌数据集上的表现。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> SVC
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

svc <span class="token operator">=</span> SVC<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>

y_prob <span class="token operator">=</span> svc<span class="token punctuation">.</span>decision_function<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>                              <span class="token comment"># 决策边界距离</span>
y_pred <span class="token operator">=</span> svc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>                                        <span class="token comment"># 模型对测试集的预测结果</span>
fpr_svc<span class="token punctuation">,</span>tpr_svc<span class="token punctuation">,</span>threshold_svc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>roc_curve<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_prob<span class="token punctuation">)</span>     <span class="token comment"># 获取真阳率、伪阳率、阈值</span>
auc_svc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>auc<span class="token punctuation">(</span>fpr_svc<span class="token punctuation">,</span>tpr_svc<span class="token punctuation">)</span>                              <span class="token comment"># 模型准确率</span>
score_svc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>score_svc<span class="token punctuation">,</span>auc_svc<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>可以得到，在不调任何参数的情况下，准确率为0.6294, AUC值为0.9126。我们会发现AUC值很高，但是准确率却不高。其实，通过metrics.precision_score(y_test,y_pred)和metrics.recall_score(y_test,y_pred)，我们会发现精确率很低只有0.6294，但召回率为1.0。（什么是精确率和召回率大家也可以自行百度下）<br>
同样，我们可以看一下svc的默认参数。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-undefined"><code class="  language-undefined">svc
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 494px; max-height: 73px; background-color: transparent;">
<div class="image-container-fill" style="padding-bottom: 14.78%;"></div>
<div class="image-view" data-width="494" data-height="73"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-bbcc81ddeb55aa3c.png" data-original-width="494" data-original-height="73" data-original-format="image/png" data-original-filesize="5045" data-image-index="3" style="cursor: zoom-in;" class="" src="./一文入门sklearn二分类实战 - 简书_files/18317213-bbcc81ddeb55aa3c.png"></div>
</div>
<div class="image-caption">svc默认参数</div>
</div>
<h2>2.2 对支持向量机的理解</h2>
<p>我们之前说逻辑回归的时候，通过最好理解的线性回归举例，通过sigmoid函数将一个本该是线性回归的问题转化为了一个线性分类的问题。那么，这个时候问题来了，如果数据不是线性可分的呢？这时候支持向量机就展现出它的核技术的魅力了，通过将低维度的数据映射到高维度，从而将非线性的问题转换成一个线性问题。<br>
是不是听着就觉得很绕？这是不可避免的，因为逻辑回归还可以用例子来理解，但是支持向量机基本上就是从数学推导的角度来选取一个最优的分类方法。但大家只要知道，算法是存在“进化”的，逻辑回归解决了线性回归到线性分类的难题，而支持向量机解决了线性分类到非线性分类的难题。<br>
支持向量机其实说白了，就是想通过数学的方法，看看怎么样能够将数据“最大程度”地分离开，就要找这个“最大程度”，我们叫“边际”(margin)最大。而支持向量机的名字也是有原因的，“边际”其实只由两类中离分割线最近的点所决定，这些点叫“支持向量”。<br>
另外，需要注意的是，在逻辑回归的损失函数求解过程中，我们是将不同类的标签记作1和0的，但是在支持向量机的损失函数求解中，我们是将不同类标签记作1和-1的，这么做是对我们求解有用的。</p>
<h2>2.3 损失函数</h2>
<p>我们之前说过，支持向量机的求解目标就是max margin，让“边际”最大。那么怎么去求这个“边际”呢？</p>
<br>
<div class="image-package">
<div class="image-container" style="max-width: 465px; max-height: 367px; background-color: transparent;">
<div class="image-container-fill" style="padding-bottom: 78.92%;"></div>
<div class="image-view" data-width="465" data-height="367"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-19884ff468022645.png" data-original-width="465" data-original-height="367" data-original-format="image/png" data-original-filesize="12716" data-image-index="4" style="cursor: zoom-in;" class="" src="./一文入门sklearn二分类实战 - 简书_files/18317213-19884ff468022645.png"></div>
</div>
<div class="image-caption">支持向量机分析</div>
</div><br>
<p>原理鄙人的画图水平有限，在上图中，我们假设有一条线w·x+b=0可以将两类点分开，这条线往上平移到红色类的边缘处，这条线就是w·x+b=1,；往下平移到橙色类边缘处，这条线就是w·x+b=-1。那有人就会问，向上移应该是截距项增加啊，那也应该是w·x+b=-1啊？而且，为什么上下移动就是等号右边加一个单位呢？同志们，我们这里只是一个记号而已，要知道，w和b都是一个符号，两边可以同时除以-1，也可以同时除以其他数，等号左边也同时除以，这时候我们会得到新的w和b，但是我们同样用w和b表示而已，不必纠结。<br>
我们如果假设Xc和Xd是决策边界上的两个点，那么就有W·Xc+b=0，W·Xd+b=0，相减得到W·(Xc-Xd)=0。两个向量点积为0，则说明垂直，w和我们要求的边际d是相同方向。<br>
这时候，假设Xa和Xb分别在上边缘和下边缘上，那么就有W·Xa+b=1，W·Xb+b=-1，相减得到W·(Xa-Xb)=2。<br>
我们对两边同时除以W的模，则有W/||W|| · (Xa-Xb) = 2/||W||，线性代数过关的小伙伴们能很快反应过来，等号左边其实就是Xa和Xb两点连线这个向量在w方向的投影，其实也就是我们要求的两条边界线之间的距离d。那么，我们的求解目标也很明确了，就是要max d，也就是max 2/||W||。我们将这个问题转化一下，其实也就是min 1/2 * W2，当然，外加一个限制条件，y * (w*x+b)&gt;1 （w·x+b&gt;1表示的是红色的点，这时候y=1；w·x+b&lt;-1表示的是橙色的点，这时候y=-1。这下明白将两类标签定义为1和-1而不是1和0的用处了吧）。<br>
当然，这只是我们弄出损失函数的第一步，这个带约束条件的最小化问题仍然是不好求解的。之后，我们还需要用拉格朗日乘数法，将上面这个约束条件去掉，转化为一个拉格朗日函数，我叫它第二步；再之后，我们要用拉格朗日对偶的性质，去求解我们第二步得到的拉格朗日函数，这是第三步。这就是我归纳的三步走。当然，这些数学推导还是有点复杂和繁琐的，我在这里也不推导了，想了解的自行百度。</p>

<h2>2.4 损失函数求解</h2>
<p>支持向量机损失函数的求解就不是梯度下降法这么简单的了，涉及太多数学推导，在这里我同样也不写了，大家一定要自行去了解序列最小优化算法（SMO），贴一篇吧：<br>
<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_39521554%2Farticle%2Fdetails%2F80723770" target="_blank">https://blog.csdn.net/qq_39521554/article/details/80723770</a></p>
<h2>2.5 核函数的选择(kernel &amp; gamma)</h2>
<p>接下来是SVM中第一个重要参数，kernel。我们之前也说过了，支持向量机之所以这么牛，就是因为核技术。核函数是什么呢？我们之前提到了，很多数据在本身的维度是线性不可分的，那怎么办呢？那就将数据映射到更高维度的空间去看一看，可能就可以线性分割了。核函数可以做什么呢？1、不需要去知道将低维映射到高维所需要用的函数；2、可以先在低维度进行点积运算，再直接映射到高纬度，大大减少了运算量；3、避免了维度诅咒。<br>
参数kernel常用的共有四个：“linear”、“poly”、“rbf”、“sigmoid”。默认的并且是最常使用的是高斯核rbf。效果最差的、也是最不常用的是“sigmoid”。当然，它们各自有各自出场的场合。<br>
我们可以来看下三种核函数(只看‘linear’、‘rbf’、‘sigmoid’，'poly'跑出结果是非常久的，这里就不实验了)在乳腺癌数据集上的表现。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> SVC
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

kernelList <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">,</span><span class="token string">'rbf'</span><span class="token punctuation">,</span><span class="token string">'sigmoid'</span><span class="token punctuation">]</span>         

<span class="token keyword">for</span> kernel <span class="token keyword">in</span> kernelList<span class="token punctuation">:</span>
    svc <span class="token operator">=</span> SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span>kernel<span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    y_pred <span class="token operator">=</span> svc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>
    score_svc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>score_svc<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>可以看到，‘linear’、‘rbf’、‘sigmoid’核表现分别为0.9790、0.6503、0.6503，可以看到，线性核表现最优，高斯核作为默认参数准确率却比较低。因为乳腺癌数据集是一个线性可分数据集。<br>
如果我们对数据标准化一下呢？</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token comment"># 读入癌症数据集</span>
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

cancer<span class="token operator">=</span>datasets<span class="token punctuation">.</span>load_breast_cancer<span class="token punctuation">(</span><span class="token punctuation">)</span>
X<span class="token operator">=</span>cancer<span class="token punctuation">.</span>data
y<span class="token operator">=</span>cancer<span class="token punctuation">.</span>target

<span class="token comment"># 数据标准化</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler

scaler <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
scaler<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
X<span class="token operator">=</span> scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

X_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>再执行一下上面的三个核函数，‘linear’、‘rbf’、‘sigmoid’的得分分别为0.9790、<br>
0.9860、0.9790。我们会发现，高斯核的表现一下子变成了number one，“sigmoid”居然也都跟线性核打平手了。<br>
因为涉及到算“距离”，所以可以看到，量纲问题就非常重要了。<br>
因为高斯核(rbf)在处理线性和非线性问题时都很好用，所以我们重点看高斯核。<br>
</p>
<p></p>
高斯径向基核函数表达式：<div class="image-package">
<div class="image-container" style="max-width: 216px; max-height: 37px;">
<div class="image-container-fill" style="padding-bottom: 17.130000000000003%;"></div>
<div class="image-view" data-width="216" data-height="37"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-b3a815b3ca2ab7f2.png" data-original-width="216" data-original-height="37" data-original-format="image/png" data-original-filesize="2006" data-image-index="5" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div><br>
我们可以调的就是公式里的gamma值。我们试着来调一下gamma。
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-go"><code class="  language-go">score_gamma<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
gammaList<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token function">logspace</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> gamma in gammaList<span class="token punctuation">:</span>
    svc <span class="token operator">=</span> <span class="token function">SVC</span><span class="token punctuation">(</span>gamma<span class="token operator">=</span>gamma<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    score_gamma<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>svc<span class="token punctuation">.</span><span class="token function">score</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'gamma={}时,得分最高={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>gammaList<span class="token punctuation">[</span>score_gamma<span class="token punctuation">.</span><span class="token function">index</span><span class="token punctuation">(</span><span class="token function">max</span><span class="token punctuation">(</span>score_gamma<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span>score_gamma<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>gamma=0.004291934260128779时,得分最高=0.986013986013986。<br>
当然，也可以将调参曲线画出来。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-css"><code class="  language-css">plt.<span class="token function">plot</span><span class="token punctuation">(</span>gammaList<span class="token punctuation">,</span>score_gamma<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 379px; max-height: 250px;">
<div class="image-container-fill" style="padding-bottom: 65.96%;"></div>
<div class="image-view" data-width="379" data-height="250"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-0e14851ef61e77c2.png" data-original-width="379" data-original-height="250" data-original-format="image/png" data-original-filesize="7788" data-image-index="6" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption">gamma调参</div>
</div><br>
<p>如果选用的是'poly'多项式核函数，还可以去调一下degree，大家可自行去试。</p>

<h2>2.6 如何避免过拟合(重要参数C)</h2>
<p>现实很残酷，基本是不可能找到一条线能够将两类点完全分开的。或者举一个更明显一点的例子，还是上面那个图，如果这时候新来了两个点1和点2，很明显原来的那条分界线就不能将两类点完全分开了，这时候蓝色的新边界才是能将点完全分开的最好分割线。但也有个问题，使用新的分割线，两类点之间的距离，也就是“边际”（margin）就变窄了，很有可能这两个点本身就是异常点，这时候就需要我们去权衡，是尽可能地将两类点分开呢，还是让大多数点分离得更远。前者呢，就叫做硬间隔（hard margin），这时候有过拟合的风险；后者呢，叫做软间隔（soft margin），会有欠拟合的风险。</p>
<br>
<div class="image-package">
<div class="image-container" style="max-width: 447px; max-height: 384px;">
<div class="image-container-fill" style="padding-bottom: 85.91%;"></div>
<div class="image-view" data-width="447" data-height="384"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-420230f7eba57b9a.png" data-original-width="447" data-original-height="384" data-original-format="image/png" data-original-filesize="15362" data-image-index="7" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div><br>
<p>为了防止过拟合呢，我们会给损失函数加上一个“松弛度”，但作为权衡，会在这个“松弛度”前面加上一个惩罚系数C，防止对分错的点“太过宽松”。<br>
需要注意的是，逻辑回归中也有这么一个惩罚系数C，但它不是在正则项前面，而SVM中的C是在“松弛度”的前面，两个C效果却是一样的，C太大，模型容易过拟合；C太小，模型容易欠拟合。</p>

<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-go"><code class="  language-go">score_C<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
CList<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token function">linspace</span><span class="token punctuation">(</span><span class="token number">0.01</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i in CList<span class="token punctuation">:</span>
    svc <span class="token operator">=</span> <span class="token function">SVC</span><span class="token punctuation">(</span>C<span class="token operator">=</span>i<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    score_C<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>svc<span class="token punctuation">.</span><span class="token function">score</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'C={}时,得分最高={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>CList<span class="token punctuation">[</span>score_C<span class="token punctuation">.</span><span class="token function">index</span><span class="token punctuation">(</span><span class="token function">max</span><span class="token punctuation">(</span>score_C<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span>score_C<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>C=0.6220408163265306时,得分最高=0.986013986013986。<br>
同样可以画出来看一看。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-css"><code class="  language-css">plt.<span class="token function">plot</span><span class="token punctuation">(</span>CList<span class="token punctuation">,</span>score_C<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 379px; max-height: 250px;">
<div class="image-container-fill" style="padding-bottom: 65.96%;"></div>
<div class="image-view" data-width="379" data-height="250"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-2bba424736ef330c.png" data-original-width="379" data-original-height="250" data-original-format="image/png" data-original-filesize="7665" data-image-index="8" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div>
<h2>2.7 如何处理不均衡问题</h2>
<p>同样，调节class_weight。</p>
<h1>三、K近邻(knn算法)</h1>
<p>knn可以说是分类中原理最简单最容易理解的了。</p>
<h2>3.1 sklearn实战</h2>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>neighbors <span class="token keyword">import</span> KNeighborsClassifier
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

knn <span class="token operator">=</span> KNeighborsClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>

y_prob <span class="token operator">=</span> knn<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>                              
y_pred <span class="token operator">=</span> knn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>                                       
fpr_knn<span class="token punctuation">,</span>tpr_knn<span class="token punctuation">,</span>threshold_knn <span class="token operator">=</span> metrics<span class="token punctuation">.</span>roc_curve<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_prob<span class="token punctuation">)</span>   
auc_knn <span class="token operator">=</span> metrics<span class="token punctuation">.</span>auc<span class="token punctuation">(</span>fpr_knn<span class="token punctuation">,</span>tpr_knn<span class="token punctuation">)</span>                              
score_knn <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>score_knn<span class="token punctuation">,</span>auc_knn<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>knn算法的准确率为0.9441, AUC值为0.9766。<br>
这个结果其实已经很不错的，如果再将数据标准化呢？<br>
我们将数据标准化后，得出准确率为0.9930, AUC值为0.9977。这么一个最简单的分类算法，在乳腺癌数据集上的表现居然如此之好。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-undefined"><code class="  language-undefined">knn
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 527px; max-height: 57px;">
<div class="image-container-fill" style="padding-bottom: 10.82%;"></div>
<div class="image-view" data-width="527" data-height="57"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-bba65cd9e79a30b5.png" data-original-width="527" data-original-height="57" data-original-format="image/png" data-original-filesize="3557" data-image-index="9" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div>
<h2>3.2 knn原理</h2>
<p>knn就是字面的意思，选定k个最近的点，看这k个点中占比最多的类作为标签。毫无疑问，k的取值至关重要。</p>
<h2>3.3 K值选择(n_neighbors)</h2>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-go"><code class="  language-go">score_K<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
KList<span class="token operator">=</span><span class="token keyword">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">13</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> k in KList<span class="token punctuation">:</span>
    knn <span class="token operator">=</span> <span class="token function">KNeighborsClassifier</span><span class="token punctuation">(</span>n_neighbors<span class="token operator">=</span>k<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    score_K<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>knn<span class="token punctuation">.</span><span class="token function">score</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'K={}时,得分最高={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>KList<span class="token punctuation">[</span>score_K<span class="token punctuation">.</span><span class="token function">index</span><span class="token punctuation">(</span><span class="token function">max</span><span class="token punctuation">(</span>score_K<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span>score_K<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>K=5时,得分最高=0.9930。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-css"><code class="  language-css">plt.<span class="token function">plot</span><span class="token punctuation">(</span>KList<span class="token punctuation">,</span>score_K<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 379px; max-height: 250px;">
<div class="image-container-fill" style="padding-bottom: 65.96%;"></div>
<div class="image-view" data-width="379" data-height="250"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-73188e68dfae5300.png" data-original-width="379" data-original-height="250" data-original-format="image/png" data-original-filesize="10859" data-image-index="10" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div>
<h2>3.4 k个点的权重(weights)</h2>
<p>我们要去想这么一个问题，假设一个点周围最近的5个点中，有2个点离得非常近，有3个点离得很远。这时候，虽然3个点的类占大头，但直觉告诉我们，它很可能是和离得比较近的那2个点是一类的。所以，可能我们需要给离得近的那2个点更大一点的权重。<br>
weights的默认值是'uniform'，表示所有最近邻样本权重都一样。如果是"distance"，则权重和距离成反比例，即距离预测目标更近的近邻具有更高的权重，这样在预测类别或者做回归时，更近的近邻所占的影响因子会更加大。当然，我们也可以自定义权重，即自定义一个函数，输入是距离值，输出是权重值。这样我们可以自己控制不同的距离所对应的权重。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>neighbors <span class="token keyword">import</span> KNeighborsClassifier
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

knn <span class="token operator">=</span> KNeighborsClassifier<span class="token punctuation">(</span>weights<span class="token operator">=</span><span class="token string">'distance'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>

y_prob <span class="token operator">=</span> knn<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>                              
y_pred <span class="token operator">=</span> knn<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>                                       
fpr_knn<span class="token punctuation">,</span>tpr_knn<span class="token punctuation">,</span>threshold_knn <span class="token operator">=</span> metrics<span class="token punctuation">.</span>roc_curve<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_prob<span class="token punctuation">)</span>   
auc_knn <span class="token operator">=</span> metrics<span class="token punctuation">.</span>auc<span class="token punctuation">(</span>fpr_knn<span class="token punctuation">,</span>tpr_knn<span class="token punctuation">)</span>                              
score_knn <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>score_knn<span class="token punctuation">,</span>auc_knn<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>将weights改为'distance'之后，会发现，精确度为0.9930，而AUC的值竟然上升到了0.9991。</p>
<h2>3.5 距离计算方式(metric)</h2>
<p>knn可以使用的距离度量较多，一般来说默认的欧式距离，但knn里默认的是‘minkowski’。可以来看下不同的距离计算方式下模型的表现。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-php"><code class="  language-php"><span class="token keyword">list</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token single-quoted-string string">'euclidean'</span><span class="token punctuation">,</span><span class="token single-quoted-string string">'manhattan'</span><span class="token punctuation">,</span><span class="token single-quoted-string string">'chebyshev'</span><span class="token punctuation">,</span><span class="token single-quoted-string string">'minkowski'</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i in <span class="token keyword">list</span><span class="token punctuation">:</span>
    knn <span class="token operator">=</span> <span class="token function">KNeighborsClassifier</span><span class="token punctuation">(</span>metric<span class="token operator">=</span>i<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>knn<span class="token punctuation">.</span><span class="token function">score</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>可以看到，euclidean欧氏距离和minkowski闵可夫斯基距离得分最高，同为0.9930，manhattan曼哈顿距离次之为0.9790，chebyshev切比雪夫距离的效果最差为0.9510。</p>
<h1>四、决策树</h1>
<p>接下来我们进入到树这个大类里了，前面三种算法基本都涉及到一个距离的计算，但是树却不用，因此也并不需要标准化（乳腺癌的数据集标准化之后反而降低了得分）。</p>
<h2>4.1sklearn实战</h2>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> tree
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

dtc <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span>                              <span class="token comment"># 建立决策树模型</span>
dtc<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>                                         <span class="token comment"># 训练模型</span>
y_prob <span class="token operator">=</span> dtc<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>                          <span class="token comment"># 预测1类的概率</span>
y_pred <span class="token operator">=</span> dtc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>                                     <span class="token comment"># 模型对测试集的预测结果 </span>
fpr_dtc<span class="token punctuation">,</span>tpr_dtc<span class="token punctuation">,</span>threshod_dtc<span class="token operator">=</span> metrics<span class="token punctuation">.</span>roc_curve<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_prob<span class="token punctuation">)</span>   <span class="token comment"># 获取真阳率、伪阳率、阈值</span>
score_dtc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>                
auc_dtc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>auc<span class="token punctuation">(</span>fpr_dtc<span class="token punctuation">,</span>tpr_dtc<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>score_dtc<span class="token punctuation">,</span>auc_dtc<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>决策树得分0.9441，AUC值为0.9443。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-undefined"><code class="  language-undefined">dtc
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 540px; max-height: 109px;">
<div class="image-container-fill" style="padding-bottom: 20.19%;"></div>
<div class="image-view" data-width="540" data-height="109"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-67b63881ee0fac22.png" data-original-width="540" data-original-height="109" data-original-format="image/png" data-original-filesize="6897" data-image-index="11" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div>
<h2>4.2 决策树原理</h2>
<p>关于决策树的原理也是可以查到很多了，只需要知道这些概念：<br>
①信息量：-lnP<br>
②熵：信息量的期望值(-∑PlnP)<br>
③gini：和信息熵类似(∑P(1-P))<br>
④信息增益<br>
⑤信息增益率<br>
⑥预剪枝方法：预剪枝方法：1限制深度；2限制叶子结点的样本个数；3对每次分裂的信息增益设定阈值。<br>
⑦搞清楚ID3、C4.5、Cart树：<br>
ID3的局限性：1信息增益存在过度拟合的缺点。2不能直接处理连续型变量。3对缺失值敏感。<br>
C4.5&amp; cart改进：1使用信息增益率。2对连续型的自动分箱。<br>
关于熵：<br>
<a href="https://www.jianshu.com/p/8a0ad237b0ed" target="_blank">https://www.jianshu.com/p/8a0ad237b0ed</a></p>
<h2>4.3 如何衡量信息纯度(criterion)</h2>
<p>决策树需要找出最佳的分枝方法，衡量的指标就是信息“不纯度”，“不纯度”越低，说明类别分的越纯，也就是分类越好。<br>
criterion这个参数就是用来选择用何种方式来衡量这个“不纯度”，提供了信息熵(entropy)和基尼系数(gini)两种方法，上面有提到过。但是这个指标基本我们也不用去调，因为sklearn中用的是cart二叉树的方法，cart树的衡量指标就是gini。</p>
<h2>4.4 如何防止过拟合(max_depth &amp; min_samples_leaf &amp; max_features &amp; min_impurity_decrease)</h2>
<p>大家一定要知道，树是天生过拟合的算法，包括之后的随机森林，甚至于以树为基础分类器的集成学习算法，都存在这样一个过拟合的属性。<br>
要防止树的过拟合，第一个要提的就是max_depth，这也是防止过拟合最好的神器，在高维度地样本量时非常好用。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-go"><code class="  language-go">depthList<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token function">arange</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
score<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i in depthList<span class="token punctuation">:</span>
    dtc <span class="token operator">=</span> tree<span class="token punctuation">.</span><span class="token function">DecisionTreeClassifier</span><span class="token punctuation">(</span>max_depth<span class="token operator">=</span>i<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    score<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>dtc<span class="token punctuation">.</span><span class="token function">score</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'最优深度为{}，最佳得分是{}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>depthList<span class="token punctuation">[</span>score<span class="token punctuation">.</span><span class="token function">index</span><span class="token punctuation">(</span><span class="token function">max</span><span class="token punctuation">(</span>score<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span>score<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>最优深度为6，最佳得分是0.9441。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-css"><code class="  language-css">plt.<span class="token function">plot</span><span class="token punctuation">(</span>depthList<span class="token punctuation">,</span>score<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 379px; max-height: 250px;">
<div class="image-container-fill" style="padding-bottom: 65.96%;"></div>
<div class="image-view" data-width="379" data-height="250"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-9a2a29ebae4b09b4.png" data-original-width="379" data-original-height="250" data-original-format="image/png" data-original-filesize="11231" data-image-index="12" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div><br>
<p>min_samples_leaf则限定每个叶子节点上的最小样本个数，防止最后分的太细。<br>
max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃，也是用来限制高维度数据的过拟合的。<br>
min_impurity_decrease则限制信息增益的大小，信息增益小于设定数值的分枝不会发生。<br>
其实可以通过网格搜索来确定以上这些参数的最佳组合。</p>

<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-jsx"><code class="  language-jsx"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> tree
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV

params<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'max_depth'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token operator">*</span>np<span class="token punctuation">.</span><span class="token function">arange</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token punctuation">,</span><span class="token string">'min_samples_leaf'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token operator">*</span>np<span class="token punctuation">.</span><span class="token function">arange</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token punctuation">,</span><span class="token string">'min_impurity_decrease'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token operator">*</span>np<span class="token punctuation">.</span><span class="token function">linspace</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
dtc <span class="token operator">=</span> tree<span class="token punctuation">.</span><span class="token function">DecisionTreeClassifier</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token constant">GS</span> <span class="token operator">=</span> <span class="token function">GridSearchCV</span><span class="token punctuation">(</span>dtc<span class="token punctuation">,</span> params<span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
<span class="token constant">GS</span><span class="token punctuation">.</span>best_params_
<span class="token constant">GS</span><span class="token punctuation">.</span>best_score_
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>得到最佳参数{'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1}，交叉验证最佳得分0.9343。可以看到，加了别的参数的时候，最优深度就不再是6而是4了。</p>
<h2>4.5 树的可视化</h2>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token comment"># 树的可视化</span>
<span class="token keyword">import</span> graphviz
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> export_graphviz

dtc <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>

export_graphviz<span class="token punctuation">(</span>dtc<span class="token punctuation">,</span>out_file<span class="token operator">=</span><span class="token string">'DecisionTree.dot'</span><span class="token punctuation">,</span>class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'0'</span><span class="token punctuation">,</span><span class="token string">'1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>impurity<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>filled<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'DecisionTree.dot'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    dot_graph<span class="token operator">=</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>

graphviz<span class="token punctuation">.</span>Source<span class="token punctuation">(</span>dot_graph<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<h1>五、随机森林</h1>
<p>随机森林，顾名思义，就是很多棵树组合在一起。</p>
<h2>5.1 sklearn实战</h2>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-python"><code class="  language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

rfc <span class="token operator">=</span> RandomForestClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span>                                     <span class="token comment"># 建立随机森林分类器</span>
rfc<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>                                           <span class="token comment"># 训练随机森林模型</span>
y_prob <span class="token operator">=</span> rfc<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>                            <span class="token comment"># 预测1类的概率</span>
y_pred<span class="token operator">=</span>rfc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>                                         <span class="token comment"># 模型对测试集的预测结果</span>
fpr_rfc<span class="token punctuation">,</span>tpr_rfc<span class="token punctuation">,</span>threshold_rfc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>roc_curve<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_prob<span class="token punctuation">)</span>   <span class="token comment"># 获取真阳率、伪阳率、阈值  </span>
auc_rfc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>auc<span class="token punctuation">(</span>fpr_rfc<span class="token punctuation">,</span>tpr_rfc<span class="token punctuation">)</span>                             <span class="token comment"># AUC得分</span>
score_rfc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>                  <span class="token comment"># 模型准确率</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>score_rfc<span class="token punctuation">,</span>auc_rfc<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>模型准确率为0.9580，AUC值为0.9942。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-undefined"><code class="  language-undefined">rfc
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 569px; max-height: 128px;">
<div class="image-container-fill" style="padding-bottom: 22.5%;"></div>
<div class="image-view" data-width="569" data-height="128"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-1b6471c7073537ac.png" data-original-width="569" data-original-height="128" data-original-format="image/png" data-original-filesize="8292" data-image-index="13" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div>
<h2>5.2 到底种多少棵树(n_estimators)</h2>
<p>我们可以在5.1中看到随机森林默认的n_estimators是10，显然，对于集成学习来说，10棵树的数量是偏少了的。好像更新的版本之后n_estimators默认值会变为100。我们来看下随着建的树越多，训练集和测试集数据的得分会有什么样的变化。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-go"><code class="  language-go">from sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier
from sklearn <span class="token keyword">import</span> metrics

train_score<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
test_score<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i in <span class="token keyword">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">101</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    rfc <span class="token operator">=</span> <span class="token function">RandomForestClassifier</span><span class="token punctuation">(</span>n_estimators<span class="token operator">=</span>i<span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">13</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    train_score<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>rfc<span class="token punctuation">.</span><span class="token function">score</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span><span class="token punctuation">)</span>
    test_score<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>rfc<span class="token punctuation">.</span><span class="token function">score</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
plt<span class="token punctuation">.</span><span class="token function">figure</span><span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">plot</span><span class="token punctuation">(</span><span class="token keyword">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">101</span><span class="token punctuation">)</span><span class="token punctuation">,</span>train_score<span class="token punctuation">,</span>color<span class="token operator">=</span><span class="token string">'orange'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">plot</span><span class="token punctuation">(</span><span class="token keyword">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">101</span><span class="token punctuation">)</span><span class="token punctuation">,</span>test_score<span class="token punctuation">,</span>color<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'test'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">legend</span><span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'lower right'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 700px; max-height: 304px;">
<div class="image-container-fill" style="padding-bottom: 26.179999999999996%;"></div>
<div class="image-view" data-width="1161" data-height="304"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-48eea9c306d08230.png" data-original-width="1161" data-original-height="304" data-original-format="image/png" data-original-filesize="23487" data-image-index="14" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption"></div>
</div><br>
<p>可以看到，随着树的数量增多，训练集的效果明显变好，测试集的得分有所上升，但变化不大。初步目测，n_estimators在60左右效果是最好的。</p>

<h2>5.3 如何防止过拟合</h2>
<p>这个跟树是几乎一样，因为咱本身也是树组成的。调节几个重要参数就好了：max_depth、min_samples_leaf、max_features、min_impurity_decrease。</p>
<h1>六、二分类模型评价指标</h1>
<p>二分类模型评价指标围绕混淆矩阵展开，这个也太经典了，不在这儿详说了：<br>
<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_27575895%2Farticle%2Fdetails%2F81476871" target="_blank">https://blog.csdn.net/qq_27575895/article/details/81476871</a><br>
以随机森林为例，看一看precision和recall值。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-bash"><code class="  language-bash">metrics.precision_score(y_test,y_pred) # 精确率
metrics.recall_score(y_test,y_pred) # 召回率
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div>
<p>随机森林模型下精确率和召回率均为0.9759。</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"></path></svg></i></button><pre class="line-numbers  language-bash"><code class="  language-bash">plt.style.use('bmh')
plt.figure(figsize=(13,10))

plt.plot(fpr_lr,tpr_lr,label='lr')                             # 逻辑回归
plt.plot(fpr_svc,tpr_svc,label='svc')                          # 支持向量机模型
plt.plot(fpr_knn,tpr_knn,label='knn')                             # K近邻
plt.plot(fpr_dtc,tpr_dtc,label='dtc')                          # 决策树
plt.plot(fpr_rfc,tpr_rfc,label='rfc')                          # 随机森林

plt.legend(loc='lower right',prop={'size':25})
plt.xlabel('伪阳率')
plt.ylabel('真阳率')
plt.title('ROC曲线')
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<div class="image-package">
<div class="image-container" style="max-width: 700px; max-height: 602px;">
<div class="image-container-fill" style="padding-bottom: 77.88000000000001%;"></div>
<div class="image-view" data-width="773" data-height="602"><img data-original-src="//upload-images.jianshu.io/upload_images/18317213-b503be1e31589c94.png" data-original-width="773" data-original-height="602" data-original-format="image/png" data-original-filesize="44917" data-image-index="15" style="cursor: zoom-in;" class="image-loading"></div>
</div>
<div class="image-caption">ROC曲线</div>
</div>
<p>当然，小白也是机器学习初学者，有什么理解错误的地方也欢迎指正。这篇文也是针对初学者做的一个总结，希望能够有帮助。本文后续还会不断更新，加入新的分类算法，加入新的调参参数。</p>
</article><div></div><div class="_1kCBjS"><div class="_18vaTa"><div class="_3BUZPB"><div class="_2Bo4Th" role="button" tabindex="-1" aria-label="给文章点赞"><i aria-label="ic-like" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-like"></use></svg></i></div><span class="_1LOh_5" role="button" tabindex="-1" aria-label="查看点赞列表">1人点赞<i aria-label="icon: right" class="anticon anticon-right"><svg viewBox="64 64 896 896" focusable="false" class="" data-icon="right" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M765.7 486.8L314.9 134.7A7.97 7.97 0 0 0 302 141v77.3c0 4.9 2.3 9.6 6.1 12.6l360 281.1-360 281.1c-3.9 3-6.1 7.7-6.1 12.6V883c0 6.7 7.7 10.4 12.9 6.3l450.8-352.1a31.96 31.96 0 0 0 0-50.4z"></path></svg></i></span></div><div class="_3BUZPB"><div class="_2Bo4Th" role="button" tabindex="-1"><i aria-label="ic-dislike" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-dislike"></use></svg></i></div></div></div><div class="_18vaTa"><a class="_3BUZPB _1x1ok9 _1OhGeD" href="https://www.jianshu.com/nb/37825232" target="_blank" rel="noopener noreferrer"><i aria-label="ic-notebook" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-notebook"></use></svg></i><span>日记本</span></a><div class="_3BUZPB ant-dropdown-trigger"><div class="_2Bo4Th"><i aria-label="ic-others" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-others"></use></svg></i></div></div></div></div><div class="_19DgIp" style="margin-top:24px;margin-bottom:24px"></div><div class="_13lIbp"><div class="_16AzcO">更多精彩内容，就在简书APP</div><div class="_6S_NkV"><canvas height="126" width="126" style="height: 110px; width: 110px;"></canvas><img src="data:image/png;base64,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" style="display: none;"></div><div class="l8ZVfE"></div><div class="_191KSt">"小礼物走一走，来简书关注我"</div><button type="button" class="_1OyPqC _3Mi9q9 _2WY0RL _1YbC5u"><span>赞赏支持</span></button><span class="_3zdmIj">还没有人赞赏，支持一下</span></div><div class="d0hShY"><a class="_1bPVBH _1OhGeD" href="https://www.jianshu.com/u/1eb8ed4c42ca" target="_blank" rel="noopener noreferrer"><img class="_27NmgV" src="./一文入门sklearn二分类实战 - 简书_files/fc1ef06e-6f88-4559-9010-33832c3baec3(2)" alt="  "></a><div class="Uz-vZq"><div class="Cqpr1X"><a class="HC3FFO _1OhGeD" href="https://www.jianshu.com/u/1eb8ed4c42ca" title="猴小白" target="_blank" rel="noopener noreferrer">猴小白</a><span class="_2WEj6j" title=""></span></div><div class="lJvI3S"><span>总资产12 (约0.81元)</span><span>共写了2.3W字</span><span>获得97个赞</span><span>共102个粉丝</span></div></div><button data-locale="zh-CN" type="button" class="_1OyPqC _3Mi9q9"><span>关注</span></button></div></section><section class="-umr26" aria-label="xingchen-ad"><div id="_xingchen_8zhgjz2cpy"></div><script type="text/javascript" smua="d=p&amp;s=b&amp;u=u3161574&amp;w=728&amp;h=220" src="./一文入门sklearn二分类实战 - 简书_files/o.js" async="" defer=""></script><div><iframe frameborder="0" scrolling="no" width="728" height="220" src="./一文入门sklearn二分类实战 - 简书_files/j.html" style="width: 728px; height: 220px; margin: 0px; padding: 0px; border: 0px;"></iframe></div></section><div id="note-page-comment"><div class="lazyload-placeholder"></div></div><section class="ouvJEz"><h3 class="QxT4hD"><span>推荐阅读</span><a class="_29KFEa _1OhGeD" href="https://www.jianshu.com/" target="_blank" rel="noopener noreferrer">更多精彩内容<i aria-label="ic-right" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-right"></use></svg></i></a></h3><ul class="_1iTR78"><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="跟我一起学scikit-learn19：支持向量机算法" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="https://www.jianshu.com/p/3675bd120bdd" target="_blank" rel="noopener noreferrer">跟我一起学scikit-learn19：支持向量机算法</a></div><div class="_3fvgn4">支持向量机（SVM，Support Vector Machine）算法是一种常见的分类算法，在工业界和学术界都有广...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="https://www.jianshu.com/u/026641a8a396" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="./一文入门sklearn二分类实战 - 简书_files/0f5901d4-45ed-46f7-8474-6bd351218f08.jpeg" alt=""><span class="_3tPsL6">金字塔下的小蜗牛</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->497</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->2</span></div></div><a class="_10MMAm _1OhGeD" href="https://www.jianshu.com/p/3675bd120bdd" target="_blank" rel="noopener noreferrer"><img class="_3zGDUj" src="./一文入门sklearn二分类实战 - 简书_files/17634123-7eb1626cdda4f92e.png" alt=""></a></li><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="自然语言处理神经网络模型入门" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="https://www.jianshu.com/p/4310dcf89c31" target="_blank" rel="noopener noreferrer">自然语言处理神经网络模型入门</a></div><div class="_3fvgn4">主要内容 自然语言输入编码 前馈网络 卷积网络 循环网络(recurrent networks ) 递归网络(re...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="https://www.jianshu.com/u/57c288f38246" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="./一文入门sklearn二分类实战 - 简书_files/9-cceda3cf5072bcdd77e8ca4f21c40998.jpg" alt=""><span class="_3tPsL6">JackHorse</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->3,241</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->2</span></div></div></li><section class="-umr26" aria-label="thirdparty-ad"><div id="_thirdparty_td7wsput88k"></div><script type="text/javascript" smua="d=p&amp;s=b&amp;u=u3163395&amp;w=728&amp;h=220" src="./一文入门sklearn二分类实战 - 简书_files/o.js" async="" defer=""></script><div><iframe frameborder="0" scrolling="no" width="728" height="220" src="./一文入门sklearn二分类实战 - 简书_files/j(1).html" style="width: 728px; height: 220px; margin: 0px; padding: 0px; border: 0px;"></iframe></div></section><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="机器学习笔记" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="https://www.jianshu.com/p/2f8437adcc36" target="_blank" rel="noopener noreferrer">机器学习笔记</a></div><div class="_3fvgn4">以西瓜书为主线，以其他书籍作为参考进行补充，例如《统计学习方法》，《PRML》等 第一章 绪论 1.2 基本术语 ...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="https://www.jianshu.com/u/8118056321a8" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="./一文入门sklearn二分类实战 - 简书_files/1-04bbeead395d74921af6a4e8214b4f61.jpg" alt=""><span class="_3tPsL6">danielAck</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->2,497</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->6</span></div></div><a class="_10MMAm _1OhGeD" href="https://www.jianshu.com/p/2f8437adcc36" target="_blank" rel="noopener noreferrer"><img class="_3zGDUj" src="./一文入门sklearn二分类实战 - 简书_files/11420610-2df5bd01fc9e8fea.jpg" alt=""></a></li><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="sklearn文档 — 1.7. 高斯过程" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="https://www.jianshu.com/p/4400f186052d" target="_blank" rel="noopener noreferrer">sklearn文档 — 1.7. 高斯过程</a></div><div class="_3fvgn4">原文章为scikit-learn中"用户指南"--&gt;"监督学习的第七节：Gaussian Processes"##...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="https://www.jianshu.com/u/22f9710c20ed" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="./一文入门sklearn二分类实战 - 简书_files/1-04bbeead395d74921af6a4e8214b4f61.jpg" alt=""><span class="_3tPsL6">HabileBadger</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->14,708</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->9</span></div></div><a class="_10MMAm _1OhGeD" href="https://www.jianshu.com/p/4400f186052d" target="_blank" rel="noopener noreferrer"><img class="_3zGDUj" src="./一文入门sklearn二分类实战 - 简书_files/3818374-d9bfcb3daf33e9cf.png" alt=""></a></li><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="【机器学习实战（三）】sklearn包中SVM算法库的使用" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="https://www.jianshu.com/p/9dbeb26f985c" target="_blank" rel="noopener noreferrer">【机器学习实战（三）】sklearn包中SVM算法库的使用</a></div><div class="_3fvgn4">目录 SVM相关知识点回顾1.1. SVM与SVR1.2. 核函数sklearn中SVM相关库的简介2.1. 分类...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="https://www.jianshu.com/u/42e711970fa8" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="./一文入门sklearn二分类实战 - 简书_files/9-cceda3cf5072bcdd77e8ca4f21c40998.jpg" alt=""><span class="_3tPsL6">UnderStorm</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->4,760</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->13</span></div></div><a class="_10MMAm _1OhGeD" href="https://www.jianshu.com/p/9dbeb26f985c" target="_blank" rel="noopener noreferrer"><img class="_3zGDUj" src="./一文入门sklearn二分类实战 - 简书_files/15455040-939b45e52befbd2a.png" alt=""></a></li></ul></section></div><aside class="_2OwGUo"><section class="-umr26" aria-label="ad360-ad"><div id="mvdiv_2554567_holder"><iframe scrolling="no" name="ifr2554567" frameborder="0" data-src="https://show-3.mediav.com/s?ver=1.2.11&amp;enifr=1&amp;showid=z0AUSc&amp;type=1&amp;of=2&amp;newf=1&amp;uid=16203547022892683363430438427178&amp;isifr=0&amp;title=%E4%B8%80%E6%96%87%E5%85%A5%E9%97%A8sklearn%E4%BA%8C%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98%20-%20%E7%AE%80&amp;refurl=&amp;size=260x250&amp;scheme=https&amp;tmprtp=" data-scroll-inview="no" data-inviewed="yes" src="./一文入门sklearn二分类实战 - 简书_files/s.html" style="width: 260px; height: 250px;"></iframe></div><script type="text/javascript" mediav_ad_width="260" mediav_ad_height="250" mediav_ad_pub="z0AUSc_2554567" src="./一文入门sklearn二分类实战 - 简书_files/mvf_g4.js" async="" defer=""></script></section><section class="_3Z3nHf"><div class="_3Oo-T1"><a class="_1b5rv9 _1OhGeD" href="https://www.jianshu.com/u/1eb8ed4c42ca" target="_blank" rel="noopener noreferrer"><img class="_3T9iJQ" src="./一文入门sklearn二分类实战 - 简书_files/fc1ef06e-6f88-4559-9010-33832c3baec3(3)" alt=""></a><div class="_32ZTTG"><div class="_2O0T_w"><div class="_2v-h3G"><span class="_2vh4fr" title="猴小白"><a class="_1OhGeD" href="https://www.jianshu.com/u/1eb8ed4c42ca" target="_blank" rel="noopener noreferrer">猴小白</a></span></div><button data-locale="zh-CN" type="button" class="tzrf9N _1OyPqC _3Mi9q9 _34692-"><span>关注</span></button></div><div class="_1pXc22">总资产12 (约0.81元)</div></div></div><div class="_19DgIp"></div><div class="_26Hhi2" role="listitem"><div class="_3TNGId" title="python批处理excel"><a class="_2ER8Tt _1OhGeD" href="https://www.jianshu.com/p/2fcfa32e8376" target="_blank" rel="noopener noreferrer">python批处理excel</a></div><div class="DfvGP9">阅读 2,006</div></div><div class="_26Hhi2" role="listitem"><div class="_3TNGId" title="PostgreSQL入门"><a class="_2ER8Tt _1OhGeD" href="https://www.jianshu.com/p/13443c6f3e34" target="_blank" rel="noopener noreferrer">PostgreSQL入门</a></div><div class="DfvGP9">阅读 702</div></div></section><div><div aria-hidden="true" style="width: 260px; height: 368.709px;"></div><div class="ant-affix" style="position: fixed; top: 66px; width: 260px; height: 368.709px;"><section class="_3Z3nHf"><h3 class="QHRnq8 QxT4hD"><span>推荐阅读</span></h3><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="集成学习（5） -  基本分类模型"><a class="_1-HJSV _1OhGeD" href="https://www.jianshu.com/p/7d2b349d6ef2" target="_blank" rel="noopener noreferrer">集成学习（5） -  基本分类模型</a></div><div class="_19haGh">阅读 276</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="机器学习(5) part2 of SVM——面试必考之支持向量机"><a class="_1-HJSV _1OhGeD" href="https://www.jianshu.com/p/b06ece80b366" target="_blank" rel="noopener noreferrer">机器学习(5) part2 of SVM——面试必考之支持向量机</a></div><div class="_19haGh">阅读 81</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="机器学习：11. SVM支持向量机（上）"><a class="_1-HJSV _1OhGeD" href="https://www.jianshu.com/p/4b75822a2bff" target="_blank" rel="noopener noreferrer">机器学习：11. SVM支持向量机（上）</a></div><div class="_19haGh">阅读 615</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="机器学习：02.sklearn-回归"><a class="_1-HJSV _1OhGeD" href="https://www.jianshu.com/p/f12b9fd98f3d" target="_blank" rel="noopener noreferrer">机器学习：02.sklearn-回归</a></div><div class="_19haGh">阅读 387</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="机器学习：01.sklearn-决策树"><a class="_1-HJSV _1OhGeD" href="https://www.jianshu.com/p/d181f542f353" target="_blank" rel="noopener noreferrer">机器学习：01.sklearn-决策树</a></div><div class="_19haGh">阅读 521</div></div></section></div></div></aside><div class="ant-back-top"><div class="_3MyrRP" role="button" tabindex="-1" aria-label="回到顶部"><i aria-label="icon: caret-up" class="anticon anticon-caret-up"><svg viewBox="0 0 1024 1024" focusable="false" class="" data-icon="caret-up" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M858.9 689L530.5 308.2c-9.4-10.9-27.5-10.9-37 0L165.1 689c-12.2 14.2-1.2 35 18.5 35h656.8c19.7 0 30.7-20.8 18.5-35z"></path></svg></i></div></div></div></div><footer style="width:100%"><div class="_2xr8G8"><div class="_1Jdfvb"><div class="TDvCqd"><textarea class="W2TSX_" placeholder="写下你的评论..."></textarea></div><div class="-pXE92"><div class="_3nj4GN" role="button" tabindex="0" aria-label="添加评论"><i aria-label="ic-reply" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-reply"></use></svg></i><span>评论<!-- -->0</span></div><div class="_3nj4GN" role="button" tabindex="0" aria-label="给文章点赞"><i aria-label="ic-like" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-like"></use></svg></i><span>赞<!-- -->1</span></div><div class="_3nj4GN ant-dropdown-trigger" role="button" tabindex="0" aria-label="更多操作"><i aria-label="ic-others" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-others"></use></svg></i></div></div></div></div><div class="_1LI0En" style="height: 55.9844px;"></div></footer><div class="_27yofX _2gYj97"><div role="button" tabindex="-1" aria-label="抽奖关闭" class="_1bKQQB"></div><div class="TlIPNx" role="button" tabindex="0"><div class="_3eg6aX">抽奖</div><img class="_3ONY7G" src="data:image/png;base64,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" alt="reward"></div></div><div class="_3Pnjry"><div class="_1pUUKr"><div class="_2VdqdF" role="button" tabindex="-1" aria-label="给文章点赞"><i aria-label="ic-like" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-like"></use></svg></i></div><div class="P63n6G"><div class="_2LKTFF"><span class="_1GPnWJ" role="button" tabindex="-1" aria-label="查看点赞列表">1<!-- -->赞</span><span class="_1GPnWJ">2<!-- -->赞</span></div></div></div><div class="_1pUUKr"><div class="_2VdqdF" role="button" tabindex="-1" aria-label="赞赏作者"><i aria-label="ic-shang" class="anticon"><svg width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xlink:href="#ic-shang"></use></svg></i></div><div class="P63n6G" role="button" tabindex="-1" aria-label="查看赞赏列表">赞赏</div></div><div class="_1pUUKr"><div class="_2VdqdF _1fDw5l"><span class="t-eN3x RhY_sp"></span></div><div class="P63n6G">更多好文</div></div></div></div><script async="" src="./一文入门sklearn二分类实战 - 简书_files/hm.js"></script><script async="" src="./一文入门sklearn二分类实战 - 简书_files/analytics.js"></script><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"isServer":true,"initialState":{"global":{"done":false,"artFromType":null,"fontType":"black","$modal":{"ContributeModal":false,"RewardListModal":false,"PayModal":false,"CollectionModal":false,"LikeListModal":false,"ReportModal":false,"QRCodeShareModal":false,"BookCatalogModal":false,"RewardModal":false},"$ua":{"value":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36","isIE11":false,"earlyIE":null,"chrome":"86.0","firefox":null,"safari":null,"isMac":false},"$diamondRate":{"displayable":false,"rate":0},"readMode":"day","locale":"zh-CN","seoList":[{"comments_count":0,"public_abbr":"支持向量机（SVM，Support Vector Machine）算法是一种常见的分类算法，在工业界和学术界都有广...","share_image_url":"https://upload-images.jianshu.io/upload_images/17634123-7eb1626cdda4f92e.png","slug":"3675bd120bdd","user":{"id":17634123,"nickname":"金字塔下的小蜗牛","slug":"026641a8a396","avatar":"https://upload.jianshu.io/users/upload_avatars/17634123/0f5901d4-45ed-46f7-8474-6bd351218f08.jpeg"},"likes_count":2,"title":"跟我一起学scikit-learn19：支持向量机算法","id":48785493,"views_count":497},{"comments_count":0,"public_abbr":"主要内容 自然语言输入编码 前馈网络 卷积网络 循环网络(recurrent networks ) 递归网络(re...","share_image_url":"","slug":"4310dcf89c31","user":{"id":7388873,"nickname":"JackHorse","slug":"57c288f38246","avatar":"https://cdn2.jianshu.io/assets/default_avatar/9-cceda3cf5072bcdd77e8ca4f21c40998.jpg"},"likes_count":2,"title":"自然语言处理神经网络模型入门","id":35119492,"views_count":3241},{"comments_count":0,"public_abbr":"以西瓜书为主线，以其他书籍作为参考进行补充，例如《统计学习方法》，《PRML》等 第一章 绪论 1.2 基本术语 ...","share_image_url":"http://upload-images.jianshu.io/upload_images/11420610-2df5bd01fc9e8fea.jpg","slug":"2f8437adcc36","user":{"id":11420610,"nickname":"danielAck","slug":"8118056321a8","avatar":"https://cdn2.jianshu.io/assets/default_avatar/1-04bbeead395d74921af6a4e8214b4f61.jpg"},"likes_count":6,"title":"机器学习笔记","id":31354264,"views_count":2497},{"comments_count":0,"public_abbr":"原文章为scikit-learn中\"用户指南\"--\u003e\"监督学习的第七节：Gaussian Processes\"##...","share_image_url":"http://upload-images.jianshu.io/upload_images/3818374-d9bfcb3daf33e9cf.png","slug":"4400f186052d","user":{"id":3818374,"nickname":"HabileBadger","slug":"22f9710c20ed","avatar":"https://cdn2.jianshu.io/assets/default_avatar/1-04bbeead395d74921af6a4e8214b4f61.jpg"},"likes_count":9,"title":"sklearn文档 — 1.7. 高斯过程","id":9146791,"views_count":14708},{"comments_count":0,"public_abbr":"目录 SVM相关知识点回顾1.1. SVM与SVR1.2. 核函数sklearn中SVM相关库的简介2.1. 分类...","share_image_url":"http://upload-images.jianshu.io/upload_images/15455040-939b45e52befbd2a.png","slug":"9dbeb26f985c","user":{"id":15455040,"nickname":"UnderStorm","slug":"42e711970fa8","avatar":"https://cdn2.jianshu.io/assets/default_avatar/9-cceda3cf5072bcdd77e8ca4f21c40998.jpg"},"likes_count":13,"title":"【机器学习实战（三）】sklearn包中SVM算法库的使用","id":41381552,"views_count":4760}]},"note":{"data":{"is_author":false,"last_updated_at":1565141807,"public_title":"一文入门sklearn二分类实战","purchased":false,"liked_note":false,"comments_count":0,"free_content":"\u003cp\u003e在小白我的第一篇文里就提出过一个问题，就是现在的教程都太“分散”太“板块”，每一个知识点都单独用一个例子，机器学习算法里也是这样的，可能逻辑回归用葡萄酒的案例讲，决策树又用鸢尾花的数据集了。今天，我们只用乳腺癌数据集，专门来看一看二分类问题的实战应用。\u003c/p\u003e\n\u003cp\u003e导入包\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 解决坐标轴刻度负号乱码\nplt.rcParams['axes.unicode_minus'] = False\n# 解决中文乱码问题\nplt.rcParams['font.sans-serif'] = ['Simhei']\n\n%matplotlib inline\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e导入数据\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003e# 读入癌症数据集\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\ncancer=datasets.load_breast_cancer()\nX=cancer.data\ny=cancer.target\n\nX_train,X_test,y_train,y_test=train_test_split(X,y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eprint('训练集维度:{}\\n测试集维度:{}'.format(X_train.shape,X_test.shape))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e训练集维度:(426, 30)\u003cbr\u003e\n测试集维度:(143, 30)\u003c/p\u003e\n\u003ch1\u003e一、逻辑回归\u003c/h1\u003e\n\u003cp\u003e我们先来看看分类里最简单却又是最经典的逻辑回归：\u003c/p\u003e\n\u003ch2\u003e1.1 sklearn实战\u003c/h2\u003e\n\u003cp\u003e废话不多说，看看用sklearn怎么做逻辑回归。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlr = LogisticRegression()                                        # 实例化一个LR模型\nlr.fit(X_train,y_train)                                          # 训练模型\ny_prob = lr.predict_proba(X_test)[:,1]                           # 预测1类的概率\ny_pred = lr.predict(X_test)                                      # 模型对测试集的预测结果\nfpr_lr,tpr_lr,threshold_lr = metrics.roc_curve(y_test,y_prob)    # 获取真阳率、伪阳率、阈值\nauc_lr = metrics.auc(fpr_lr,tpr_lr)                              # AUC得分\nscore_lr = metrics.accuracy_score(y_test,y_pred)                 # 模型准确率\nprint([score_lr,auc_lr])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e得到准确率为0.9720, AUC值为0.9954。这个结果可以说是相当不错了。\u003cbr\u003e\n可以查看一下LR模型的默认参数。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elr\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 550px; max-height: 93px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 16.91%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"550\" data-height=\"93\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-753d97319b161c5c.png\" data-original-width=\"550\" data-original-height=\"93\" data-original-format=\"image/png\" data-original-filesize=\"5898\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003eLR默认参数\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2\u003e1.2 对逻辑回归的理解\u003c/h2\u003e\n\u003cp\u003e这里的乳腺癌数据集有30个特征，我们需要根据这些特征去判断一个是否会患乳腺癌。不妨换一个思路，如果按照我们熟悉的线性回归，这个问题该怎么思考呢？那么我们会根据每个人自身的特征给他打个分，我们规定分数越高就越不健康。进一步，我们希望把每个人的得分进行归一化，让分数限制在[0,1]这个区间，这样，打分的问题就变成了概率问题。\u003cbr\u003e\n那怎么把得分变成概率呢？有很多方法，最经典的就要说一说sigmoid函数了。\u003cbr\u003e\n\u003ca href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cnblogs.com%2Fxitingxie%2Fp%2F9924523.html\" target=\"_blank\"\u003ehttps://www.cnblogs.com/xitingxie/p/9924523.html\u003c/a\u003e\u003cbr\u003e\n我随意贴了一个网上找的比较全的sigmoid函数解析，偷个懒。\u003c/p\u003e\n\u003ch2\u003e1.3 损失函数\u003c/h2\u003e\n\u003cp\u003e在这里，不得不说一下，小白我刚开始接触逻辑回归的时候，就掉入了这个坑，LR带有回归二字，我想当然地就认为损失函数肯定和回归一样嘛，真实值和预测值之间差值的平方和嘛！but，它本质其实还是分类问题啊，分类模型的损失函数，是交叉熵。\u003cbr\u003e\n熵这个问题可能又会让很多初学者很困扰了，我刚开始学的时候也是一知半解，在这里分享一篇觉得写的很好的交叉熵的文：\u003cbr\u003e\n\u003ca href=\"https://www.jianshu.com/p/8a0ad237b0ed\" target=\"_blank\"\u003ehttps://www.jianshu.com/p/8a0ad237b0ed\u003c/a\u003e\u003cbr\u003e\n另外贴一篇损失函数推导：\u003cbr\u003e\n\u003ca href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cnblogs.com%2Fshayue%2Fp%2F10520414.html\" target=\"_blank\"\u003ehttps://www.cnblogs.com/shayue/p/10520414.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e1.4 求解损失函数(max_iter)\u003c/h2\u003e\n\u003cp\u003e使用梯度下降。这个经典方法我在这儿也不过多说了，同样，贴个链接大家可以自己去看：\u003cbr\u003e\n\u003ca href=\"https://www.jianshu.com/p/93d9fea7f4c2\" target=\"_blank\"\u003ehttps://www.jianshu.com/p/93d9fea7f4c2\u003c/a\u003e\u003cbr\u003e\n我们这里只来看一看max_iter这个参数，它表示梯度下降法中最大迭代次数，max_iter越大，代表步长越小，模型迭代时间越长。我们来看一看效果。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etrainList=[]  # 用来记录训练集得分\ntestList=[]   # 用来记录测试集得分\n\nfor i in range(1,101,10):\n    lr=LogisticRegression(max_iter=i).fit(X_train,y_train)\n    trainList.append(metrics.accuracy_score(y_train,lr.predict(X_train)))\n    testList.append(metrics.accuracy_score(y_test,lr.predict(X_test)))\n    \nplt.figure(figsize=(20, 5))\nplt.style.use('seaborn-colorblind')\nplt.plot(range(1,101,10),trainList,color='orange',label='train')\nplt.plot(range(1,101,10),testList,color='red',label='train')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 700px; max-height: 304px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 26.21%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"1160\" data-height=\"304\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-599af8d696612c30.png\" data-original-width=\"1160\" data-original-height=\"304\" data-original-format=\"image/png\" data-original-filesize=\"19460\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003emax_iter\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cp\u003e我们可以看到，基本迭代到30次的时候测试集和训练集的得分就不再提高了，继续增大迭代次数也只是浪费了资源。\u003c/p\u003e\n\n\u003ch2\u003e1.5 如何防止过拟合(penalty \u0026amp; C)\u003c/h2\u003e\n\u003cp\u003e逻辑回归和线性回归一样，为了防止过拟合，可以在损失函数的后面添加正则项。同样的，我也贴个详解的连接在后面，但大家只要知道我们有l1和l2两种正则项，sklearn里大多数算法默认的都是l2范式，它会让特征前的系数尽量小防止过拟合，但不会像l1范式那样直接将某些系数减小到0：\u003cbr\u003e\n\u003ca href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fred_stone1%2Farticle%2Fdetails%2F80755144\" target=\"_blank\"\u003ehttps://blog.csdn.net/red_stone1/article/details/80755144\u003c/a\u003e\u003cbr\u003e\n在sklearn中，通过penalty选择l1还是l2范式，而通过C控制对系数的惩罚力度。要注意的是，C是加在损失函数前面的，而不是加在正则项前面，因此C越大，表示对系数的惩罚力度越小，模型越容易过拟合。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elr1=LogisticRegression(penalty='l1').fit(X_train,y_train)\nlr2=LogisticRegression(penalty='l2').fit(X_train,y_train)\n\nprint('L1正则得分：{}\\nL2正则得分：{}'.format(metrics.accuracy_score(y_test,lr1.predict(X_test)),metrics.accuracy_score(y_test,lr2.predict(X_test))))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e当然，这个数据集过于理想，因此使用l1和l2基本没有不同。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elist1=[]  # 用来记录l1正则得分\nlist2=[]   # 用来记录l2正则得分\n\nfor i in np.linspace(0.05,1,20):\n    lr1=LogisticRegression(penalty='l1',C=i).fit(X_train,y_train)\n    lr2=LogisticRegression(penalty='l2',C=i).fit(X_train,y_train)\n    list1.append(metrics.accuracy_score(y_test,lr1.predict(X_test)))\n    list2.append(metrics.accuracy_score(y_test,lr2.predict(X_test)))\n    \nplt.figure(figsize=(20, 5))\nplt.style.use('seaborn-colorblind')\nplt.plot(np.linspace(0.05,1,20),list1,color='orange',label='l1-test')\nplt.plot(np.linspace(0.05,1,20),list2,color='red',label='l2-test')\nplt.legend(loc='lower right')\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 700px; max-height: 304px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 26.07%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"1166\" data-height=\"304\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-9e16571696d1e629.png\" data-original-width=\"1166\" data-original-height=\"304\" data-original-format=\"image/png\" data-original-filesize=\"26462\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e正则系数C\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cp\u003e我们一般是要防止过拟合，所以对C的调参一般是在[0,1]之间调整。可以看到，加入C之后，l1和l2的区别就出来了。C取默认值1.0的时候其实就是得分最高的了，再调整C意义不大。\u003c/p\u003e\n\n\u003ch2\u003e1.6 如何处理样本不均衡(class_weight)\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003epd.Series(y).value_counts()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e在乳腺癌的数据集中，正例：负例是357:212，并不存在样本不均衡问题。但在实际现实问题建模时候，样本不均衡却是常态，比如在处理信用违约时，不违约的人总是多数，而违约的人是非常少的，除了我们常用的上采用的方法，还可以通过class_weight参数调节，在这里就不举例了。\u003c/p\u003e\n\u003ch1\u003e二、支持向量机\u003c/h1\u003e\n\u003cp\u003e支持向量机在深度学习没有兴起的时候那可是不可撼动的王者地位，当然，它这么好用，也可想而知，这家伙肯定是很难很复杂的。\u003c/p\u003e\n\u003ch2\u003e2.1 sklearn实战\u003c/h2\u003e\n\u003cp\u003e同样，废话不多说，先来看支持向量机算法在乳腺癌数据集上的表现。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.svm import SVC\nfrom sklearn import metrics\n\nsvc = SVC().fit(X_train,y_train)\n\ny_prob = svc.decision_function(X_test)                              # 决策边界距离\ny_pred = svc.predict(X_test)                                        # 模型对测试集的预测结果\nfpr_svc,tpr_svc,threshold_svc = metrics.roc_curve(y_test,y_prob)     # 获取真阳率、伪阳率、阈值\nauc_svc = metrics.auc(fpr_svc,tpr_svc)                              # 模型准确率\nscore_svc = metrics.accuracy_score(y_test,y_pred)\nprint([score_svc,auc_svc])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e可以得到，在不调任何参数的情况下，准确率为0.6294, AUC值为0.9126。我们会发现AUC值很高，但是准确率却不高。其实，通过metrics.precision_score(y_test,y_pred)和metrics.recall_score(y_test,y_pred)，我们会发现精确率很低只有0.6294，但召回率为1.0。（什么是精确率和召回率大家也可以自行百度下）\u003cbr\u003e\n同样，我们可以看一下svc的默认参数。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esvc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 494px; max-height: 73px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 14.78%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"494\" data-height=\"73\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-bbcc81ddeb55aa3c.png\" data-original-width=\"494\" data-original-height=\"73\" data-original-format=\"image/png\" data-original-filesize=\"5045\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003esvc默认参数\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2\u003e2.2 对支持向量机的理解\u003c/h2\u003e\n\u003cp\u003e我们之前说逻辑回归的时候，通过最好理解的线性回归举例，通过sigmoid函数将一个本该是线性回归的问题转化为了一个线性分类的问题。那么，这个时候问题来了，如果数据不是线性可分的呢？这时候支持向量机就展现出它的核技术的魅力了，通过将低维度的数据映射到高维度，从而将非线性的问题转换成一个线性问题。\u003cbr\u003e\n是不是听着就觉得很绕？这是不可避免的，因为逻辑回归还可以用例子来理解，但是支持向量机基本上就是从数学推导的角度来选取一个最优的分类方法。但大家只要知道，算法是存在“进化”的，逻辑回归解决了线性回归到线性分类的难题，而支持向量机解决了线性分类到非线性分类的难题。\u003cbr\u003e\n支持向量机其实说白了，就是想通过数学的方法，看看怎么样能够将数据“最大程度”地分离开，就要找这个“最大程度”，我们叫“边际”(margin)最大。而支持向量机的名字也是有原因的，“边际”其实只由两类中离分割线最近的点所决定，这些点叫“支持向量”。\u003cbr\u003e\n另外，需要注意的是，在逻辑回归的损失函数求解过程中，我们是将不同类的标签记作1和0的，但是在支持向量机的损失函数求解中，我们是将不同类标签记作1和-1的，这么做是对我们求解有用的。\u003c/p\u003e\n\u003ch2\u003e2.3 损失函数\u003c/h2\u003e\n\u003cp\u003e我们之前说过，支持向量机的求解目标就是max margin，让“边际”最大。那么怎么去求这个“边际”呢？\u003c/p\u003e\n\u003cbr\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 465px; max-height: 367px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 78.92%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"465\" data-height=\"367\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-19884ff468022645.png\" data-original-width=\"465\" data-original-height=\"367\" data-original-format=\"image/png\" data-original-filesize=\"12716\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e支持向量机分析\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cp\u003e原理鄙人的画图水平有限，在上图中，我们假设有一条线w·x+b=0可以将两类点分开，这条线往上平移到红色类的边缘处，这条线就是w·x+b=1,；往下平移到橙色类边缘处，这条线就是w·x+b=-1。那有人就会问，向上移应该是截距项增加啊，那也应该是w·x+b=-1啊？而且，为什么上下移动就是等号右边加一个单位呢？同志们，我们这里只是一个记号而已，要知道，w和b都是一个符号，两边可以同时除以-1，也可以同时除以其他数，等号左边也同时除以，这时候我们会得到新的w和b，但是我们同样用w和b表示而已，不必纠结。\u003cbr\u003e\n我们如果假设Xc和Xd是决策边界上的两个点，那么就有W·Xc+b=0，W·Xd+b=0，相减得到W·(Xc-Xd)=0。两个向量点积为0，则说明垂直，w和我们要求的边际d是相同方向。\u003cbr\u003e\n这时候，假设Xa和Xb分别在上边缘和下边缘上，那么就有W·Xa+b=1，W·Xb+b=-1，相减得到W·(Xa-Xb)=2。\u003cbr\u003e\n我们对两边同时除以W的模，则有W/||W|| · (Xa-Xb) = 2/||W||，线性代数过关的小伙伴们能很快反应过来，等号左边其实就是Xa和Xb两点连线这个向量在w方向的投影，其实也就是我们要求的两条边界线之间的距离d。那么，我们的求解目标也很明确了，就是要max d，也就是max 2/||W||。我们将这个问题转化一下，其实也就是min 1/2 * W2，当然，外加一个限制条件，y * (w*x+b)\u0026gt;1 （w·x+b\u0026gt;1表示的是红色的点，这时候y=1；w·x+b\u0026lt;-1表示的是橙色的点，这时候y=-1。这下明白将两类标签定义为1和-1而不是1和0的用处了吧）。\u003cbr\u003e\n当然，这只是我们弄出损失函数的第一步，这个带约束条件的最小化问题仍然是不好求解的。之后，我们还需要用拉格朗日乘数法，将上面这个约束条件去掉，转化为一个拉格朗日函数，我叫它第二步；再之后，我们要用拉格朗日对偶的性质，去求解我们第二步得到的拉格朗日函数，这是第三步。这就是我归纳的三步走。当然，这些数学推导还是有点复杂和繁琐的，我在这里也不推导了，想了解的自行百度。\u003c/p\u003e\n\n\u003ch2\u003e2.4 损失函数求解\u003c/h2\u003e\n\u003cp\u003e支持向量机损失函数的求解就不是梯度下降法这么简单的了，涉及太多数学推导，在这里我同样也不写了，大家一定要自行去了解序列最小优化算法（SMO），贴一篇吧：\u003cbr\u003e\n\u003ca href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_39521554%2Farticle%2Fdetails%2F80723770\" target=\"_blank\"\u003ehttps://blog.csdn.net/qq_39521554/article/details/80723770\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e2.5 核函数的选择(kernel \u0026amp; gamma)\u003c/h2\u003e\n\u003cp\u003e接下来是SVM中第一个重要参数，kernel。我们之前也说过了，支持向量机之所以这么牛，就是因为核技术。核函数是什么呢？我们之前提到了，很多数据在本身的维度是线性不可分的，那怎么办呢？那就将数据映射到更高维度的空间去看一看，可能就可以线性分割了。核函数可以做什么呢？1、不需要去知道将低维映射到高维所需要用的函数；2、可以先在低维度进行点积运算，再直接映射到高纬度，大大减少了运算量；3、避免了维度诅咒。\u003cbr\u003e\n参数kernel常用的共有四个：“linear”、“poly”、“rbf”、“sigmoid”。默认的并且是最常使用的是高斯核rbf。效果最差的、也是最不常用的是“sigmoid”。当然，它们各自有各自出场的场合。\u003cbr\u003e\n我们可以来看下三种核函数(只看‘linear’、‘rbf’、‘sigmoid’，'poly'跑出结果是非常久的，这里就不实验了)在乳腺癌数据集上的表现。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.svm import SVC\nfrom sklearn import metrics\n\nkernelList = ['linear','rbf','sigmoid']         \n\nfor kernel in kernelList:\n    svc = SVC(kernel=kernel).fit(X_train,y_train)\n    y_pred = svc.predict(X_test)\n    score_svc = metrics.accuracy_score(y_test,y_pred)\n    print(score_svc)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e可以看到，‘linear’、‘rbf’、‘sigmoid’核表现分别为0.9790、0.6503、0.6503，可以看到，线性核表现最优，高斯核作为默认参数准确率却比较低。因为乳腺癌数据集是一个线性可分数据集。\u003cbr\u003e\n如果我们对数据标准化一下呢？\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# 读入癌症数据集\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\ncancer=datasets.load_breast_cancer()\nX=cancer.data\ny=cancer.target\n\n# 数据标准化\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X)\nX= scaler.transform(X)\n\nX_train,X_test,y_train,y_test=train_test_split(X,y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e再执行一下上面的三个核函数，‘linear’、‘rbf’、‘sigmoid’的得分分别为0.9790、\u003cbr\u003e\n0.9860、0.9790。我们会发现，高斯核的表现一下子变成了number one，“sigmoid”居然也都跟线性核打平手了。\u003cbr\u003e\n因为涉及到算“距离”，所以可以看到，量纲问题就非常重要了。\u003cbr\u003e\n因为高斯核(rbf)在处理线性和非线性问题时都很好用，所以我们重点看高斯核。\u003cbr\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n高斯径向基核函数表达式：\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 216px; max-height: 37px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 17.130000000000003%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"216\" data-height=\"37\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-b3a815b3ca2ab7f2.png\" data-original-width=\"216\" data-original-height=\"37\" data-original-format=\"image/png\" data-original-filesize=\"2006\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n我们可以调的就是公式里的gamma值。我们试着来调一下gamma。\n\u003cpre\u003e\u003ccode\u003escore_gamma=[]\ngammaList=np.logspace(-10,1,50)\n\nfor gamma in gammaList:\n    svc = SVC(gamma=gamma).fit(X_train,y_train)\n    score_gamma.append(svc.score(X_test,y_test))\n\nprint('gamma={}时,得分最高={}'.format(gammaList[score_gamma.index(max(score_gamma))],max(score_gamma)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003egamma=0.004291934260128779时,得分最高=0.986013986013986。\u003cbr\u003e\n当然，也可以将调参曲线画出来。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eplt.plot(gammaList,score_gamma)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 379px; max-height: 250px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 65.96%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"379\" data-height=\"250\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-0e14851ef61e77c2.png\" data-original-width=\"379\" data-original-height=\"250\" data-original-format=\"image/png\" data-original-filesize=\"7788\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003egamma调参\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cp\u003e如果选用的是'poly'多项式核函数，还可以去调一下degree，大家可自行去试。\u003c/p\u003e\n\n\u003ch2\u003e2.6 如何避免过拟合(重要参数C)\u003c/h2\u003e\n\u003cp\u003e现实很残酷，基本是不可能找到一条线能够将两类点完全分开的。或者举一个更明显一点的例子，还是上面那个图，如果这时候新来了两个点1和点2，很明显原来的那条分界线就不能将两类点完全分开了，这时候蓝色的新边界才是能将点完全分开的最好分割线。但也有个问题，使用新的分割线，两类点之间的距离，也就是“边际”（margin）就变窄了，很有可能这两个点本身就是异常点，这时候就需要我们去权衡，是尽可能地将两类点分开呢，还是让大多数点分离得更远。前者呢，就叫做硬间隔（hard margin），这时候有过拟合的风险；后者呢，叫做软间隔（soft margin），会有欠拟合的风险。\u003c/p\u003e\n\u003cbr\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 447px; max-height: 384px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 85.91%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"447\" data-height=\"384\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-420230f7eba57b9a.png\" data-original-width=\"447\" data-original-height=\"384\" data-original-format=\"image/png\" data-original-filesize=\"15362\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cp\u003e为了防止过拟合呢，我们会给损失函数加上一个“松弛度”，但作为权衡，会在这个“松弛度”前面加上一个惩罚系数C，防止对分错的点“太过宽松”。\u003cbr\u003e\n需要注意的是，逻辑回归中也有这么一个惩罚系数C，但它不是在正则项前面，而SVM中的C是在“松弛度”的前面，两个C效果却是一样的，C太大，模型容易过拟合；C太小，模型容易欠拟合。\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003escore_C=[]\nCList=np.linspace(0.01,30,50)\n\nfor i in CList:\n    svc = SVC(C=i).fit(X_train,y_train)\n    score_C.append(svc.score(X_test,y_test))\n\nprint('C={}时,得分最高={}'.format(CList[score_C.index(max(score_C))],max(score_C)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eC=0.6220408163265306时,得分最高=0.986013986013986。\u003cbr\u003e\n同样可以画出来看一看。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eplt.plot(CList,score_C)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 379px; max-height: 250px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 65.96%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"379\" data-height=\"250\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-2bba424736ef330c.png\" data-original-width=\"379\" data-original-height=\"250\" data-original-format=\"image/png\" data-original-filesize=\"7665\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2\u003e2.7 如何处理不均衡问题\u003c/h2\u003e\n\u003cp\u003e同样，调节class_weight。\u003c/p\u003e\n\u003ch1\u003e三、K近邻(knn算法)\u003c/h1\u003e\n\u003cp\u003eknn可以说是分类中原理最简单最容易理解的了。\u003c/p\u003e\n\u003ch2\u003e3.1 sklearn实战\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nknn = KNeighborsClassifier().fit(X_train,y_train)\n\ny_prob = knn.predict_proba(X_test)[:,1]                              \ny_pred = knn.predict(X_test)                                       \nfpr_knn,tpr_knn,threshold_knn = metrics.roc_curve(y_test,y_prob)   \nauc_knn = metrics.auc(fpr_knn,tpr_knn)                              \nscore_knn = metrics.accuracy_score(y_test,y_pred)\nprint([score_knn,auc_knn])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eknn算法的准确率为0.9441, AUC值为0.9766。\u003cbr\u003e\n这个结果其实已经很不错的，如果再将数据标准化呢？\u003cbr\u003e\n我们将数据标准化后，得出准确率为0.9930, AUC值为0.9977。这么一个最简单的分类算法，在乳腺癌数据集上的表现居然如此之好。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eknn\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 527px; max-height: 57px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 10.82%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"527\" data-height=\"57\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-bba65cd9e79a30b5.png\" data-original-width=\"527\" data-original-height=\"57\" data-original-format=\"image/png\" data-original-filesize=\"3557\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2\u003e3.2 knn原理\u003c/h2\u003e\n\u003cp\u003eknn就是字面的意思，选定k个最近的点，看这k个点中占比最多的类作为标签。毫无疑问，k的取值至关重要。\u003c/p\u003e\n\u003ch2\u003e3.3 K值选择(n_neighbors)\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003escore_K=[]\nKList=range(2,13)\n\nfor k in KList:\n    knn = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)\n    score_K.append(knn.score(X_test,y_test))\n\nprint('K={}时,得分最高={}'.format(KList[score_K.index(max(score_K))],max(score_K)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eK=5时,得分最高=0.9930。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eplt.plot(KList,score_K)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 379px; max-height: 250px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 65.96%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"379\" data-height=\"250\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-73188e68dfae5300.png\" data-original-width=\"379\" data-original-height=\"250\" data-original-format=\"image/png\" data-original-filesize=\"10859\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2\u003e3.4 k个点的权重(weights)\u003c/h2\u003e\n\u003cp\u003e我们要去想这么一个问题，假设一个点周围最近的5个点中，有2个点离得非常近，有3个点离得很远。这时候，虽然3个点的类占大头，但直觉告诉我们，它很可能是和离得比较近的那2个点是一类的。所以，可能我们需要给离得近的那2个点更大一点的权重。\u003cbr\u003e\nweights的默认值是'uniform'，表示所有最近邻样本权重都一样。如果是\"distance\"，则权重和距离成反比例，即距离预测目标更近的近邻具有更高的权重，这样在预测类别或者做回归时，更近的近邻所占的影响因子会更加大。当然，我们也可以自定义权重，即自定义一个函数，输入是距离值，输出是权重值。这样我们可以自己控制不同的距离所对应的权重。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nknn = KNeighborsClassifier(weights='distance').fit(X_train,y_train)\n\ny_prob = knn.predict_proba(X_test)[:,1]                              \ny_pred = knn.predict(X_test)                                       \nfpr_knn,tpr_knn,threshold_knn = metrics.roc_curve(y_test,y_prob)   \nauc_knn = metrics.auc(fpr_knn,tpr_knn)                              \nscore_knn = metrics.accuracy_score(y_test,y_pred)\nprint([score_knn,auc_knn])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e将weights改为'distance'之后，会发现，精确度为0.9930，而AUC的值竟然上升到了0.9991。\u003c/p\u003e\n\u003ch2\u003e3.5 距离计算方式(metric)\u003c/h2\u003e\n\u003cp\u003eknn可以使用的距离度量较多，一般来说默认的欧式距离，但knn里默认的是‘minkowski’。可以来看下不同的距离计算方式下模型的表现。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elist=['euclidean','manhattan','chebyshev','minkowski']\n\nfor i in list:\n    knn = KNeighborsClassifier(metric=i).fit(X_train,y_train)\n    print(knn.score(X_test,y_test))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e可以看到，euclidean欧氏距离和minkowski闵可夫斯基距离得分最高，同为0.9930，manhattan曼哈顿距离次之为0.9790，chebyshev切比雪夫距离的效果最差为0.9510。\u003c/p\u003e\n\u003ch1\u003e四、决策树\u003c/h1\u003e\n\u003cp\u003e接下来我们进入到树这个大类里了，前面三种算法基本都涉及到一个距离的计算，但是树却不用，因此也并不需要标准化（乳腺癌的数据集标准化之后反而降低了得分）。\u003c/p\u003e\n\u003ch2\u003e4.1sklearn实战\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn import tree\nfrom sklearn import metrics\n\ndtc = tree.DecisionTreeClassifier()                              # 建立决策树模型\ndtc.fit(X_train,y_train)                                         # 训练模型\ny_prob = dtc.predict_proba(X_test)[:,1]                          # 预测1类的概率\ny_pred = dtc.predict(X_test)                                     # 模型对测试集的预测结果 \nfpr_dtc,tpr_dtc,threshod_dtc= metrics.roc_curve(y_test,y_prob)   # 获取真阳率、伪阳率、阈值\nscore_dtc = metrics.accuracy_score(y_test,y_pred)                \nauc_dtc = metrics.auc(fpr_dtc,tpr_dtc) \nprint([score_dtc,auc_dtc])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e决策树得分0.9441，AUC值为0.9443。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edtc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 540px; max-height: 109px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 20.19%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"540\" data-height=\"109\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-67b63881ee0fac22.png\" data-original-width=\"540\" data-original-height=\"109\" data-original-format=\"image/png\" data-original-filesize=\"6897\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2\u003e4.2 决策树原理\u003c/h2\u003e\n\u003cp\u003e关于决策树的原理也是可以查到很多了，只需要知道这些概念：\u003cbr\u003e\n①信息量：-lnP\u003cbr\u003e\n②熵：信息量的期望值(-∑PlnP)\u003cbr\u003e\n③gini：和信息熵类似(∑P(1-P))\u003cbr\u003e\n④信息增益\u003cbr\u003e\n⑤信息增益率\u003cbr\u003e\n⑥预剪枝方法：预剪枝方法：1限制深度；2限制叶子结点的样本个数；3对每次分裂的信息增益设定阈值。\u003cbr\u003e\n⑦搞清楚ID3、C4.5、Cart树：\u003cbr\u003e\nID3的局限性：1信息增益存在过度拟合的缺点。2不能直接处理连续型变量。3对缺失值敏感。\u003cbr\u003e\nC4.5\u0026amp; cart改进：1使用信息增益率。2对连续型的自动分箱。\u003cbr\u003e\n关于熵：\u003cbr\u003e\n\u003ca href=\"https://www.jianshu.com/p/8a0ad237b0ed\" target=\"_blank\"\u003ehttps://www.jianshu.com/p/8a0ad237b0ed\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e4.3 如何衡量信息纯度(criterion)\u003c/h2\u003e\n\u003cp\u003e决策树需要找出最佳的分枝方法，衡量的指标就是信息“不纯度”，“不纯度”越低，说明类别分的越纯，也就是分类越好。\u003cbr\u003e\ncriterion这个参数就是用来选择用何种方式来衡量这个“不纯度”，提供了信息熵(entropy)和基尼系数(gini)两种方法，上面有提到过。但是这个指标基本我们也不用去调，因为sklearn中用的是cart二叉树的方法，cart树的衡量指标就是gini。\u003c/p\u003e\n\u003ch2\u003e4.4 如何防止过拟合(max_depth \u0026amp; min_samples_leaf \u0026amp; max_features \u0026amp; min_impurity_decrease)\u003c/h2\u003e\n\u003cp\u003e大家一定要知道，树是天生过拟合的算法，包括之后的随机森林，甚至于以树为基础分类器的集成学习算法，都存在这样一个过拟合的属性。\u003cbr\u003e\n要防止树的过拟合，第一个要提的就是max_depth，这也是防止过拟合最好的神器，在高维度地样本量时非常好用。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edepthList=np.arange(3,10)\nscore=[]\n\nfor i in depthList:\n    dtc = tree.DecisionTreeClassifier(max_depth=i).fit(X_train,y_train)\n    score.append(dtc.score(X_test,y_test))\n\nprint('最优深度为{}，最佳得分是{}'.format(depthList[score.index(max(score))],max(score)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e最优深度为6，最佳得分是0.9441。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eplt.plot(depthList,score)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 379px; max-height: 250px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 65.96%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"379\" data-height=\"250\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-9a2a29ebae4b09b4.png\" data-original-width=\"379\" data-original-height=\"250\" data-original-format=\"image/png\" data-original-filesize=\"11231\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cp\u003emin_samples_leaf则限定每个叶子节点上的最小样本个数，防止最后分的太细。\u003cbr\u003e\nmax_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃，也是用来限制高维度数据的过拟合的。\u003cbr\u003e\nmin_impurity_decrease则限制信息增益的大小，信息增益小于设定数值的分枝不会发生。\u003cbr\u003e\n其实可以通过网格搜索来确定以上这些参数的最佳组合。\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003efrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\n\nparams={'max_depth':[*np.arange(3,10)]\n        ,'min_samples_leaf':[*np.arange(1,50,5)]\n        ,'min_impurity_decrease':[*np.linspace(0,0.5,20)]}\ndtc = tree.DecisionTreeClassifier()\n\nGS = GridSearchCV(dtc, params, cv=10).fit(X_train,y_train)\nGS.best_params_\nGS.best_score_\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e得到最佳参数{'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1}，交叉验证最佳得分0.9343。可以看到，加了别的参数的时候，最优深度就不再是6而是4了。\u003c/p\u003e\n\u003ch2\u003e4.5 树的可视化\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e# 树的可视化\nimport graphviz\nfrom sklearn.tree import export_graphviz\n\ndtc = tree.DecisionTreeClassifier().fit(X_train,y_train)\n\nexport_graphviz(dtc,out_file='DecisionTree.dot',class_names=['0','1'],impurity=False,filled=True)\n\nwith open('DecisionTree.dot') as f:\n    dot_graph=f.read()\n\ngraphviz.Source(dot_graph)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e五、随机森林\u003c/h1\u003e\n\u003cp\u003e随机森林，顾名思义，就是很多棵树组合在一起。\u003c/p\u003e\n\u003ch2\u003e5.1 sklearn实战\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nrfc = RandomForestClassifier()                                     # 建立随机森林分类器\nrfc.fit(X_train,y_train)                                           # 训练随机森林模型\ny_prob = rfc.predict_proba(X_test)[:,1]                            # 预测1类的概率\ny_pred=rfc.predict(X_test)                                         # 模型对测试集的预测结果\nfpr_rfc,tpr_rfc,threshold_rfc = metrics.roc_curve(y_test,y_prob)   # 获取真阳率、伪阳率、阈值  \nauc_rfc = metrics.auc(fpr_rfc,tpr_rfc)                             # AUC得分\nscore_rfc = metrics.accuracy_score(y_test,y_pred)                  # 模型准确率\nprint([score_rfc,auc_rfc])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e模型准确率为0.9580，AUC值为0.9942。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erfc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 569px; max-height: 128px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 22.5%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"569\" data-height=\"128\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-1b6471c7073537ac.png\" data-original-width=\"569\" data-original-height=\"128\" data-original-format=\"image/png\" data-original-filesize=\"8292\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2\u003e5.2 到底种多少棵树(n_estimators)\u003c/h2\u003e\n\u003cp\u003e我们可以在5.1中看到随机森林默认的n_estimators是10，显然，对于集成学习来说，10棵树的数量是偏少了的。好像更新的版本之后n_estimators默认值会变为100。我们来看下随着建的树越多，训练集和测试集数据的得分会有什么样的变化。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\ntrain_score=[]\ntest_score=[]\n\nfor i in range(10,101):\n    rfc = RandomForestClassifier(n_estimators=i,random_state=13).fit(X_train,y_train)\n    train_score.append(rfc.score(X_train,y_train))\n    test_score.append(rfc.score(X_test,y_test))\n    \nplt.figure(figsize=(20, 5))\nplt.plot(range(10,101),train_score,color='orange',label='train')\nplt.plot(range(10,101),test_score,color='red',label='test')\nplt.legend(loc='lower right')\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 700px; max-height: 304px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 26.179999999999996%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"1161\" data-height=\"304\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-48eea9c306d08230.png\" data-original-width=\"1161\" data-original-height=\"304\" data-original-format=\"image/png\" data-original-filesize=\"23487\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cp\u003e可以看到，随着树的数量增多，训练集的效果明显变好，测试集的得分有所上升，但变化不大。初步目测，n_estimators在60左右效果是最好的。\u003c/p\u003e\n\n\u003ch2\u003e5.3 如何防止过拟合\u003c/h2\u003e\n\u003cp\u003e这个跟树是几乎一样，因为咱本身也是树组成的。调节几个重要参数就好了：max_depth、min_samples_leaf、max_features、min_impurity_decrease。\u003c/p\u003e\n\u003ch1\u003e六、二分类模型评价指标\u003c/h1\u003e\n\u003cp\u003e二分类模型评价指标围绕混淆矩阵展开，这个也太经典了，不在这儿详说了：\u003cbr\u003e\n\u003ca href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_27575895%2Farticle%2Fdetails%2F81476871\" target=\"_blank\"\u003ehttps://blog.csdn.net/qq_27575895/article/details/81476871\u003c/a\u003e\u003cbr\u003e\n以随机森林为例，看一看precision和recall值。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emetrics.precision_score(y_test,y_pred) # 精确率\nmetrics.recall_score(y_test,y_pred) # 召回率\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e随机森林模型下精确率和召回率均为0.9759。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eplt.style.use('bmh')\nplt.figure(figsize=(13,10))\n\nplt.plot(fpr_lr,tpr_lr,label='lr')                             # 逻辑回归\nplt.plot(fpr_svc,tpr_svc,label='svc')                          # 支持向量机模型\nplt.plot(fpr_knn,tpr_knn,label='knn')                             # K近邻\nplt.plot(fpr_dtc,tpr_dtc,label='dtc')                          # 决策树\nplt.plot(fpr_rfc,tpr_rfc,label='rfc')                          # 随机森林\n\nplt.legend(loc='lower right',prop={'size':25})\nplt.xlabel('伪阳率')\nplt.ylabel('真阳率')\nplt.title('ROC曲线')\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 700px; max-height: 602px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 77.88000000000001%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"773\" data-height=\"602\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/18317213-b503be1e31589c94.png\" data-original-width=\"773\" data-original-height=\"602\" data-original-format=\"image/png\" data-original-filesize=\"44917\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003eROC曲线\u003c/div\u003e\n\u003c/div\u003e\n\u003cp\u003e当然，小白也是机器学习初学者，有什么理解错误的地方也欢迎指正。这篇文也是针对初学者做的一个总结，希望能够有帮助。本文后续还会不断更新，加入新的分类算法，加入新的调参参数。\u003c/p\u003e\n","voted_down":false,"rewardable":true,"show_paid_comment_tips":false,"share_image_url":"https://upload-images.jianshu.io/upload_images/18317213-599af8d696612c30.png","slug":"3c2dfd6e8e4e","user":{"liked_by_user":false,"following_count":102,"gender":1,"avatar_widget":null,"slug":"1eb8ed4c42ca","intro":"","likes_count":97,"nickname":"猴小白","badges":[],"total_fp_amount":"12282881736446541887","wordage":23767,"avatar":"https://upload.jianshu.io/users/upload_avatars/18317213/fc1ef06e-6f88-4559-9010-33832c3baec3","id":18317213,"liked_user":false},"likes_count":1,"paid_type":"free","show_ads":true,"paid_content_accessible":false,"hide_search_input":true,"total_fp_amount":"72000000000000000","trial_open":false,"reprintable":true,"bookmarked":false,"wordage":5937,"featured_comments_count":0,"downvotes_count":0,"wangxin_trial_open":null,"guideShow":{"audit_user_nickname_spliter":0,"pc_note_bottom_btn":1,"pc_like_author_guidance":1,"ban_some_labels":1,"h5_real_name_auth_link":1,"audit_user_background_image_spliter":0,"audit_note_spliter":0,"new_user_no_ads":1,"launch_tab":0,"include_post":0,"pc_login_guidance":1,"audit_comment_spliter":0,"pc_note_bottom_qrcode":1,"audit_user_avatar_spliter":0,"audit_collection_spliter":0,"pc_top_lottery_guidance":1,"subscription_guide_entry":1,"creation_muti_function_on":1,"explore_score_searcher":1,"audit_user_spliter":0,"pc_note_popup":2},"commentable":true,"total_rewards_count":0,"id":50429503,"notebook":{"name":""},"activity_collection_slug":null,"description":"在小白我的第一篇文里就提出过一个问题，就是现在的教程都太“分散”太“板块”，每一个知识点都单独用一个例子，机器学习算法里也是这样的，可能逻辑回归用葡萄酒的案例讲，决策树又用鸢...","first_shared_at":1562961700,"views_count":4429,"notebook_id":37825232},"baseList":{"likeList":[],"rewardList":[]},"status":"success","statusCode":0},"user":{"isLogin":false,"userInfo":{}},"comments":{"list":[],"featuredList":[]}},"initialProps":{"pageProps":{"query":{"slug":"3c2dfd6e8e4e"}},"localeData":{"common":{"jianshu":"简书","diamond":"简书钻","totalAssets":"总资产{num}","diamondValue":" (约{num}元)","login":"登录","logout":"注销","register":"注册","on":"开","off":"关","follow":"关注","followBook":"关注连载","following":"已关注","cancelFollow":"取消关注","publish":"发布","wordage":"字数","audio":"音频","read":"阅读","reward":"赞赏","zan":"赞","comment":"评论","expand":"展开","prevPage":"上一页","nextPage":"下一页","floor":"楼","confirm":"确定","delete":"删除","report":"举报","fontSong":"宋体","fontBlack":"黑体","chs":"简体","cht":"繁体","jianChat":"简信","postRequest":"投稿请求","likeAndZan":"喜欢和赞","rewardAndPay":"赞赏和付费","home":"我的主页","markedNotes":"收藏的文章","likedNotes":"喜欢的文章","paidThings":"已购内容","wallet":"我的钱包","setting":"设置","feedback":"帮助与反馈","loading":"加载中...","needLogin":"请登录后进行操作","trialing":"文章正在审核中...","reprintTip":"禁止转载，如需转载请通过简信或评论联系作者。"},"error":{"rewardSelf":"无法打赏自己的文章哟~"},"message":{"paidNoteTip":"付费购买后才可以参与评论哦","CommentDisableTip":"作者关闭了评论功能","contentCanNotEmptyTip":"回复内容不能为空","addComment":"评论发布成功","deleteComment":"评论删除成功","likeComment":"评论点赞成功","setReadMode":"阅读模式设置成功","setFontType":"字体设置成功","setLocale":"显示语言设置成功","follow":"关注成功","cancelFollow":"取消关注成功","copySuccess":"复制代码成功"},"header":{"homePage":"首页","download":"下载APP","discover":"发现","message":"消息","reward":"赞赏支持","editNote":"编辑文章","writeNote":"写文章"},"note":{},"noteMeta":{"lastModified":"最后编辑于 ","wordage":"字数 {num}","viewsCount":"阅读 {num}"},"divider":{"selfText":"以下内容为付费内容，定价 ¥{price}","paidText":"已付费，可查看以下内容","notPaidText":"还有 {percent} 的精彩内容","modify":"点击修改"},"paidPanel":{"buyNote":"支付 ¥{price} 继续阅读","buyBook":"立即拿下 ¥{price}","freeTitle":"该作品为付费连载","freeText":"购买即可永久获取连载内的所有内容，包括将来更新的内容","paidTitle":"还没看够？拿下整部连载！","paidText":"永久获得连载内的所有内容, 包括将来更新的内容"},"book":{"last":"已是最后","lookCatalog":"查看连载目录","header":"文章来自以下连载"},"action":{"like":"{num}人点赞","collection":"收入专题","report":"举报文章"},"comment":{"allComments":"全部评论","featuredComments":"精彩评论","closed":"评论已关闭","close":"关闭评论","open":"打开评论","desc":"按时间倒序","asc":"按时间正序","disableText1":"用户已关闭评论，","disableText2":"与Ta简信交流","placeholder":"写下你的评论...","publish":"发表","create":" 添加新评论","reply":" 回复","restComments":"还有{num}条评论，","expandImage":"展开剩余{num}张图","deleteText":"确定要删除评论么？"},"collection":{"title":"被以下专题收入，发现更多相似内容","putToMyCollection":"收入我的专题"},"seoList":{"title":"推荐阅读","more":"更多精彩内容"},"sideList":{"title":"推荐阅读"},"wxShareModal":{"desc":"打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮"},"bookChapterModal":{"try":"试读","toggle":"切换顺序"},"collectionModal":{"title":"收入到我管理的专题","search":"搜索我管理的专题","newCollection":"新建专题","create":"创建","nothingFound":"未找到相关专题","loadMore":"展开查看更多"},"contributeModal":{"search":"搜索专题投稿","newCollection":"新建专题","addNewOne":"去新建一个","nothingFound":"未找到相关专题","loadMore":"展开查看更多","managed":"我管理的专题","recommend":"推荐专题"},"QRCodeShow":{"payTitle":"微信扫码支付","payText":"支付金额"},"rewardModal":{"title":"给作者送糖","custom":"自定义","placeholder":"给Ta留言...","choose":"选择支付方式","balance":"简书余额","tooltip":"网站该功能暂时下线，如需使用，请到简书App操作","confirm":"确认支付","success":"赞赏成功"},"payModal":{"payBook":"购买连载","payNote":"购买文章","promotion":"优惠券","promotionFetching":"优惠券获取中...","noPromotion":"无可用优惠券","promotionNum":"{num}张可用","noUsePromotion":"不使用优惠券","validPromotion":"可用优惠券","invalidPromotion":"不可用优惠券","total":"支付总额","tip1":"· 你将购买的商品为虚拟内容服务，购买后不支持退订、转让、退换，请斟酌确认。","tip2":"· 购买后可在“已购内容”中查看和使用。","success":"购买成功"},"reportModal":{"ad":"广告及垃圾信息","plagiarism":"抄袭或未授权转载","placeholder":"写下举报的详情情况（选填）","success":"举报成功"},"guidModal":{"modalAText":"相似文章推荐","subText":"下载简书APP，浏览更多相似文章","btnAText":"先不下载，下次再说","followOkText":"关注作者成功！","followTextTip":"下载简书APP，作者更多精彩内容更新及时提醒！","followBtn":"下次再说","downloadTipText":"更多精彩内容，就在简书APP","footerDownLoadText":"下载简书APP","modabTitle":"免费送你2次抽奖机会","modalbTip":"抽取10000收益加成卡，下载简书APP概率翻倍","modalbFooterTip":"下载简书APP，天天参与抽大奖","modalReward":"抽奖","scanQrtip":"扫码下载简书APP","downloadAppText":"下载简书APP，随时随地发现和创作内容","redText":"阅读","likesText":"赞","downLoadLeft":"更多好文","leftscanText":"把文字装进口袋"}},"currentLocale":"zh-CN","asPath":"/p/3c2dfd6e8e4e"}},"page":"/p/[slug]","query":{"slug":"3c2dfd6e8e4e"},"buildId":"Fv-3OxfBINx_pJU3GkZeZ","assetPrefix":"https://cdn2.jianshu.io/shakespeare"}</script><script nomodule="" src="./一文入门sklearn二分类实战 - 简书_files/polyfills-83c9f0eea3aa0edfd89e.js"></script><script async="" data-next-page="/p/[slug]" src="./一文入门sklearn二分类实战 - 简书_files/[slug].js"></script><script async="" data-next-page="/_app" src="./一文入门sklearn二分类实战 - 简书_files/_app.js"></script><script src="./一文入门sklearn二分类实战 - 简书_files/webpack-f504f6266e52ee1a40ff.js" async=""></script><script src="./一文入门sklearn二分类实战 - 简书_files/commons.f8bed0abd693d06a6ea3.js" async=""></script><script src="./一文入门sklearn二分类实战 - 简书_files/styles.848aa67b1a6c6dddd4c2.js" async=""></script><script src="./一文入门sklearn二分类实战 - 简书_files/main-550fe512e049aff0be03.js" async=""></script></body></html>